<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP - 标签 - fengchen</title><link>http://fengchen321.github.io/tags/nlp/</link><description>fengchen</description><generator>Hugo 0.139.0 &amp; FixIt v0.3.15</generator><language>zh-CN</language><lastBuildDate>Sat, 10 Jun 2023 18:22:27 +0800</lastBuildDate><atom:link href="http://fengchen321.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>BERT</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/</link><pubDate>Sat, 10 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="bert" class="heading-element">&lt;span>BERT&lt;/span>
 &lt;a href="#bert" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1810.04805"target="_blank" rel="external nofollow noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a>
作者：Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
发表时间：(NAACL-HLT 2019)&lt;/p></description></item><item><title>Transformer</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/</link><pubDate>Sat, 10 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="transformer" class="heading-element">&lt;span>Transformer&lt;/span>
 &lt;a href="#transformer" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1706.03762#"target="_blank" rel="external nofollow noopener noreferrer">Attention Is All You Need&lt;/a>
作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
发表时间：(NIPS 2017)&lt;/p></description></item></channel></rss>