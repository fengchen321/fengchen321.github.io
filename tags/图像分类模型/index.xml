<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>图像分类模型 - 标签 - fengchen</title><link>http://fengchen321.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</link><description>fengchen</description><generator>Hugo 0.139.0 &amp; FixIt v0.3.15</generator><language>zh-CN</language><lastBuildDate>Fri, 02 Jun 2023 18:22:27 +0800</lastBuildDate><atom:link href="http://fengchen321.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml"/><item><title>AlexNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="alexnet" class="heading-element">&lt;span>AlexNet&lt;/span>
 &lt;a href="#alexnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"target="_blank" rel="external nofollow noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>AutoML</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/automl/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/automl/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="nir" class="heading-element">&lt;span>NIR&lt;/span>
 &lt;a href="#nir" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2002.12580"target="_blank" rel="external nofollow noopener noreferrer">Neural Inheritance Relation Guided One-Shot Layer Assignment Search&lt;/a>
作者：&lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Meng%2C&amp;#43;R"target="_blank" rel="external nofollow noopener noreferrer">Rang Meng&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chen%2C&amp;#43;W"target="_blank" rel="external nofollow noopener noreferrer">Weijie Chen&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xie%2C&amp;#43;D"target="_blank" rel="external nofollow noopener noreferrer">Di Xie&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang%2C&amp;#43;Y"target="_blank" rel="external nofollow noopener noreferrer">Yuan Zhang&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Pu%2C&amp;#43;S"target="_blank" rel="external nofollow noopener noreferrer">Shiliang Pu&lt;/a>
发表时间：(AAAI 2020)&lt;/p></description></item><item><title>ConvNeXt</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/convnext-/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/convnext-/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="convnext" class="heading-element">&lt;span>ConvNeXt&lt;/span>
 &lt;a href="#convnext" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2201.03545v1"target="_blank" rel="external nofollow noopener noreferrer">A ConvNet for the 2020s&lt;/a>
作者：&lt;a href="https://liuzhuang13.github.io/"target="_blank" rel="external nofollow noopener noreferrer">Zhuang Liu&lt;/a>, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie&lt;/p></description></item><item><title>EfficientNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/efficientnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/efficientnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="efficientnet" class="heading-element">&lt;span>EfficientNet&lt;/span>
 &lt;a href="#efficientnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1905.11946"target="_blank" rel="external nofollow noopener noreferrer">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks&lt;/a>
作者：Mingxing Tan, Quoc V. Le
发表时间：(ICML 2019)&lt;/p></description></item><item><title>Inception</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/inception/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/inception/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;p>策略：&lt;font color=#f12c60>&lt;strong>split-transform-merge&lt;/strong>&lt;/font>&lt;/p>
&lt;h2 id="inceptionv1googlenet" class="heading-element">&lt;span>InceptionV1（GoogLeNet）&lt;/span>
 &lt;a href="#inceptionv1googlenet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1409.4842"target="_blank" rel="external nofollow noopener noreferrer">Going Deeper with Convolutions&lt;/a>&lt;/p></description></item><item><title>ResNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="resnet" class="heading-element">&lt;span>ResNet&lt;/span>
 &lt;a href="#resnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1512.03385"target="_blank" rel="external nofollow noopener noreferrer">Deep Residual Learning for Image Recognition&lt;/a>&lt;/p></description></item><item><title>SENet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/senet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/senet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="senet" class="heading-element">&lt;span>SENet&lt;/span>
 &lt;a href="#senet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1709.01507"target="_blank" rel="external nofollow noopener noreferrer">Squeeze-and-Excitation Networks&lt;/a>
作者：Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu
发表时间：(CVPR 2018)&lt;/p></description></item><item><title>VGGNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/vggnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/vggnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="vggnet" class="heading-element">&lt;span>VGGNet&lt;/span>
 &lt;a href="#vggnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1409.1556"target="_blank" rel="external nofollow noopener noreferrer">Very Deep Convolutional Networks for Large-Scale Visual Recognition&lt;/a>
作者：&lt;a href="https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=L7lMQkQAAAAJ"target="_blank" rel="external nofollow noopener noreferrer">Simonyan K&lt;/a>, &lt;a href="https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=UZ5wscMAAAAJ"target="_blank" rel="external nofollow noopener noreferrer">Zisserman A. V&lt;/a>
发表时间：(ICLR 2015)
&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"target="_blank" rel="external nofollow noopener noreferrer">论文主页&lt;/a>&lt;/p></description></item></channel></rss>