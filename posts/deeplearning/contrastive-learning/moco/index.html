<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Moco - fengchen</title><meta name=author content><meta name=description content="Moco"><meta name=keywords content='Deep Learning,对比学习'><meta itemprop=name content="Moco"><meta itemprop=description content="Moco"><meta itemprop=datePublished content="2023-06-07T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-07T18:22:27+08:00"><meta itemprop=wordCount content="3063"><meta itemprop=keywords content="Deep Learning,对比学习"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="Moco"><meta property="og:description" content="Moco"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-07T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-07T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="对比学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="Moco"><meta name=twitter:description content="Moco"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/ title="Moco - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/simclr/ title=SimCLR><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/instdisc/ title=InstDisc><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Moco","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/contrastive-learning\/moco\/"},"genre":"posts","keywords":"Deep Learning, 对比学习","wordcount":3063,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/contrastive-learning\/moco\/","datePublished":"2023-06-07T18:22:27+08:00","dateModified":"2023-06-07T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"作者"},"description":"Moco"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Moco</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
Anonymous</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-07 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-07>2023-06-07</time></span>&nbsp;<span title="3063 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3100 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 7 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#moco标题>MoCo标题</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related work</a></li><li><a href=#methods>Methods</a></li><li><a href=#拓展阅读>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=moco class=heading-element><span>MoCo</span>
<a href=#moco class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1911.05722 target=_blank rel="external nofollow noopener noreferrer">Momentum Contrast for Unsupervised Visual Representation Learning</a> <a href=https://www.semanticscholar.org/paper/Momentum-Contrast-for-Unsupervised-Visual-Learning-He-Fan/ec46830a4b275fd01d4de82bffcabe6da086128f target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：<a href=https://kaiminghe.github.io/ target=_blank rel="external nofollow noopener noreferrer">Kaiming He,</a> <a href=https://haoqifan.github.io/ target=_blank rel="external nofollow noopener noreferrer">Haoqi Fan,</a> <a href=https://ppwwyyxx.com/ target=_blank rel="external nofollow noopener noreferrer">Yuxin Wu</a>, <a href=https://www.sainingxie.com/ target=_blank rel="external nofollow noopener noreferrer">Saining Xie</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=W8VIEZgAAAAJ&amp;pagesize=1000&amp;sortby=pubdate" target=_blank rel="external nofollow noopener noreferrer">Ross Girshick</a></p><p>发表时间：(CVPR 2020)</p><p><a href=https://github.com/facebookresearch/moco target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>视觉 + 对比学习的里程碑式的工作</p></blockquote><p><strong>无监督学习的目的</strong>：在一个很大的无标注的数据集上训练，模型学到的特征可以很好的迁移到下游任务。</p><p><strong>对比学习</strong>：通过对比去学习模型，只需要知道图 A和 图 B相似，图 A、图 B和 图 C不相似；而不需要真的知道各个图片的具体类别。</p><blockquote><p>3 张图进入一个网络 M 得到特征 f1、f2、f3，在一个学习好的特征空间 embedding space 中，f1、f2 的特征尽量近，和 f3 的特征尽量远离。</p><p>对比学习学到的很好的特征：类似物体在这个特征空间 <strong>相邻</strong>，不类似的物体在特征空间 <strong>远离</strong></p></blockquote><p><strong>Q: 图 1 和 图 2 相似，和图 3 都不相似，难道不是有监督学习吗？Why 对比学习在 CV 领域被认为是无监督训练呢？</strong></p><p>CV 领域 设计巧妙的代理任务 pre-text task，人为设立一些规则 —— 定义哪些图片相似、哪些图片不相似，为自监督学习提供监督信号，从而自监督训练</p><p>一个无标注的数据集，n 张图片，$x_1, x_2, &mldr;, x_n$， 随机选取一张图片，做 transformation。</p><p>以 $x_1 $图片为例，$x_1 $随机裁剪 + 数据增广 得到 $x_i^1$, $x_i^2 $（看起来和 $x_1$ 有区别的 2 张照片， $x_1$ 的正样本），数据集中的其它图片 $x_j$, $j ≠ i $是 $x_1$ 的负样本</p><blockquote><p>基于图片和图片本身的变换是正样本，和其它图片是负样本</p><p>ImageNet-1K 此时不是 1000 个类别，而是 100w 个类别。每个图片都是它自己的正样本，其余都是负样本。</p></blockquote><p>对比学习的框架：灵活性&ndash;定义正负样本的规则</p><ul><li>同一个视频里的任意两帧是正样本，和其它视频的所有帧是负样本</li><li>NLP, simCSE 把同样的句子扔给模型，但是做 2 次 forward，通过不同的 dropout 得到一个句子的 2 个特征；和其它所有句子的特征都是负样本。</li><li>CMC 论文：一个物体的不同视角 view（正面、背面；RGB 图像、深度图像）作为不同形式的正样本。</li><li>多模态领域：Open AI 的 CLIP 模型</li></ul><h2 id=moco标题 class=heading-element><span>MoCo标题</span>
<a href=#moco%e6%a0%87%e9%a2%98 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>Momentum Contrast for Unsupervised Visual Representation Learning</p><p>动量对比学习的方法做无监督视觉特征学习</p></blockquote><p>Momentum Contrast: 动量对比学习</p><blockquote><p>动量：(指数)加权移动平均值 $y_t = m * y_{t-1} + (1 - m) * x_t$</p><ul><li><p>m: 动量的超参数</p></li><li><p>$y_{t-1}$: 上一个时刻的输出</p></li><li><p>$x_t$: 当前时刻的输入</p></li><li><p>m 趋近于 1，$y_t $改变缓慢，当前时刻的输入 $x_t $没什么影响</p></li><li><p>m 趋近于 0, $y_t $更多依赖于当前时刻的输入。</p></li></ul><p>MoCo 利用动量的特性，缓慢的更新一个编码器，从而让中间学习到的字典中的特征尽可能保持一致。</p></blockquote><p><strong>MoCo 从什么角度做对比学习呢？</strong></p><p><strong>dictionary look-up</strong>, 字典查询任务, a dynamic dictionary with a queue and a moving-averaged encoder 动态字典</p><ul><li><strong>一个队列</strong>：队列中的样本<strong>无需梯度回传</strong>，可以放很多负样本，让字典变得很大</li><li><strong>一个移动平均的编码器</strong>：让字典的特征尽可能的保持一致</li><li>一个大的、一致的字典，有利于无监督的对比学习训练。</li></ul><h2 id=introduction class=heading-element><span>Introduction</span>
<a href=#introduction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>NLP 的离散单词更具语义性，CV的连续、高维信号不好构建字典</strong></p><p>无监督在 CV 不成功的原因是什么？</p><ul><li><p>原始信号空间的不同</p></li><li><p>NLP 原始信号是离散的，词、词根、词缀，容易构建 tokenized dictionaries 做无监督学习</p><blockquote><p>tokenized: 把一个词对应成某一个特征</p><p>Why tokenized dictionaries 有助于无监督学习？</p><ul><li>把字典的 key 认为是一个类别，有类似标签的信息帮助学习</li><li>NLP 无监督学习很容易建模，建好的模型也好优化</li><li>CV 原始信号是连续的、高维的，不像单词具有浓缩好的、简洁的语义信息，不适合构建一个字典</li><li>如果没有字典，无监督学习很难建模</li></ul></blockquote></li></ul><p><strong>给CV 无监督对比学习 构建一个 ==大 (by queue)==+ ==一致 (momentum encoder)==的字典</strong></p><p>$f_{11} $当成 query 在 $f_{12}, f_2, f_3, &mldr;, f_n$ 组成的字典的 key 特征条目 $k_1, k_2, &mldr;$ 里面查找，dictionary look-up 靠近 $f_{12}$, 远离 $f_2, f_3, &mldr;$</p><p>从动态字典的角度看对比学习，什么样的字典才适合呢？ <strong>大 + 一致性</strong></p><blockquote><p>large</p><blockquote><p>从连续高维空间做更多的采样。字典 key 越多，表示的视觉信息越丰富，匹配时更容易找到具有区分性的本质特征。</p><p>如果 字典小、key 少，模型可能学到 shortcut 捷径，不能泛化</p></blockquote><p>consistent</p><blockquote><p>字典里的 key ($k_0, k_1, k_2, &mldr;, k_N$) 应该由相同的 or 相似的编码器生成</p><p>如果字典的 key 是由不同的编码器得到的，query q 做字典查询时，很有可能 找到和 query 使用同一个 or 相似编码器生成的 key，而不是语义相似的 key。另一种形式的 shortcut solution</p></blockquote></blockquote><h2 id=related-work class=heading-element><span>Related work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>两个可以做的点：pretext tasks and loss functions</p><ul><li><strong>代理任务</strong>：为了学习一个好的数据特征表示</li><li><strong>损失函数</strong>：和代理任务可以分开研究。 MoCo 的创新点在损失函数，又大又一致的字典 影响 infoNCE 目标函数的计算</li></ul><p><strong>损失函数：判别式、生成式、对比学习、对抗学习</strong></p><blockquote><p>对比学习的损失：目标不固定，训练过程中不断改变。目标有编码器抽出来的特征（MoCo 的字典）而决定</p><p>判别式：预测 8 个位置中的哪一个方位（九宫格）</p><p>生成式：重建整张图</p><p>对比学习的目标：测量样本对 在特征空间的相似性。相似样本离得近，不相似样本离得远</p><p>对抗学习的目标：衡量两个概率分布之间的差异</p></blockquote><p>对比学习和代理任务的关系：</p><ul><li>不同的代理任务 可以和 某种形式的对比学习的目标函数 配对使用</li><li>Instance discrimination 个体判别方法 &mdash;&ndash; examplar based 代理任务很相关</li><li>CPC contrastive predictive coding 用上下文信息预测未来 &mdash;&ndash; context auto-encoding 上下文自编码</li><li>CMC contrastive multiview coding 利用一个物体的不同视角做对比 &mdash;&ndash; olorization 图片上色（同一个图片的 2 个视角：黑白 和 彩色）</li></ul><h2 id=methods class=heading-element><span>Methods</span>
<a href=#methods class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>对比学习和最近的发展，都可以看成是一个训练一个 encoder 来做 字典查询 的任务</p>$$
L_q=-log\ \frac{exp(q\cdot k_+ / \tau)}{\sum_{i=0}^K exp(q\cdot k_i / \tau)}
$$<blockquote><p><em>q</em> : 编码查询,;${k_0, k_1, k_2, &mldr;}$: 一组键 <em>key</em> 编码样本 。假设字典中只有一个键$（k_+）$与 <em>q</em> 匹配。</p><p>$\tau$: 温度超参数； K : 负样本个数。</p><p>试图将 q 分类为 k+ 的 (K+1)-way softmax-based 分类器的对数损失。</p></blockquote><center><img src="/images/Contrastive learning/MoCo.assets/moco.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">MoCo 框架图</div></center><blockquote><p>end to end: 编码器都能通过反向传播实时更新学习。（特征高度一致）</p><blockquote><p>局限性：字典大小 == mini-batch size 硬件限制；大 batch size 优化难，难收敛</p></blockquote><p>memory bank: 只有query 编码器可以进行梯度回传更新。 把所有特征存在memory bank里， 从memory bank中随机抽取 key 当作字典；没有反向传播，所以它可以支持大的字典大小。</p><blockquote><p>局限性：不同时刻编码器(梯度回传更新)得到的特征缺乏一致性</p></blockquote><p>MoCo: 采用队列形式实现一个字典（ 不受batch size限制 ），使用动量编码器（提高一致性）</p></blockquote><p><strong>queue</strong> 数据结构: 剥离字典的大小 和 显卡内存的限制，让字典的大小和模型每次做前向传播的 batch size 的大小分开</p><blockquote><p>当前 mini-batch 入队，最早进入队列的 mini-batch 出队</p><p>队列的大小 == 字典的大小，但是每次做 iteration 更新，并不需要更新字典中所有 key 元素的值。</p></blockquote><p><strong>momentum encoder</strong>：使用 queue，只有当前 mini-batch 的特征是由当前的编码器得到的；之前的 key 是由不同时刻的编码器抽取的特征，如何保持 consistent 呢？</p><blockquote><p>momentum encoder 由当前时刻的 encoder 初始化而来</p><p>$\theta_k = m * \theta_{k-1}+ (1-m) * \theta_q$</p><blockquote><p>动量系数: $m\in[0,1)$。只有参数 θq 通过反向传播更新。</p><p>动量参数 m 较大时，$\theta_k $的更新缓慢，不过多的依赖于 $\theta_q $当前时刻的编码器，即不随着当前时刻的编码器快速改变，尽可能保证 字典里的 key 都是由相似的编码器生成的特征，保证特征的 consistent</p></blockquote></blockquote><center><img src="/images/Contrastive learning/MoCo.assets/moco_algorithm.png" width=600><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">MoCo pytorch 算法</div></center><p><strong>Shuffling BN</strong></p><blockquote><p>小 trick ,后续没再用</p></blockquote><p>BN会阻止模型学习好的表征，可能是由于样本中间的信息（由BN引起）泄露。</p><p>操作：配给GPU之前打乱样本顺序，用多个GPU进行训练，并对每个GPU的样本独立进行BN。再把顺序恢复后算loss。</p><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.bilibili.com/video/BV1C3411s7t9/?vd_source=d28e92983881d85b633a5acf8e46efaa" target=_blank rel="external nofollow noopener noreferrer">MoCo 论文逐段精读【论文精读】</a></p><h2 id=moco-v2 class=heading-element><span>MoCo-V2</span>
<a href=#moco-v2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2003.04297 target=_blank rel="external nofollow noopener noreferrer">Improved Baselines with Momentum Contrastive Learning</a> <a href=https://www.semanticscholar.org/paper/Improved-Baselines-with-Momentum-Contrastive-Chen-Fan/a1b8a8df281bbaec148a897927a49ea47ea31515 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者: <a href=https://xinleic.xyz/ target=_blank rel="external nofollow noopener noreferrer">Xinlei Chen</a>, <a href=https://haoqifan.github.io/ target=_blank rel="external nofollow noopener noreferrer">Haoqi Fan,</a> <a href="https://scholar.google.com/citations?hl=en&amp;user=W8VIEZgAAAAJ&amp;pagesize=1000&amp;sortby=pubdate" target=_blank rel="external nofollow noopener noreferrer">Ross Girshick</a>, <a href=https://kaiminghe.github.io/ target=_blank rel="external nofollow noopener noreferrer">Kaiming He</a></p><p>发表时间: (Arxiv 2021) 技术报告 (2页)</p></blockquote><p>MoCo v2发现SimCLR里的那些技术都是即插即用型的，引入了mlp projection head以及使用更多的数据增强，就又刷新ImageNet 上的最好成绩。</p><blockquote><p>加了一个 mlp 层</p><blockquote><p>没有 batch norm ；直接 fc + Relu + fc + Relu</p></blockquote><p>加了更多的数据增强</p><p>训练的时候用了cosine的 learning rate schedule</p><p>训练更长的 epoch，从200变到了800</p></blockquote><h2 id=moco-v3 class=heading-element><span>MoCo-V3</span>
<a href=#moco-v3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2104.02057 target=_blank rel="external nofollow noopener noreferrer">An Empirical Study of Training Self-Supervised Vision Transformers</a> <a href=https://www.semanticscholar.org/paper/An-Empirical-Study-of-Training-Self-Supervised-Chen-Xie/739ceacfafb1c4eaa17509351b647c773270b3ae target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者: <a href=https://xinleic.xyz/ target=_blank rel="external nofollow noopener noreferrer">Xinlei Chen</a>, Saining Xie, <a href=https://kaiminghe.github.io/ target=_blank rel="external nofollow noopener noreferrer">Kaiming He</a></p><p>发表时间: (ICCV 2021)</p><p>MoCo v2 + SimSiam</p><p><a href=https://github.com/facebookresearch/moco-v3 target=_blank rel="external nofollow noopener noreferrer">offical code</a></p></blockquote><center><img src="/images/Contrastive learning/MoCo.assets/mocov3_algorithm.png" width=600><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">MoCoV3 pytorch 算法</div></center><p>残差网络换成 ViT</p><blockquote><p>当这个 batch size 变大了以后曲线会抖动，效果变差
方法：观察了一下训练时，每一层这个回传梯度的情况；发现每次 loss 有大幅震动
导致这个准确度大幅下降的时候，梯度也会有一个波峰（发生在第一层）</p><p>第一层：patch projection
解决：随机初始化了一个 patch projection 层；然后冻结使得整个训练过程中都不变</p></blockquote></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-07 18:22:27">更新于 2023-06-07&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/ data-title=Moco data-hashtags="Deep Learning,对比学习"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/ data-title=Moco><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/ class=post-tag title="标签 - 对比学习">对比学习</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/contrastive-learning/simclr/ class=post-nav-item rel=prev title=SimCLR><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>SimCLR</a><a href=/posts/deeplearning/contrastive-learning/instdisc/ class=post-nav-item rel=next title=InstDisc>InstDisc<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/></a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>