<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>BYOL - fengchen</title><meta name=author content="fengchen"><meta name=description content="BYOL"><meta name=keywords content='Deep Learning,对比学习'><meta itemprop=name content="BYOL"><meta itemprop=description content="BYOL"><meta itemprop=datePublished content="2023-06-07T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-07T18:22:27+08:00"><meta itemprop=wordCount content="2610"><meta itemprop=keywords content="Deep Learning,对比学习"><meta property="og:url" content="https://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="BYOL"><meta property="og:description" content="BYOL"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-07T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-07T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="对比学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="BYOL"><meta name=twitter:description content="BYOL"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=https://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/ title="BYOL - fengchen"><link rel=prev type=text/html href=https://fengchen321.github.io/posts/deeplearning/contrastive-learning/instdisc/ title=InstDisc><link rel=next type=text/html href=https://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/ title=VILT><link rel=alternate type=text/markdown href=https://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/index.md title="BYOL - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"BYOL","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fengchen321.github.io\/posts\/deeplearning\/contrastive-learning\/byol\/"},"genre":"posts","keywords":"Deep Learning, 对比学习","wordcount":2610,"url":"https:\/\/fengchen321.github.io\/posts\/deeplearning\/contrastive-learning\/byol\/","datePublished":"2023-06-07T18:22:27+08:00","dateModified":"2023-06-07T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"BYOL"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>BYOL</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-07 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-07>2023-06-07</time></span>&nbsp;<span title="2610 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2700 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 6 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#swav>SwAV</a><ul><li><a href=#methods>methods</a></li><li><a href=#multi-crop><strong>multi crop</strong></a></li></ul></li><li><a href=#byol>BYOL</a><ul><li><a href=#标题>标题</a></li><li><a href=#methods-1>methods</a></li><li><a href=#推荐阅读>推荐阅读</a></li></ul></li><li><a href=#simsiam>SimSiam</a><ul><li><a href=#methods-2>methods</a></li></ul></li><li><a href=#barlow-twins>Barlow Twins</a><ul><li><a href=#methods-3>methods</a></li></ul></li><li><a href=#dino>DINO</a><ul><li><a href=#methods-4>methods</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 id=swav class=heading-element><span>SwAV</span>
<a href=#swav class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2006.09882 target=_blank rel="external nofollow noopener noreferrer">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</a> <a href=https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin</p><p>发表时间：(NIPS 2020)</p><p>对比学习和聚类结合</p></blockquote><h3 id=methods class=heading-element><span>methods</span>
<a href=#methods class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>给定同样一张图片，如果生成不同的视角，不同的 views 的话，希望可以用一个视角得到的特征去预测另外一个视角得到的特征</p><center><img src="/images/Contrastive learning/BYOL.assets/SwAV.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">SwAV 网路</div></center><blockquote><p>左边：一个图片 $ X$，做两次数据增强得到了$X_1、X_2$，然后所有的样本通过一个编码器 $f_{\theta}$，输出一个特征$Z_1、Z_2$，用这些特征做一个对比学习的 loss</p><blockquote><p>MoCo从memory bank取负样本6万个：这是一种近似做法</p><p>直接拿所有图片的特征跟特征做对比有点原始而且有点费资源</p></blockquote><p>SwAV：跟聚类的中心 $C$ (prototype) 比</p><blockquote><p>C 的维度是$d\times k$，d是特征的维度，k是聚类中心个数3,000</p></blockquote><p>一个图片 $ X$，做两次数据增强得到了$X_1、X_2$，然后所有的样本通过一个编码器 $f_{\theta}$，输出一个特征$Z_1、Z_2$，先通过clustering让特征 $Z$ 和prototype $C$ 生成目标$Q_1、Q_2$；C点乘$Z_1$去预测$Q_2$，换位预测</p></blockquote><h3 id=multi-crop class=heading-element><span><strong>multi crop</strong></span>
<a href=#multi-crop class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>思想：全局的和这个局部的特征都要关注</p></blockquote><p>过去的方法：用的两个crop，一个正样本对$X_1、X_2$两个图片</p><blockquote><p>一个图片$X$，先把它resize 到$256\times 256$，然后随机crop两个$224\times 224$的图片当成 $X_1、X_2$</p></blockquote><p>SwAV：大的crop抓住的是整个场景的特征，如果更想学习这些局部物体的特征，最好能多个 crop，去图片里crop一些区域，这样就能关注到一些局部的物体</p><blockquote><p>但是增加crop，会增加模型的计算复杂度，因为相当于使用了更多的正样本</p><p>进行取舍：把这个crop变得小一点，变成160 ，取2个160的crop去学全局的特征；然后为了增加正样本的数量，为了学一些局部的特征，再去随机选4个小一点crop，大小为$96\times96$</p></blockquote><center><img src="/images/Contrastive learning/BYOL.assets/SwAV_multi_crop.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">SwAV_multi_crop 实验</div></center><blockquote><p>基线模型 2 个$224\times224$，multi crop 2个$160\times160$+4个$96\times96$</p><p>SimCLR+ multi crop 涨了2.4个点，如果把 multi crop这个技术用到 BYOL 上有可能BYOL会比SwAV的效果高</p><p>如果没有这个multi crop的这个技术其实SwAV的性能也就跟MoCo v2是差不多的</p></blockquote><h2 id=byol class=heading-element><span>BYOL</span>
<a href=#byol class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2006.07733 target=_blank rel="external nofollow noopener noreferrer">Bootstrap your own latent: A new approach to self-supervised Learning</a> <a href=https://www.semanticscholar.org/paper/Bootstrap-Your-Own-Latent%3A-A-New-Approach-to-Grill-Strub/38f93092ece8eee9771e61c1edaf11b1293cae1b target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec</p><p>发表时间：(2020)</p><p>没有负样本</p><p><a href=https://github.com/open-mmlab/mmselfsup/blob/master/mmselfsup/models/algorithms/byol.py target=_blank rel="external nofollow noopener noreferrer">openmmlab</a></p></blockquote><h3 id=标题 class=heading-element><span>标题</span>
<a href=#%e6%a0%87%e9%a2%98 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p><strong>Bootstrap your own latent: A new approach to self-supervised Learning</strong></p><blockquote><p>Bootstrap: If you <strong>bootstrap</strong> an organization or an activity, you set it up or achieve it alone, using very few resources.</p><p>latent: 特征 hidden、feature、embedding</p></blockquote><p>只有正样本；目的：让所有相似的物体，特征也尽可能的相似</p><blockquote><p>缺陷：有一个躺平解</p><blockquote><p>如果一个模型不论什么输入，都返回同样的输出，那所有的特征都是一模一样的，loss就都是 0</p><p>而只有加上<strong>负样本的约束</strong>，不光相似的物体要有相似的特征；不相似的物体也要有不相似的特征；模型才有动力去继续学（防止模型学到这个躺平解）</p><blockquote><p>如果输出的所有特征都一样，那在负样本的 loss 无穷大；模型更新让正样本和负样本的 loss 都往下降，达到一个最优解</p></blockquote></blockquote></blockquote><h3 id=methods-1 class=heading-element><span>methods</span>
<a href=#methods-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Contrastive learning/BYOL.assets/BYOL_1.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">BYOL 网络流程</div></center><p><strong>前向过程</strong></p><ul><li><p>一个mini-batch 式的图片 $x$，做两次数据增强得到了$v、v&rsquo;$;</p></li><li><p>$v$ 通过编码器 $f_\theta$ 得到特征$y_\theta$；$v&rsquo;$ 通过编码器 $f_\xi$ 得到特征$y&rsquo;_\xi$；输出2048维(ResNet50)</p><blockquote><p>$f_\theta$ 和 $f_\xi$ 使用同样的网络架构(ResNet50)；参数不同。$f_\theta$ 随着梯度更新而更新；$f_\xi$ 跟 MoCo 一样，使用动量编码器，以 moving average 形式更新</p></blockquote></li><li><p>$y_\theta$通过 $g_\theta$ 得到特征$z_\theta$； $y&rsquo;<em>\xi$ 通过 $g</em>\xi$ 得到特征$z&rsquo;_\xi$；输出256维</p><blockquote><p>$g_\theta$ 和 $g_\xi$ 使用同样的网络架构 (fc + BN+ ReLU + fc )；参数不同</p><p>SimCLR 使用projection head 输出是128维
BYOL使用projector 输出是256维 （两者都是MLP层）</p></blockquote></li><li><p>$z_\theta$ 通过 $q_\theta$ 得到新的特征 $q_\theta (z_\theta)$； $q_\theta (z_\theta)$ 和 $sg(z&rsquo;_\xi)$ 尽可能一致</p><blockquote><p>sg：stop gradient</p><p>$g_\theta$ 和 $q_\theta$ 使用同样的网络架构</p><p>用自己一个视角的特征去预测另外一个视角的特征</p></blockquote></li><li><p>2048维的 $y_\theta$​ 做下游任务；损失函数：mean square error loss</p></li></ul><center><img src="/images/Contrastive learning/BYOL.assets/BYOL_2.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">BYOL草图</div></center><h3 id=推荐阅读 class=heading-element><span>推荐阅读</span>
<a href=#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p><a href=https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/ target=_blank rel="external nofollow noopener noreferrer">Understanding self-supervised and contrastive learning with &ldquo;Bootstrap Your Own Latent&rdquo;(BYOL)</a></p><blockquote><p>跟BN后的平均图片mode 做对比</p><p>使用 BN 会产生样本信息泄漏</p></blockquote><p><a href=https://arxiv.org/abs/2010.10241 target=_blank rel="external nofollow noopener noreferrer">原作解释：BYOL works even without batch statistics</a></p><blockquote><p>BYOL 不需要 batch norm 提供的那些 batch 的这个统计量照样能工作，回应之前博客里提出来假设</p></blockquote></blockquote><h2 id=simsiam class=heading-element><span>SimSiam</span>
<a href=#simsiam class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2011.10566 target=_blank rel="external nofollow noopener noreferrer">Exploring Simple Siamese Representation Learning</a> <a href=https://www.semanticscholar.org/paper/Exploring-Simple-Siamese-Representation-Learning-Chen-He/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者： <a href=https://xinleic.xyz/ target=_blank rel="external nofollow noopener noreferrer">Xinlei Chen</a>, <a href=https://kaiminghe.github.io/ target=_blank rel="external nofollow noopener noreferrer">Kaiming He</a></p><p>发表时间：(2020)</p><p><a href=https://github.com/facebookresearch/simsiam target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>没有负样本，不需要大的batch size, 不需要动量编码器</p><p>可以看成是一种 EM 算法，通过这种逐步更新的方式避免模型坍塌</p></blockquote><h3 id=methods-2 class=heading-element><span>methods</span>
<a href=#methods-2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><table border=0><tr><td align=center><img src="/images/Contrastive learning/BYOL.assets/simsiam_net.png"></td><td align=center><img src="/images/Contrastive learning/BYOL.assets/simsiam_Algorithm.png"></td></tr><tr><td align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">simsiam 网络</td><td align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">算法</td></tr></table><p><strong>前向过程</strong></p><ul><li><p>一个mini-batch 式的图片 $x$，做两次数据增强得到了$x_1、x_2&rsquo;$;</p></li><li><p>$x_1, x_2$ 通过编码器 $f$ 得到特征 $z_1, z_2$ ;</p></li><li><p>$z_1,z_2$ 通过predictor $h$ 得到 $p_1,p_2$;</p></li></ul><center><img src="/images/Contrastive learning/BYOL.assets/simsiam_model_compare.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">不同的对比学习模型</div></center><blockquote><p><strong>SimCLR</strong> ：两编码器都有梯度回传；对比任务
<strong>SwAV</strong> ：没有跟负样本；跟聚类中心去比；对比任务
<strong>BYOL</strong> ：用左边呢去预测右边；同时使用了动量编码器；预测任务
<strong>SimSiam</strong> ：没有负样本，不需要大的batch size, 不需要动量编码器；预测任务</p></blockquote><center><img src="/images/Contrastive learning/BYOL.assets/simsiam_model_compare_1.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">不同的对比学习模型ImageNet实验</div></center><blockquote><p><strong>batch size</strong></p><blockquote><p>只有 MoCo v2 和 SimSiam 是可以用256的；其它工作都是要用更大的 batch size</p></blockquote><p><strong>负样本</strong></p><blockquote><p>SimCLR 和 MoCo v2 要用负样本</p></blockquote><p><strong>动量编码器</strong></p><blockquote><p>SimCLR 没有用；SimCLR v2用了
SwAV 没有用</p></blockquote><p>epoch越大，Simsiam就不行了。</p></blockquote><h2 id=barlow-twins class=heading-element><span>Barlow Twins</span>
<a href=#barlow-twins class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题： <a href=https://arxiv.org/abs/2103.03230 target=_blank rel="external nofollow noopener noreferrer">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a> <a href=https://www.semanticscholar.org/paper/Barlow-Twins%3A-Self-Supervised-Learning-via-Zbontar-Jing/8a9d84d86ac0d76e63914802f9738325c3bece9c target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者: Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, Stéphane Deny</p><p>发表时间: (ICML 2021)</p></blockquote><h3 id=methods-3 class=heading-element><span>methods</span>
<a href=#methods-3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><table border=0><tr><td align=center><img src="/images/Contrastive learning/BYOL.assets/Barlow_Twins_net.png"></td><td align=center><img src="/images/Contrastive learning/BYOL.assets/Barlow_Twins_Algorithm.png"></td></tr><tr><td align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">Barlow Twins 网络</td><td align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">算法</td></tr></table><p>损失函数</p><blockquote><p>生成了一个关联矩阵cross correlation matrix；希望这个矩阵能跟一个单位矩阵 identity matrix尽量的相似</p><blockquote><p>希望正样本的相似性尽量都逼近于1；跟别的样本相似性尽可能是0</p></blockquote></blockquote><h2 id=dino class=heading-element><span>DINO</span>
<a href=#dino class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2104.14294 target=_blank rel="external nofollow noopener noreferrer">Emerging Properties in Self-Supervised Vision Transformers</a> <a href=https://www.semanticscholar.org/paper/Emerging-Properties-in-Self-Supervised-Vision-Caron-Touvron/ad4a0938c48e61b7827869e4ac3baffd0aefab35 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者: Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin</p><p>发表时间: (2021)</p><p><a href=https://github.com/facebookresearch/dino target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>transformer加自监督</p></blockquote><p>一个完全不用任何标签信息训练出的 Vision Transformer ；如果把它的自注意力图进行可视化；发现它能非常准确的抓住每个物体的轮廓 (媲美图像分割)</p><h3 id=methods-4 class=heading-element><span>methods</span>
<a href=#methods-4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>MoCo：左边的网络叫做 query 编码器；右边叫做 key 编码器
BYOL ：左边的网络叫做 online network；右边叫做 target network
DINO ：左边的网络叫做 student network；右边叫做 teacher network</p><table border=0><tr><td align=center><img src="/images/Contrastive learning/BYOL.assets/DINO_net.png"></td><td align=center><img src="/images/Contrastive learning/BYOL.assets/DINO_algorithm.png"></td></tr><tr><td align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">DINO 网络</td><td align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">算法</td></tr></table><p>避免模型坍塌：centering 操作</p><blockquote><p>把整个 batch 里的样本都算一个均值然后减掉这个均值</p><p>MoCoV3：随机初始化了一个 patch projection 层；然后冻结使得整个训练过程中都不变</p></blockquote></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-07 18:22:27">更新于 2023-06-07&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=https://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/ data-title=BYOL data-hashtags="Deep Learning,对比学习"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/ data-title=BYOL><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/ class=post-tag title="标签 - 对比学习">对比学习</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/contrastive-learning/instdisc/ class=post-nav-item rel=prev title=InstDisc><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>InstDisc</a><a href=/posts/deeplearning/multimodal-learning/vilt/ class=post-nav-item rel=next title=VILT>VILT<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024 - 2026</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>