# Distilling Knowledge 

## Distilling knowledge 

&gt;  文章标题：[Distilling the knowledge in a neural network](https://arxiv.org/abs/1503.02531)
&gt;
&gt;  作者：Hinton G, Vinyals O, [Dean J](https://www.infoq.cn/article/rAJiubRpi9xSl_LEhI2N).
&gt;
&gt;  发表时间：(NIPS 2014)

### Distillation

$$
q_i = \frac{exp(z_i/T)}{\sum _j exp(z_j/T)}
$$



### 拓展阅读

[论文主页](https://scholar.google.com/citations?view_op=view_citation&amp;hl=zh-CN&amp;user=JicYPdAAAAAJ&amp;citation_for_view=JicYPdAAAAAJ:jlhcAiayVhoC)

**蒸馏机理论文**

&gt; [Hinton-NeurIPS2019论文：When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)
&gt;
&gt; [ICLR2021论文：Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](https://openreview.net/pdf?id=PObuuGVrGaZ) ：[知乎解读](https://zhuanlan.zhihu.com/p/387164549)
&gt;
&gt; [Does Knowledge Distillation Really Work?](https://arxiv.org/abs/2106.05945) ：[知乎解读](https://zhuanlan.zhihu.com/p/403585095)

[标签平滑和知识蒸馏的关系](https://www.reddit.com/r/deeplearning/comments/rqkcj1/knowledge_distillation_model_ensemble_and_its/)

**发展趋势**

&gt; 多老师多学生
&gt;
&gt; 知识的表示、数据集蒸馏、对比学习：
&gt;
&gt; &gt; Attention Transfer论文：https://arxiv.org/abs/1612.03928
&gt; &gt;
&gt; &gt; [Dataset Distillation](https://arxiv.org/abs/1811.10959)
&gt;
&gt; 多模态、知识图谱、预训练大模型的知识蒸馏

**论文解读**

[Knowledge Distillation 百科](https://devopedia.org/knowledge-distillation#summary)

[Knowledge distillation in deep learning and its applications](https://peerj.com/articles/cs-474/)

[Knowledge Distillation: Principles, Algorithms, Applications](https://neptune.ai/blog/knowledge-distillation)

[Hinton官方PPT](https://zhuanlan.zhihu.com/p/75031938)

[知乎：知识蒸馏是什么？一份入门随笔——公式（2）的详细推导](https://zhuanlan.zhihu.com/p/90049906)

[知乎：【经典简读】知识蒸馏(Knowledge Distillation) 经典之作](https://zhuanlan.zhihu.com/p/102038521)

[知乎：哪位来聊聊深度学习知识蒸馏（knowledge distillation）？](https://www.zhihu.com/question/266035318/answer/2276286571)

[博客解读1](https://medium.com/analytics-vidhya/knowledge-distillation-in-a-deep-neural-network-c9dd59aff89b)

[博客解读2](https://raviteja-ganta.github.io/distilling-the-knowledge-in-a-neural-network)

[博客解读3](https://blog.csdn.net/nature553863/article/details/80568658)

[博客解读4](https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764)

[知乎:陀飞轮](https://www.zhihu.com/people/chen-jia-yu-65-36/answers)

[代码库](https://github.com/HobbitLong/RepDistiller)



---

> 作者: fengchen  
> URL: https://fengchen321.github.io/posts/deeplearning/light-weight/distilling-knowledge-/  

