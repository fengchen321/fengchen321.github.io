<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Transformer - fengchen</title><meta name=author content="fengchen"><meta name=description content="Transformer"><meta name=keywords content='Deep Learning,Transformer,NLP'><meta itemprop=name content="Transformer"><meta itemprop=description content="Transformer"><meta itemprop=datePublished content="2023-06-10T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-10T18:22:27+08:00"><meta itemprop=wordCount content="3740"><meta itemprop=keywords content="Deep Learning,Transformer,NLP"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="Transformer"><meta property="og:description" content="Transformer"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-10T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-10T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformer"><meta name=twitter:description content="Transformer"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/ title="Transformer - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_cv/mae/ title=MAE><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/ title=BERT><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/index.md title="Transformer - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Transformer","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/transformer_nlp\/transformer\/"},"genre":"posts","keywords":"Deep Learning, Transformer, NLP","wordcount":3740,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/transformer_nlp\/transformer\/","datePublished":"2023-06-10T18:22:27+08:00","dateModified":"2023-06-10T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"Transformer"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Transformer</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-10 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-10>2023-06-10</time></span>&nbsp;<span title="3740 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3800 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 8 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a></li><li><a href=#model-architecture>Model Architecture</a><ul><li><a href=#encoder>Encoder</a><ul><li><a href=#attention>Attention</a><ul><li><a href=#scaled-dot-product-attention>Scaled Dot-product Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li></ul></li></ul></li><li><a href=#decoder>Decoder</a></li><li><a href=#the-final-linear-and-softmax-layer><strong>The Final Linear and Softmax Layer</strong></a></li></ul></li><li><a href=#拓展阅读>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=transformer class=heading-element><span>Transformer</span>
<a href=#transformer class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1706.03762# target=_blank rel="external nofollow noopener noreferrer">Attention Is All You Need</a>
作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
发表时间：(NIPS 2017)</p><p>继MLP、CNN、RNN后的第四大类架构</p></blockquote><h2 id=introduction class=heading-element><span>Introduction</span>
<a href=#introduction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>sequence transduction:</strong> 序列转录，序列到序列的生成。input一个序列，output一个序列。</p><blockquote><p>机器翻译：输入一句中文，输出一句英文。</p></blockquote><p>RNN ：从左往右一步一步计算，对第 t 个状态 $h_t$，由 $h_{t-1}$（历史信息）和 当前词 t 计算。</p><blockquote><p>难以并行。</p><blockquote><p>通过 factorization 分解 tricks 和 conditional computation 并行化来提升计算效率</p></blockquote><p>过早的历史信息可能被丢掉。时序信息是一步一步往后传递的</p><blockquote><p>时序长的时候一个大的 $h_t$存历史信息。每一个 计算步都需要存储，内存开销大</p></blockquote></blockquote><h2 id=background class=heading-element><span>Background</span>
<a href=#background class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>CNN（局部像素&ndash;>全部像素；多通道 &ndash;> multi-head）</p><blockquote><p>Transformer 的 attention mechanism 每一次看到所有的像素，一层能够看到整个序列。</p><p>Transformer 的 multi-head self-attention 模拟 CNNs 多通道输出的效果。</p></blockquote><p>自注意力，是一种将单个序列的不同位置关联起来以计算序列表示的注意力机制</p><h2 id=model-architecture class=heading-element><span>Model Architecture</span>
<a href=#model-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Transformer_NLP/Transformer.assets/The Transformer - model architecture.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">The Transformer - model architecture</div></center><p>先将输入<strong>Input</strong>使用<a href=https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca target=_blank rel="external nofollow noopener noreferrer"><strong>embedding algorithm</strong></a>转成向量。</p><blockquote><p>编码器的都会接收到一个list（每个元素都是512维的词向量）。list的尺寸是可以设置的超参，通常是训练集的最长句子的长度。</p></blockquote><p>加入位置编码<strong>Positional Encoding</strong></p><blockquote><p>RNN ：把上一时刻的输出 作为下一个时刻的输入，来传递时序信息</p><p>Attention： 在输入里面加入时序信息 &ndash;> positional encoding</p><blockquote><p>output 是 value 的加权和（权重是 query 和 key 之间的距离，和序列信息无关）</p><p>一个词在嵌入层表示成一个 512 维的向量，用另一个 512 维的向量来表示一个数字代表位置信息</p><center><img src=/images/Transformer_NLP/Transformer.assets/Transformer_positional_Encoding.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Transformer_positional_Encoding</div></center><blockquote><p>positional encoding 是 cos 和 sin 的一个函数，在 [-1, +1] 之间抖动的。</p><blockquote><p>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$</p><p>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$</p><p>矩阵第pos行第2i列；行代表词元在序列中的位置，列代表位置编码的不同维度</p><p>为啥设计这样的函数，参考<a href=https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html target=_blank rel="external nofollow noopener noreferrer">位置编码</a></p></blockquote><p>$input\ embedding * \sqrt{d_{model}}$</p><blockquote><p>学 embedding 的时候，会把每一个向量的 L2 Norm 学的比较小。</p><p>乘上$\sqrt{d_{model}}$使得 embedding 和 positional encoding 的 scale 也是在差不多的 [-1, +1] 数值区间，可以做加法</p></blockquote></blockquote></blockquote></blockquote><p>加入位置编码后再进行dropout=0.1。</p><h3 id=encoder class=heading-element><span>Encoder</span>
<a href=#encoder class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>Transformer的编码器是由多(N=6)个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。</p><blockquote><p>第一个子层是**[多头自注意力](#####Multi-Head Attention)（multi-head self-attention）**；</p><blockquote><p>输入key、value 和 query 其实就是一个东西，就是自己本身</p></blockquote><p>第二个子层是基于位置的前馈网络（position-wise feed-forward network）。</p><blockquote><p>作用在最后一个维度的 <strong>MLP</strong></p><p>Point-wise: 把一个 MLP 对每一个词 （position）作用一次，对每个词作用的是是同一个多层感知机（MLP）</p><p>$FFN(x)=max(0,xW_1+b_1)W_2+b_2$：512&ndash;>2048&ndash;>512</p></blockquote></blockquote><p>每个子层都采用了残差连接（residual connection）和层规范化（layer normalization）</p><blockquote><p>$LayerNorm(x+Sublayer(x))$</p><center><img src=/images/Transformer_NLP/Transformer.assets/Transformer_LayerNor.png height=400><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Transformer_LayerNor</div></center><p>residual connections 需要输入输出维度一致，不一致需要做投影。简单起见，固定每一层的输出维度$d_{model }$= 512</p><blockquote><p>简单设计：只需调 2 个参数: $d_{model }$ 每层维度有多大 和 N 多少层，影响后续一系列网络的设计，BERT、GPT。</p></blockquote><p>层规范化（layer normalization）</p><blockquote><img src=/images/Transformer_NLP/Transformer.assets/transformer_LN.png><p>H：句长，W：词向量长 N：Batch</p><p>Layer Normalization：是在一个句上的进行归一化。</p><p>Batch Normalization：是把每个Batch中每句话的第一个字的同一维度看成一组做归一化。</p><p>LayerNorm 每个样本自己算均值和方差，不需要存全局的均值和方差。</p><p>LayerNorm 更稳定，不管样本长还是短，均值和方差是在每个样本内计算。</p></blockquote></blockquote><h4 id=attention class=heading-element><span>Attention</span>
<a href=#attention class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>注意力函数是 一个将一个 query 和一些 key - value 对 映射成一个输出的函数，其中所有的 query、key、value 和 output 都是一些向量。</p><blockquote><p>output 是 value 的一个加权和 &ndash;> 输出的维度 == value 的维度。</p><p>query改变，权值分配不一样，输出不一样</p></blockquote><p>query 和 key 的长度是等长的，都等于 dk。value 的维度是 dv，输出也是 dv。</p><p>query 和 key 可以不等长，可用加性的注意力机制处理。</p><center><img src=/images/Transformer_NLP/Transformer.assets/Transformer_attention.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Transformer_attention</div></center><h5 id=scaled-dot-product-attention class=heading-element><span>Scaled Dot-product Attention</span>
<a href=#scaled-dot-product-attention class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><center><img src=/images/Transformer_NLP/Transformer.assets/Transformer_attention_1.png width=1000><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Transformer_attention</div></center><p>注意力的具体计算是：对每一个 query 和 key 做内积，然后把它作为相似度。</p><blockquote><p><strong>两个向量做内积：用来衡量两向量的相似度。内积的值越大，它的余弦值越大，这两个向量的相似度就越高。如果你的内积的值为 0 ，这两个向量正交了，没有相似度。</strong></p></blockquote>$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt {d_k}})V
$$<p>一个 query 对所有 key 的内积值，然后再除以$\sqrt{d_k}$， 再做 softmax。 softmax 是对每一行的值做 softmax，然后每一行之间是独立的，会得到权重。</p><blockquote><p>除以$\sqrt{d_k}$：防止softmax函数的梯度消失。</p><blockquote><p>2 个向量的长度比较长的时候，点积的值可能会比较大，相对的差距会变大，导致最大值 softmax会更加靠近于1，剩下那些值就会更加靠近于0。值就会更加向两端靠拢，算梯度的时候，梯度比较小。</p></blockquote></blockquote><p><strong>Mask机制</strong></p><blockquote><p><strong>padding mask</strong>：对输入序列进行对齐。</p><blockquote><p>具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。</p><p>操作和Sequence mask一致。</p></blockquote><p><font color=#e16f00><strong>Sequence mask</strong></font>：避免在 t 时刻，看到 t 时刻以后的东西。(选择使用，在decoder时使用)</p><blockquote><p>操作实现：把$ Q_t $和 $K_t $和他们之后的值换成一个很大的负数，进入 softmax 后，权重为0。</p><p>和 V 矩阵做矩阵乘法时，没看到 t 时刻以后的内容，只看 t 时刻之前的 key - value pair。</p><p>mask是个 0 1矩阵，和attention（scale QK）size一样，t 时刻以后 mask 为 0。</p></blockquote></blockquote><h5 id=multi-head-attention class=heading-element><span>Multi-Head Attention</span>
<a href=#multi-head-attention class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><ol><li><p>多头机制扩大了模型对不同位置的关注能力</p></li><li><p>多头机制赋予attention多种子表达方式</p><blockquote><p>先投影到低维，投影的 w 是可以学习的；multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。</p></blockquote></li></ol><p>输入：原始的 value、key、query</p><p>进入一个Linear层，把 value、key、query 投影到比较低的维度。然后再做一个 scaled dot product 。执行 h 次会得到 h 个输出，再把 h 个 输出向量全部合并 concat 在一起，最后做一次线性的投影 Linear。</p><blockquote><p>投影维度 $d_v = d_{model} / h = 512 / 8 = 64$，每个 head 得到 64 维度，concat，再投影回 $d_{model}$。</p></blockquote><center><img src=/images/Transformer_NLP/Transformer.assets/Transformer_multi-headed_self-attention.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">concat过程</div></center><h3 id=decoder class=heading-element><span>Decoder</span>
<a href=#decoder class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>Decoder 是 auto-regressive 自回归。当前时刻的输入是之前一些时刻的输出。做预测时，decoder 不能看到之后时刻的输出。</p><p>Transformer解码器也是由多(N=6)个相同的层叠加而成的，每个层都有三个子层（子层表示为sublayer）。</p><p>attention mechanism 每一次能看完完整的输入，要避免这个情况的发生。</p><blockquote><p>第一个子层是<strong>带掩码的多头自注意力（Masked multi-head self-attention）</strong>；</p><blockquote><p>输入qkv复制 3 份</p><p>masked 体现在，在预测第 t 个时刻的输出的时候，看不到 t 时刻以后的输入,具体操作看<a href=####Attention>Mask机制</a>，两个Mask相加。</p><p>保留了自回归（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p></blockquote><p>第二个子层是**[多头自注意力](#####Multi-Head Attention)（multi-head self-attention）**；</p><blockquote><p>不再是 self-attention。</p><p><strong>key - value</strong> 来自 encoder 的输出。 <strong>query</strong> 是来自 decoder 里 masked multi-head attention 的输出。</p><p>attention：query 注意到当前的 query 感兴趣的东西，对当前的 query的不感兴趣的内容，可以忽略掉。</p><blockquote><p>在 encoder 和 decoder 之间传递信息</p></blockquote></blockquote><p>第三个子层是基于位置的前馈网络（position-wise feed-forward network）。</p></blockquote><p>每个子层都采用了残差连接（residual connection）和层规范化（layer normalization）</p><p>关于序列到序列模型（sequence-to-sequence model），在训练阶段，其输出序列的所有位置的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，只有生成的词元才能用于解码器的自注意力计算中。流程如下（包含解码器Decoder的shifted right 输入状况）：</p><center><img src=/images/Transformer_NLP/Transformer.assets/transformer_decoding_1.gif><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">decoder_step1</div></center><center><img src=/images/Transformer_NLP/Transformer.assets/transformer_decoding_2.gif><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Decoder_step_end</div></center><h3 id=the-final-linear-and-softmax-layer class=heading-element><span><strong>The Final Linear and Softmax Layer</strong></span>
<a href=#the-final-linear-and-softmax-layer class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>线性层是个简单的全连接层，将解码器的最后输出映射到一个非常大的logits向量上。</p><blockquote><p>假设模型已知有1万个单词（输出的词表）从训练集中学习得到。那么，logits向量就有1万维，每个值表示是某个词的可能倾向值。</p></blockquote><p>softmax层将这些分数转换成概率值（都是正值，且加和为1），最高值对应的维上的词就是这一步的输出单词。</p><center><img src=/images/Transformer_NLP/Transformer.assets/transformer_decoder_output_softmax.png></center><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=http://nlp.seas.harvard.edu/2018/04/03/attention.html target=_blank rel="external nofollow noopener noreferrer">哈佛注释版：The Annotated Transformer</a></p><p><a href=https://arxiv.org/abs/2108.07258 target=_blank rel="external nofollow noopener noreferrer">斯坦福100+作者的200+页综述</a></p><p><a href=https://arxiv.org/pdf/1911.07013.pdf target=_blank rel="external nofollow noopener noreferrer">对LayerNorm的新研究</a></p><p><a href=https://arxiv.org/abs/2103.03404 target=_blank rel="external nofollow noopener noreferrer">对Attention在Transformer里面作用的研究</a></p><p><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.788" target=_blank rel="external nofollow noopener noreferrer">B站：Transformer论文逐段精读【论文精读】</a></p><p><a href="https://www.bilibili.com/video/BV15v411W78M?spm_id_from=333.999.0.0&amp;vd_source=d28e92983881d85b633a5acf8e46efaa" target=_blank rel="external nofollow noopener noreferrer">B站：Transformer中Self-Attention以及Multi-Head Attention详解</a></p><p><a href="https://www.bilibili.com/video/BV1SK4y1d7Qh?spm_id_from=333.999.0.0" target=_blank rel="external nofollow noopener noreferrer">B站：Transformer模型(1/2): 剥离RNN，保留Attention</a></p><p><a href=http://jalammar.github.io/illustrated-transformer/ target=_blank rel="external nofollow noopener noreferrer">The Illustrated Transformer</a></p><p><a href=https://zhuanlan.zhihu.com/p/366014410 target=_blank rel="external nofollow noopener noreferrer">Transformer 论文详细解读:多配图</a></p><p><a href=https://blog.csdn.net/qq_37541097/article/details/117691873 target=_blank rel="external nofollow noopener noreferrer">详解Transformer中Self-Attention以及Multi-Head Attention</a></p><p><a href=https://zhuanlan.zhihu.com/p/403433120 target=_blank rel="external nofollow noopener noreferrer">知乎：【Transformer】10分钟学会Transformer | Pytorch代码讲解 | 代码可运行</a></p><p><a href=https://www.zhihu.com/question/325839123 target=_blank rel="external nofollow noopener noreferrer">知乎：深度学习attention机制中的Q,K,V分别是从哪来的？</a></p><p><a href=https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#can-kao-zi-liao target=_blank rel="external nofollow noopener noreferrer">芦苇的机器学习笔记：Self-Attention和Transformer</a></p><p><a href=https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html target=_blank rel="external nofollow noopener noreferrer">李沐：动手学深度学习——10.7. Transformer</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-10 18:22:27">更新于 2023-06-10&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/ data-title=Transformer data-hashtags="Deep Learning,Transformer,NLP"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/ data-title=Transformer><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/transformer/ class=post-tag title="标签 - Transformer">Transformer</a><a href=/tags/nlp/ class=post-tag title="标签 - NLP">NLP</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/transformer_cv/mae/ class=post-nav-item rel=prev title=MAE><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>MAE</a><a href=/posts/deeplearning/transformer_nlp/bert/ class=post-nav-item rel=next title=BERT>BERT<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>