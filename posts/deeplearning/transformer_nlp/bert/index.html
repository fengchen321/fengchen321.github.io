<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>BERT - fengchen</title><meta name=author content="fengchen"><meta name=description content="BERT"><meta name=keywords content='Deep Learning,Transformer,NLP'><meta itemprop=name content="BERT"><meta itemprop=description content="BERT"><meta itemprop=datePublished content="2023-06-10T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-10T18:22:27+08:00"><meta itemprop=wordCount content="3146"><meta itemprop=keywords content="Deep Learning,Transformer,NLP"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="BERT"><meta property="og:description" content="BERT"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-10T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-10T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="BERT"><meta name=twitter:description content="BERT"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/ title="BERT - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/ title=Transformer><link rel=next type=text/html href=http://fengchen321.github.io/posts/computer/linux%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/ title=Linx常用指令><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/index.md title="BERT - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"BERT","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/transformer_nlp\/bert\/"},"genre":"posts","keywords":"Deep Learning, Transformer, NLP","wordcount":3146,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/transformer_nlp\/bert\/","datePublished":"2023-06-10T18:22:27+08:00","dateModified":"2023-06-10T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"BERT"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>BERT</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-10 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-10>2023-06-10</time></span>&nbsp;<span title="3146 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 3200 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 7 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related Work</a></li><li><a href=#bert-1>Bert</a><ul><li><a href=#inputoutput-representation预训练微调共通部分>Input/Output Representation(预训练&微调共通部分）</a></li><li><a href=#pre-training-bert>Pre-training BERT</a><ul><li><a href=#mlm>MLM</a></li><li><a href=#nsp>NSP</a></li></ul></li><li><a href=#fine-tuning-bert>Fine-tuning BERT</a></li></ul></li><li><a href=#拓展阅读>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=bert class=heading-element><span>BERT</span>
<a href=#bert class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1810.04805 target=_blank rel="external nofollow noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
作者：Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
发表时间：(NAACL-HLT 2019)</p><p><a href=https://github.com/google-research/bert target=_blank rel="external nofollow noopener noreferrer">官方代码</a></p><p>==Transformer一统NLP的开始==</p></blockquote><p>BERT: 用深的、双向的、transformer 来做预训练，用来做语言理解的任务。</p><blockquote><p>pre-training: 在一个大的数据集上训练好一个模型 pre-training，模型的主要任务是用在其它任务 training 上</p><p>deep bidirectional transformers: 深的双向 transformers</p><p>language understanding: 更广义，transformer 主要用在机器翻译 MT</p></blockquote><h2 id=abstract class=heading-element><span>Abstract</span>
<a href=#abstract class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>新的语言表征模型 BERT: <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers</p><blockquote><center><img src=/images/Transformer_NLP/BERT.assets/Elmo_GPT_Bert.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Elmo_GPT_Bert</div></center><p><a href=https://arxiv.org/abs/1802.05365v2 target=_blank rel="external nofollow noopener noreferrer">ELMo</a>：使用左右侧的上下文信息 ；基于RNN，应用下游任务需要一点点调整架构</p><p>GPT：使用左边的上下文信息，预测未来</p><p>BERT：使用左右侧的上下文信息 ；基于Transformer，应用下游任务只需要调整最上层</p><blockquote><p>从无标注的文本中（jointly conditioning 联合左右的上下文信息）预训练得到无标注文本的 deep bidirectional representations</p><p>BERT = ELMo 的 bidirectional 信息 + GPT 的新架构 transformer</p></blockquote></blockquote><h2 id=introduction class=heading-element><span>Introduction</span>
<a href=#introduction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>NLP任务分两类</p><blockquote><p>sentence-level tasks ：句子情绪识别、两个句子的关系；</p><p>token-level tasks ：NER (人名、街道名) 需要 fine-grained output</p></blockquote><p>BERT训练方法</p><blockquote><p>通过 MLM 带掩码的语言模型作为预训练的目标，来减轻语言模型的单向约束。inspired by the Close task 1953</p><p>**MLM ** (masked language model)：每次随机选输入的词源 tokens, 然后 mask 它们，目标函数是预测被 masked 的词；15%的词汇mask</p><blockquote><blockquote><p>假设输入里面的第二个词汇是被盖住的，把其对应的embedding输入到一个多分类模型中，来预测被盖住的单词。类似挖空填词、完形填空</p></blockquote><p>standard language model：只看左边的信息</p></blockquote><p><strong>NSP</strong>: (next sentence prediction )：预测下一个句子；判断两个句子是随机采样的 or 原文相邻，学习sentence-level 的信息。</p><blockquote><p>把两句话连起来，中间加一个[SEP]作为两个句子的分隔符。而在两个句子的开头，放一个[CLS]标志符，将其得到的embedding输入到二分类的模型，输出两个句子是不是接在一起的。</p></blockquote></blockquote><p>在训练BERT的时候，这两个任务是同时训练的。所以，BERT的损失函数是把这两个任务的损失函数加起来的，是一个「多任务」训练</p><p><strong>贡献</strong></p><blockquote><p>bidirectional 双向信息的重要性</p><p>BERT 首个微调模型，在 sentence-level and token-level task效果好</p><p>好的预训练模型，不用对特定任务做一些模型架构的改动</p></blockquote><h2 id=related-work class=heading-element><span>Related Work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>Unsupervised Feature-based approaches</p><blockquote><p>非监督的基于特征表示的工作：词嵌入、ELMo等</p></blockquote><p>Unsupervised Fine-tuning approaches</p><blockquote><p>非监督的基于微调的工作：GPT等</p></blockquote><p>Transfer Learning from Supervised Data</p><blockquote><p>在有标签的数据上做迁移学习。</p></blockquote><h2 id=bert-1 class=heading-element><span>Bert</span>
<a href=#bert-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>预训练 + 微调</strong></p><center><img src=/images/Transformer_NLP/BERT.assets/bert_stage.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">bert_stage</div></center><blockquote><p>pre-training：使用 unlabeled data 训练</p><p>fine-tuning：微调的 BERT 使用预训练的参数 初始化，所有的权重参数通过下游任务的 labeled data 进行微调。</p><p>每一个下游任务会创建一个 新的 BERT 模型，（由预训练参数初始化），但每一个下游任务会根据自己任务的labeled data 来微调自己的 BERT 模型。</p></blockquote><table><thead><tr><th style=text-align:center>model name</th><th style=text-align:center>L</th><th style=text-align:center>H</th><th style=text-align:center>A</th><th style=text-align:center>Total Parameters</th></tr></thead><tbody><tr><td style=text-align:center>$BERT_{base}$</td><td style=text-align:center>12</td><td style=text-align:center>768</td><td style=text-align:center>12</td><td style=text-align:center>110M</td></tr><tr><td style=text-align:center>$BERT_{base}$</td><td style=text-align:center>24</td><td style=text-align:center>1024</td><td style=text-align:center>16</td><td style=text-align:center>340M</td></tr></tbody></table><blockquote><p>L：transform blocks的个数
H：hidden size 隐藏层大小
A：自注意力机制 multi-head 中 head 头的个数</p></blockquote><p>BERT 模型复杂度和层数 L 是 linear, 和宽度 H 是 平方关系。
深度变成了以前的两倍，在宽度上面也选择一个值，使得这个增加的平方大概是之前的两倍。</p><blockquote><p>$H_{large}=\sqrt {2} H_{base}=\sqrt 2 \times 768=1086$</p></blockquote><p>H = 16，因为每个 head 的维度都固定在了64。所以宽度增加了， head 数也增加了</p><blockquote><p>$H = 64 \times A:\ \ 768=64\times 12;\ \ 1024=64\times 16$</p></blockquote><p>嵌入层：输入字典大小30k，输出H</p><p>transformer blocks($H^2\times 12$)：self-attention($H^2\times 4$) + MLP ($H^2\times 8$)</p><blockquote><p>Transformer block:</p><blockquote><p>多头Q,K,V投影矩阵合并$H(64\times A)$+输出后再H*H投影</p></blockquote><p>MLP 的 2个全连接层：</p><blockquote><p>第一个全连接层输入是 H，输出是 4 * H；
第二个全 连接层输入是 4 * H，输出是 H。</p></blockquote></blockquote><p>$Total \ Parameters = 30K\times H + 12 \times H^2 \times L$</p><blockquote><p>$BERT_{base} = 30000\times 768 + 12 \times 768^2 \times 12 = 107.97M$</p><p>$BERT_{large} = 30000\times 1024+ 12 \times 1024^2\times 24= 332.71M$</p></blockquote><h3 id=inputoutput-representation预训练微调共通部分 class=heading-element><span>Input/Output Representation(预训练&微调共通部分）</span>
<a href=#inputoutput-representation%e9%a2%84%e8%ae%ad%e7%bb%83%e5%be%ae%e8%b0%83%e5%85%b1%e9%80%9a%e9%83%a8%e5%88%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>BERT 的输入和 transformer 区别</p><blockquote><p>transformer 预训练时候的输入是一个序列对。编码器和解码器分别会输入一个序列。
BERT 只有一个编码器，为了使 BERT 能处理两个句子的情况，需要把两个句子并成一个序列。</p></blockquote><p>BERT切词</p><blockquote><p>WordPiece, 把一个出现概率低的词切开，只保留一个词出现频率高的子序列，30k token 经常出现的词（子
序列）的字典。
否则，空格切词 &ndash;> 一个词是一个 token。数据量打的时候，词典会特别大，到百万级别。可学习的参数基
本都在嵌入层了。</p></blockquote><p>BERT 的输入序列构成 [ CLS ] + [ SEP ]</p><center><img src=/images/Transformer_NLP/BERT.assets/Input_Representation.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">输入序列</div></center><blockquote><p>Token embeddings: 词源的embedding层，整成的embedding层， 每一个 token 有对应的词向量。
Segement embeddings: 这个 token 属于第一句话 A还是第二句话 B。
Position embedding 的输入是 token 词源在这个序列 sequence 中的位置信息。（和Transformer不一样，这是学习出来的）</p></blockquote><p>BERT 的 segment embedding （属于哪个句子）和 position embedding （位置在哪里）是学习得来的，
transformer 的 position embedding 是给定的。</p><blockquote><p>序列开始:<strong>[CLS]</strong> 输出的是句子层面的信息 sequence representation</p><blockquote><p>BERT 使用的是 transformer 的 encoder，self-attention layer 会看输入的每个词和其它所有词的关系。
就算 <strong>[ CLS ]</strong> 这个词放在我的第一个的位置，他也是有办法能看到之后所有的词。所以他放在第一个是没关
系的，不一定要放在最后。</p></blockquote><p>区分两个合在一起的句子的方法：</p><blockquote><p>每个句子后 + <strong>[ SEP ]</strong> 表示 seperate
学一个嵌入层 来表示整个句子是第一句还是第二句</p></blockquote></blockquote><h3 id=pre-training-bert class=heading-element><span>Pre-training BERT</span>
<a href=#pre-training-bert class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>预训练的 key factors: 目标函数，预训练的数据</p><h4 id=mlm class=heading-element><span>MLM</span>
<a href=#mlm class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>由 WordPiece 生成的词源序列中的词源，它有 15% 的概率会随机替换成一个掩码。但是对于特殊的词源不
做替换</p><blockquote><p>15% 计划被 masked 的词：80% 的概率被替换为 [MASK], 10% 换成 random token,10% 不改变原 token。</p><p>特殊的词源：第一个词源 [ CLS ] 和中间的分割词源 [SEP]。</p><p>问题：预训练和微调看到的数据不一样</p><blockquote><p>预训练的输入序列有 15% [MASK]，微调时的数据没有 [MASK].</p></blockquote><p>为什么要Mask</p><blockquote><p>语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。</p></blockquote><p>Mask方式优缺点：</p><blockquote><p>1）被随机选择15%的词当中以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力；</p><p>2）被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）。</p><p>3）针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息。主要针对这一短板，因此google此后发表了BERT-WWM，国内的哈工大联合讯飞发表了中文版的BERT-WWM。</p></blockquote></blockquote><h4 id=nsp class=heading-element><span>NSP</span>
<a href=#nsp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>输入序列有 2 个句子 A 和 B，50% 正例，50%反例</p><blockquote><p>50% B 在 A 之后,是一对连续句子，标记为 IsNext；50% 是语料库中 a random sentence 随机采样的，标记为 NotNext。</p></blockquote><center><img src=/images/Transformer_NLP/BERT.assets/NSP.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">NSP</div></center><blockquote><p>flight ## less：flightless 出现概率不高，WordPiece 分成了 2 个出现频率高的子序列，## 表示 less 是flightless 的一部分。</p></blockquote><h3 id=fine-tuning-bert class=heading-element><span>Fine-tuning BERT</span>
<a href=#fine-tuning-bert class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>BERT 经过微小的改造（增加一个小小的层），就可以用于各种各样的语言任务。</p><p>（a,b）与 Next Sentence Prediction类似，通过在 <strong>「[CLS]」</strong> 标记的 Transformer 输出顶部添加分类层，完成诸如情感分析之类的**「分类」**任务</p><p>（c）在问答任务（例如 SQuAD v1.1）中，会收到一个关于文本序列的问题，并需要在序列中标记答案。使用 BERT，可以通过学习标记答案开始和结束的两个额外向量来训练问答模型。</p><p>（d）在命名实体识别 (NER) 中，接收文本序列，并需要标记文本中出现的各种类型的实体（人、组织、日期等）。使用 BERT，可以通过将每个标记的输出向量输入到预测 NER 标签的分类层来训练 NER 模型</p><center><img src="/images/Transformer_NLP/BERT.assets/Fine-tuning BERT.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">differernt tasks</div></center><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.bilibili.com/video/BV1PL411M7eQ?spm_id_from=333.999.0.0" target=_blank rel="external nofollow noopener noreferrer">BERT 论文逐段精读【论文精读】</a></p><p><a href="https://www.youtube.com/watch?v=UYPa347-DdE" target=_blank rel="external nofollow noopener noreferrer">李宏毅：ELMO, BERT, GPT</a></p><p><a href=https://jalammar.github.io/illustrated-bert/ target=_blank rel="external nofollow noopener noreferrer">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></p><p><a href=https://wmathor.com/index.php/archives/1456/ target=_blank rel="external nofollow noopener noreferrer">BERT 详解（附带 ELMo、GPT 介绍）</a></p><p><a href=https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html target=_blank rel="external nofollow noopener noreferrer">BERT 科普文</a></p><p><a href=https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/ target=_blank rel="external nofollow noopener noreferrer">作者对双向的回应</a></p><p><a href=https://hal.inria.fr/hal-02131630/document target=_blank rel="external nofollow noopener noreferrer">ACL 2019：What does BERT learn about the structure of language?</a>：BERT的低层网络就学习到了短语级别的信息表征，BERT的中层网络就学习到了丰富的语言学特征，而BERT的高层网络则学习到了丰富的语义信息特征</p><p><a href=https://arxiv.org/abs/1905.05950 target=_blank rel="external nofollow noopener noreferrer">BERT Rediscovers the Classical NLP Pipeline</a></p><p><a href=https://mp.weixin.qq.com/s/cDG7DwHFL1kHErwyYGT4UA target=_blank rel="external nofollow noopener noreferrer">关于BERT：你不知道的事</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-10 18:22:27">更新于 2023-06-10&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/ data-title=BERT data-hashtags="Deep Learning,Transformer,NLP"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/ data-title=BERT><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/transformer/ class=post-tag title="标签 - Transformer">Transformer</a><a href=/tags/nlp/ class=post-tag title="标签 - NLP">NLP</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/transformer_nlp/transformer/ class=post-nav-item rel=prev title=Transformer><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Transformer</a><a href=/posts/computer/linux%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/ class=post-nav-item rel=next title=Linx常用指令>Linx常用指令<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>