<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>YOLO - fengchen</title><meta name=author content="fengchen"><meta name=description content="YOLO"><meta name=keywords content='Deep Learning,目标检测'><meta itemprop=name content="YOLO"><meta itemprop=description content="YOLO"><meta itemprop=datePublished content="2023-06-04T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-04T18:22:27+08:00"><meta itemprop=wordCount content="14241"><meta itemprop=keywords content="Deep Learning,目标检测"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="YOLO"><meta property="og:description" content="YOLO"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-04T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-04T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="目标检测"><meta name=twitter:card content="summary"><meta name=twitter:title content="YOLO"><meta name=twitter:description content="YOLO"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/ title="YOLO - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/light-weight/distilling-knowledge-/ title="Distilling knowledge "><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/object-detection/ssd/ title=SSD><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/index.md title="YOLO - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"YOLO","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/object-detection\/yolo\/"},"genre":"posts","keywords":"Deep Learning, 目标检测","wordcount":14241,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/object-detection\/yolo\/","datePublished":"2023-06-04T18:22:27+08:00","dateModified":"2023-06-04T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"YOLO"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>YOLO</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-04 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-04>2023-06-04</time></span>&nbsp;<span title="14241 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 14300 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 29 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#unified-detection>Unified Detection</a><ul><li><a href=#network-design>Network design</a></li><li><a href=#training>Training</a></li><li><a href=#inference>Inference</a></li><li><a href=#limitations-of-yolo>Limitations of YOLO</a></li></ul></li><li><a href=#comparison-to-other-detection-systems>Comparison to Other Detection Systems</a></li><li><a href=#experiments>Experiments</a></li><li><a href=#real-time-detection-in-the-wild>Real-Time Detection In The Wild</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#better>Better</a><ul><li><a href=#batch-normalizationhttpsarxivorgabs150203167><a href=https://arxiv.org/abs/1502.03167>Batch Normalization</a></a></li><li><a href=#high-resolution-classifier>High Resolution Classifier</a></li><li><a href=#convolutional-with-anchor-boxes>Convolutional With Anchor Boxes</a></li><li><a href=#dimension-clusters--聚类>Dimension Clusters （聚类）</a></li><li><a href=#direct-location-prediction>Direct location prediction</a></li><li><a href=#fine-grained-features--细粒度特征>Fine-Grained Features 细粒度特征</a></li><li><a href=#multi-scale-training>Multi-Scale Training</a></li></ul></li><li><a href=#faster>Faster</a><ul><li><a href=#darknet-19>Darknet-19</a></li><li><a href=#training-for-detection>Training for detection</a></li><li><a href=#损失函数>损失函数</a></li></ul></li><li><a href=#stronger>Stronger</a></li><li><a href=#拓展阅读-1>拓展阅读</a></li></ul><ul><li><a href=#the-deal>The Deal</a><ul><li><a href=#bounding-box-predictiondirect-location-prediction>[Bounding Box Prediction](###Direct location prediction)</a></li><li><a href=#predictions-across-scales多尺度>Predictions Across Scales多尺度</a></li><li><a href=#yolov3网络图>yolov3网络图</a></li></ul></li><li><a href=#损失函数-1>损失函数</a></li><li><a href=#拓展阅读-2>拓展阅读</a></li></ul><ul><li><a href=#introduction-1>Introduction</a></li><li><a href=#related-work>Related work</a><ul><li><a href=#bag-of-freebies>Bag of freebies</a><ul><li><a href=#data-augmentation-数据增强>Data Augmentation 数据增强</a></li><li><a href=#类别不平衡>类别不平衡</a></li><li><a href=#one-hot难表达类别之间的关联>One-hot难表达类别之间的关联</a></li><li><a href=#bbox-regression>BBox Regression</a></li></ul></li><li><a href=#bag-of-specials>Bag of specials</a><ul><li><a href=#enlarging-receptive-field-扩大感受野>Enlarging Receptive Field 扩大感受野</a></li><li><a href=#attention-mechanism-注意力机制>Attention Mechanism 注意力机制</a></li><li><a href=#feature-integration-特征融合模块>Feature Integration 特征融合模块</a></li><li><a href=#activation--function-激活函数>Activation Function 激活函数</a></li><li><a href=#post-processing--method---后处理方法>Post-processing Method 后处理方法</a></li></ul></li></ul></li><li><a href=#methodology>Methodology</a><ul><li><ul><li><a href=#selection-of-architecture>Selection of architecture</a></li><li><a href=#selection-of-bof-and-bos>Selection of BoF and BoS</a></li><li><a href=#additional-improvements>Additional improvements</a></li><li><a href=#yolo-v4-1>YOLO V4</a></li></ul></li></ul></li><li><a href=#experiments-1>Experiments</a><ul><li><a href=#实验设置>实验设置</a></li><li><a href=#不同技巧对分类器和检测器训练的影响>不同技巧对分类器和检测器训练的影响</a></li><li><a href=#不同backbone和预训练权重对检测器训练的影响>不同backbone和预训练权重对检测器训练的影响</a></li><li><a href=#不同的mini-batch-size对检测器训练的影响>不同的mini-batch size对检测器训练的影响</a></li></ul></li><li><a href=#拓展阅读-3>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-4>拓展阅读</a></li></ul><ul><li><a href=#损失计算>损失计算</a><ul><li><a href=#平衡不同尺度损失>平衡不同尺度损失</a></li></ul></li><li><a href=#拓展阅读-5>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-6>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><p>[toc]</p><h2 id=yolo-v1 class=heading-element><span>YOLO V1</span>
<a href=#yolo-v1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1506.02640 target=_blank rel="external nofollow noopener noreferrer">You Only Look Once:Unified, Real-Time Object Detection</a>
作者：<a href=https://pjreddie.com/ target=_blank rel="external nofollow noopener noreferrer">Joseph Redmon</a>, Santosh Divvalay, <a href=http://www.rossgirshick.info/ target=_blank rel="external nofollow noopener noreferrer">Ross Girshick</a>, <a href=https://homes.cs.washington.edu/~ali/index.html target=_blank rel="external nofollow noopener noreferrer">Ali Farhadi</a>
发表时间：(CVPR 2016)</p></blockquote><p>YOLO算法是单阶段目标检测的经典算法，能实现快速、实时、高精度的图像识别和目标检测。</p><h2 id=abstract class=heading-element><span>Abstract</span>
<a href=#abstract class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>介绍yolo算法及其速度快的优点</p><blockquote><p>​ 将检测变为一个 regression problem，YOLO 从输入的图像，仅仅经过一个 neural network，直接得到 bounding boxes 以及每个 bounding box 所属类别的概率。正因为整个的检测过程仅仅有一个网络，所以它可以直接 end-to-end 的优化。</p><p>速度快：标准的 YOLO 版本每秒可以实时地处理 45 帧图像。一个较小版本：Fast YOLO，可以每秒处理 155 帧图像，它的 mAP（mean Average Precision） 依然可以达到其他实时检测算法的两倍。</p><p>出现较多coordinate errors定位误差，但YOLO 有更少的 background errors背景误差。</p></blockquote><h2 id=introduction class=heading-element><span>Introduction</span>
<a href=#introduction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>yolo简单原理图；与R-CNN相比yolo的优点；与传统检测算法相比yolo的优点</p><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_yolo简单原理图.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">yolo流程图</div></center><ol><li>Resize image.将图片尺寸变为448*448</li><li>Run convolutional network.输入到神经网络中</li><li>Non-max suppression.使用非极大值抑制到最后结果</li></ol><h2 id=unified-detection class=heading-element><span>Unified Detection</span>
<a href=#unified-detection class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>one stage detection算法的原理与细节</p><blockquote><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_yolo算法原理.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">yolo算法原理</div></center><ol><li><p>将图片隐式的分为S*S个网格(grid cell)</p></li><li><p>物体的中心落在哪个网格内，哪个网格就负责预测这个物体</p></li><li><p>每个网格需要预测B个bounding box，C个类别(这B个框预测的为一个类别，一个物体)</p><blockquote><ol><li><p>如果一个网格内出现两个物体中心？</p></li><li><p>一个网格里包含了很多小物体？</p><p>yolo对靠的很近的物体以及小目标群体检测效果不是很好</p></li></ol></blockquote></li><li><p>每个框包含了位置信息和置信度(x,y,w,h,confidence)</p><blockquote><p>xy表示bounding box的中心相对于cell左上角坐标偏移</p><p>宽高则是相对于整张图片的宽高进行归一化的。（物体相对grid cell的大小）</p><p>图中框线粗细表示confidence的大小</p></blockquote><p>一张图预测的信息有S*S*(B*5+C)（<strong>注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。</strong>）</p></li></ol></blockquote><p>Comfidence Score：指的是一个边界框中包含某个物体的可能性大小以及位置的准确性（即是否恰好包裹这个物体）。</p><blockquote><p>Pr(object)是bounding box内存在对象的概率。Pr(object)并不管是哪个对象，它表示的是有或没有对象的概率。如果有object落在一个grid cell里，第一项取1，否则取0。第二项是预测的bounding box和实际的groundtruth之间的IoU值。其中IOU表示了预测的bbox与真实bbox（GT）的接近程度。置信度高表示这里存在一个对象且位置比较准确，置信度低表示可能没有对象或即便有对象也存在较大的位置偏差。</p></blockquote><blockquote><p>训练阶段：</p><blockquote><p>Pr(object)标签值非0即1；$IOU^{truth}_{pred}$按实际计算</p><p>两者乘积即为Comfidence Score的标签值</p><p>对于负责预测物体的box，这个便签值就是$IOU^{truth}_{pred}$</p></blockquote><p>预测阶段：</p><blockquote><p>回归多少就是多少</p><p>隐含包含两者</p></blockquote></blockquote><p>YOLO的bbox是没有设定大小和形状的，只是对两个bbox进行预测，保留预测比较准的bbox。YOLO的2个bounding box事先并不知道会在什么位置，只有经过前向计算，网络会输出2个bounding box，这两个bounding box与样本中对象实际的bounding box计算IOU。</p><h3 id=network-design class=heading-element><span>Network design</span>
<a href=#network-design class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_yolo网络结构图.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">yolo网络结构图</div></center><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_yolo网络结构图1.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">yolo网络结构</div></center><p>24层卷积层提取图像特征</p><p>2层全连接层回归得到$7\times7\times30$的Tensor</p><h3 id=training class=heading-element><span>Training</span>
<a href=#training class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>yolo训练方法，损失函数及参数</p></blockquote><p>最后一层用线性激活函数，其他层用leaky ReLU;</p><blockquote><p>相比于ReLU，leaky并不会让负数直接为0，而是乘以一个很小的系数（恒定），保留负数输出，但是衰减负数输出</p></blockquote><p>损失函数</p><blockquote><p>设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。</p></blockquote><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_yolov1损失函数.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">yolov1损失函数</div></center><blockquote><p>$\mathbb I_{i j}^{obj}$：第$i$个grid cell的第$j$个bounding box若==<strong>负责</strong>==预测物体则为1，否则为0；</p><p>$\mathbb I_{i j}^{nobj}$：第$i$个grid cell的第$j$个bounding box若==<strong>不负责</strong>==预测物体则为1，否则为0；</p><p>$\mathbb I_{i }^{obj}$：第$i$个grid cell是否包含物体，即是否有ground truth 框的中心点落在此grid cell中，若有则为1，否则为0</p></blockquote><ul><li><p><strong>全部采用sum-squared error loss存在的问题：</strong></p><ul><li><p>第一，8维的localization error和20维的classification error同等重要显然是不合理的；</p></li><li><p>第二，如果一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的box的confidence push到0，相比于较少的有object的网格，这种做法是overpowering的，这会导致网络不稳定甚至发散。</p></li></ul></li><li><p><strong>解决办法：</strong></p><ul><li><p>更重视8维的坐标预测，给这些损失前面赋予更大的loss weight，记为$\lambda_{coord}$在pascal VOC训练中取5。</p></li><li><p>对没有object的box的confidence loss，赋予小的loss weight，记为$\lambda_{noobj}$在pascal VOC训练中取0.5。</p></li><li><p>有object的box的confidence loss和类别的loss的loss weight正常取1。</p></li><li><p>对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏一点更不能忍受。而sum-square error loss中对同样的偏移loss是一样。 为了缓和这个问题，作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width。 如下图：small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。</p><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_yolov1损失函数 (2).png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">yolov1损失函数</div></center></li></ul><p>训练设置</p><blockquote><p>batchsize=64；momentum=0.9(动量因子)；decay=0.0005(权重衰减$ L_2$正则化)</p><p>第一个迭代周期学习率从$10^{-3}$到 $10^{-2}$；$10^{-2}$训练第2-75轮；$10^{-3}$再训练30轮；$10^{-4}$再训练30轮；</p><p>在第一个连接层之后，丢弃层使用=.05的比例，防止层之间的互相适应</p><p>数据增强：</p><blockquote><p>引入原始图像$20%$大小的随机缩放和转换</p><p>在HSV色彩空间中使用1.5的因子来随机调整图像的曝光和饱和度。</p></blockquote></blockquote></li></ul><h3 id=inference class=heading-element><span>Inference</span>
<a href=#inference class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p><strong>yolo预测阶段细节</strong></p><blockquote>$$
Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Object)*IOU^{truth}_{pred}
$$<p>等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。</p></blockquote><h3 id=limitations-of-yolo class=heading-element><span>Limitations of YOLO</span>
<a href=#limitations-of-yolo class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ul><li><p>速度快：把检测作为回归问题处理，流程简单，仅需要输入一张图</p></li><li><p>泛化能力强：yolo可以学习到物体的通用特征，泛化能力更好。应用在新领域不会崩掉。</p></li><li><p>全局推理：对整张图处理，利用全图信息，假阳性错误少（背景当作物体错误率少）</p></li><li><p>精度与最先进的算法比不高，对小物体不友好</p></li><li><p>分类正确但定位误差大</p></li></ul><h2 id=comparison-to-other-detection-systems class=heading-element><span>Comparison to Other Detection Systems</span>
<a href=#comparison-to-other-detection-systems class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>DPM</strong></p><blockquote><p>传统特征：HOG</p><p>传统分类器：SVM</p><p>滑窗套模板</p><p>弹簧模型：子模型+主模型</p></blockquote><p><strong>R-CNN</strong></p><blockquote><p>候选区域生成</p><p>提取特征</p><p>SVM进行分类</p><p>NMS剔除重叠建议框</p><p>使用回归器精细修正候选框位置</p></blockquote><p><strong>Deep MultiBox</strong></p><p><strong>OverFeat</strong></p><blockquote><p>使用全卷积网络进行高效滑窗运算</p></blockquote><h2 id=experiments class=heading-element><span>Experiments</span>
<a href=#experiments class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Object Detection/YOLO.assets/YOLO-V1_R-T Systems on pas VOC 2007.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">R-T Systems on pas VOC 2007结果分析</div></center><p>与实时检测器相比：</p><ul><li><p>fast yolo 不仅速度而且map还高</p></li><li><p>yolo的map比fast yolo高，而且也可以达到实时检测</p></li></ul><p>与速度稍慢的检测器相比：yolo在保证不错的精度同时速度最快。</p><p><strong>各类错误比例分析</strong></p><h2 id=real-time-detection-in-the-wild class=heading-element><span>Real-Time Detection In The Wild</span>
<a href=#real-time-detection-in-the-wild class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>yolo可以连接摄像头进行实时检测</p><h2 id=conclusion class=heading-element><span>Conclusion</span>
<a href=#conclusion class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>结论再次强调yolo的优点：one-stage 快速 鲁棒</p><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://arxiv.org/abs/1905.05055 target=_blank rel="external nofollow noopener noreferrer">Object Detection in 20 Years: A Survey</a></p><p><a href=https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088 target=_blank rel="external nofollow noopener noreferrer">YOLO发展路线博客</a></p><p><a href=https://pjreddie.com/darknet/yolo/ target=_blank rel="external nofollow noopener noreferrer">YOLO官网</a></p><p><a href=https://pjreddie.com/darknet/yolov1/ target=_blank rel="external nofollow noopener noreferrer">YOLOv1官网</a></p><p><a href="https://www.youtube.com/watch?list=PLrrmP4uhN47Y-hWs7DVfCmLwUACRigYyT&amp;v=NM6lrxy0bxs" target=_blank rel="external nofollow noopener noreferrer">YOLOv1作者CVPR2016大会汇报</a></p><p><a href="https://www.slideshare.net/TaegyunJeon1/pr12-you-only-look-once-yolo-unified-realtime-object-detection?from_action=save" target=_blank rel="external nofollow noopener noreferrer">一个不错的slide介绍</a></p><p><strong>Joseph Redmon</strong></p><blockquote><p><a href="https://twitter.com/pjreddie?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" target=_blank rel="external nofollow noopener noreferrer">推特</a></p><p><a href="https://scholar.google.com/citations?user=TDk_NfkAAAAJ&amp;hl=en" target=_blank rel="external nofollow noopener noreferrer">谷歌学术主页</a></p><p><a href=https://github.com/pjreddie target=_blank rel="external nofollow noopener noreferrer">Github主页</a></p><p>[简历]([https://pjreddie.com/static/Redmon%20Resume.pdf](<a href=https://pjreddie.com/static/Redmon target=_blank rel="external nofollow noopener noreferrer">https://pjreddie.com/static/Redmon</a> Resume.pdf))</p><p><a href="https://www.youtube.com/watch?v=Cgxsv1riJhI&amp;t=1s" target=_blank rel="external nofollow noopener noreferrer">2017年8月TED演讲：How computers learn to recognize objects instantly | Joseph Redmon</a></p><p><a href="https://www.youtube.com/watch?v=XS2UWYuh5u0" target=_blank rel="external nofollow noopener noreferrer">2018年6月TED演讲：Computers can see. Now what? | Joseph Redmon | TEDxGateway</a></p></blockquote><h2 id=yolo-v2 class=heading-element><span>YOLO V2</span>
<a href=#yolo-v2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1612.08242 target=_blank rel="external nofollow noopener noreferrer">YOLO9000: Better, Faster, Stronger</a>
作者：<a href=https://pjreddie.com/ target=_blank rel="external nofollow noopener noreferrer">Joseph Redmon</a>, <a href=https://homes.cs.washington.edu/~ali/index.html target=_blank rel="external nofollow noopener noreferrer">Ali Farhadi</a>
发表时间：(CVPR 2017)</p></blockquote><p>YOLOV2是YOLO目标检测系列算法的第二个版本。</p><p>第一部分：在YOLOV1基础上进行了若干改进优化，得到YOLOV2，提升算法准确度和速度。特别是增加了Anchor机制，改进了骨干网络。</p><p>第二部分：提出分层树状的分类标签结构WordTree，在目标检测和图像分类数据集上联合训练，YOLO9000可以检测超过9000个类别的物体。</p><p>CVPR 2017论文：YOLO9000: Better, Faster, Stronger，获得CVPR 2017 Best Paper Honorable Mention</p><h2 id=better class=heading-element><span>Better</span>
<a href=#better class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>其目的是弥补YOLO的两个缺陷：</p><blockquote><p>定位误差</p><p>召回率（Recall）较低（和基于候选区域的方法相比）</p><blockquote><p>Recall 是被正确识别出来的物体个数与测试集中所有对应物体的个数的比值。</p></blockquote></blockquote><center><img src="/images/Object Detection/YOLO.assets/YOLO-V2_YOLOv2相比YOLOv1的改进策略.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOv2相比YOLOv1的改进策略</div></center><h3 id=batch-normalizationhttpsarxivorgabs150203167 class=heading-element><span><a href=https://arxiv.org/abs/1502.03167 target=_blank rel="external nofollow noopener noreferrer">Batch Normalization</a></span>
<a href=#batch-normalizationhttpsarxivorgabs150203167 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>CNN网络通用的方法，不但能够改善网络的收敛性，而且能够抑制过拟合，有正则化的作用。</p><p>BN与Dropout通常不一起使用</p></blockquote><h3 id=high-resolution-classifier class=heading-element><span>High Resolution Classifier</span>
<a href=#high-resolution-classifier class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>在YOLO V2中使用ImageNet数据集，首先使用224×224的分辨率训练160个epochs，然后调整为448×448在训练10个epochs。</p></blockquote><h3 id=convolutional-with-anchor-boxes class=heading-element><span>Convolutional With Anchor Boxes</span>
<a href=#convolutional-with-anchor-boxes class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>在YOLO V2中借鉴 Fast R-CNN中的Anchor的思想。</p><ul><li><p>去掉了YOLO网络的全连接层和最后的池化层，使提取特征的网络能够得到更高分辨率的特征。</p></li><li><p>使用$416\times416$代替$448\times448$作为网络的输入，得到的特征图的尺寸为奇数。</p><blockquote><p>奇数大小的宽和高会使得每个特征图在划分cell的时候就只有一个center cell</p><p>网络最终将$416\times416$的输入变成$13\times13$大小的feature map输出，也就是缩小比例为32。（5个池化层，每个池化层将输入的尺寸缩小1/2）。</p></blockquote></li><li><p><strong>Anchor Boxes</strong>（ 提高object的定位准确率）在YOLO中，每个grid cell只预测2个bbox，最终只能预测$7\times7\times2=98$个bbox。在YOLO V2中引入了Anchor Boxes的思想，，每个grid cell只预测5个anchor box，预测$13\times13\times5=845$个bbox。 总性能下降；recall增大；precision降低</p></li></ul></blockquote><h3 id=dimension-clusters--聚类 class=heading-element><span>Dimension Clusters （聚类）</span>
<a href=#dimension-clusters--%e8%81%9a%e7%b1%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>(解决每个Grid Cell生成的bounding box的个数问题)</p><p>K均值聚类</p><blockquote><p>距离度量指标：$d(box,centroid)=1-IOU(box,centroid)$</p><p>针对同一个grid cell，其将IOU相近的聚到一起</p><p>选择k=5</p></blockquote><h3 id=direct-location-prediction class=heading-element><span>Direct location prediction</span>
<a href=#direct-location-prediction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>模型不稳定,由于预测box的位置(x,y)引起的</p><blockquote><p>Faster RCNN：</p><blockquote><p>$x=(t_x\times w_a)+x_a$</p><p>$y=(t_y\times h_a)+y_a$</p><p>$x,y$是预测边框的中心，
$x_a,y_a$是先验框（anchor）的中心点坐标，
$w_a,h_a$是先验框（anchor）的宽和高，
$t_x,t_y$是要学习的参数。输出的偏移量</p></blockquote><p>YOLOV2：将预测边框的中心约束在特定gird网格内</p><blockquote><p>$b_x=\sigma(t_x)+c_x$</p><p>$b_y=\sigma(t_y)+c_y$</p><p>$b_w=p_we^{t_w}$</p><p>$b_h=p_he^{t_h}$</p><p>$Pr(object)*IOU(b,object)=\sigma(t_o)$</p><p>$b_x,b_y,b_w,b_h$是预测边框的中心和宽高。
$Pr(object)∗IOU(b,object)$是预测边框的置信度，YOLO1是直接预测置信度的值，这里对预测参数$t_o$进行σ变换后作为置信度的值。
$c_x,c_y$是当前网格左上角到图像左上角的距离，要先将网格大小归一化，即令一个网格的宽=1，高=1。
$p_w,p_h$是先验框的宽和高。
$\sigma$ 是sigmoid函数。
$t_x,t_y,t_w,t_h,t_o$是要学习的参数，分别用于预测边框的中心和宽高，以及置信度。</p><center><img src="/images/Object Detection/YOLO.assets/YOLO-V2_边框预测.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">边框预测</div></center></blockquote></blockquote><h3 id=fine-grained-features--细粒度特征 class=heading-element><span>Fine-Grained Features 细粒度特征</span>
<a href=#fine-grained-features--%e7%bb%86%e7%b2%92%e5%ba%a6%e7%89%b9%e5%be%81 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>提出一种称之为“直通”层（passthrough layer）的操作，也是将具有丰富纹理信息的浅层特征与具有丰富语义信息的深层特征进行融合，实现对目标的“大小通吃”。</p><p>据YOLO2的代码，特征图先用$1\times1$卷积从$ 26\times26\times512 $降维到$ 26\times26\times64$，再做1拆4并passthrough。</p></blockquote><center><img src="/images/Object Detection/YOLO.assets/YOLO-V2_passthrough.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">passthrough</div></center><h3 id=multi-scale-training class=heading-element><span>Multi-Scale Training</span>
<a href=#multi-scale-training class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>通过不同分辨率图片的训练来提高网络的适应性。</p><blockquote><p>采用了{320,352,&mldr;,608}等10种输入图像的尺寸，这些尺寸的输入图像对应输出的特征图宽和高是{10,11,&mldr;19}。训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。</p></blockquote></blockquote><h2 id=faster class=heading-element><span>Faster</span>
<a href=#faster class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=darknet-19 class=heading-element><span>Darknet-19</span>
<a href=#darknet-19 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Object Detection/YOLO.assets/YOLO-V2_BackBone_Darknet19.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BackBone：Darknet19</div></center><h3 id=training-for-detection class=heading-element><span>Training for detection</span>
<a href=#training-for-detection class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Object Detection/YOLO.assets/YOLO-V2_网络图.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV2模型框架</div></center><h3 id=损失函数 class=heading-element><span>损失函数</span>
<a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3>$$
\begin{array}{r}
\operatorname{loss}_{t}=\sum_{i=0}^{W} \sum_{j=0}^{H} \sum_{k=0}^{A} \mathbb I_{\text {Max IOU }<\text { Thresh }} \lambda_{\text {noobj }} *\left(-b_{i j k}^{o}\right)^{2} \\
+\mathbb I_{t<12800} \lambda_{\text {prior }} * \sum_{r \in(x, y, w, h)}\left(\text { prior }_{k}^{r}-b_{i j k}^{r}\right)^{2} \\
+\mathbb I_{k}^{\text {truth }}\left(\lambda_{\text {coord }} * \sum_{r \in(x, y, w, h)}\left(\text { truth }^{r}-b_{i j k}^{r}\right)^{2}\right. \\
+\lambda_{o b j} *\left(I O U_{\text {truth }}^{k}-b_{i j k}^{o}\right)^{2} \\
\left.+\lambda_{\text {class }} *\left(\sum_{c=1}^{C}\left(\operatorname{truth}^{c}-b_{i j k}^{c}\right)^{2}\right)\right)
\end{array}
$$<p>W：输出特征图宽度13；H：输出特征图高度13； A：先验框个数为5</p><ul><li><p>置信度误差（边框内无对象）background的置信度误差</p><blockquote><p>$b_{ijk}^o$预测框置信度</p><p>计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background</p></blockquote></li><li><p>预测框与Anchor位置误差（前12800次迭代）</p><blockquote><p>$prior_k^r$：Anchor位置；$b_{ijk}^r$：预测框位置</p></blockquote></li><li><p>$\mathbb I_k^{truth}$：该Anchor和ground truth的IOU最大对应的预测框负责预测物体（IOU>0.6但非最大的预测框忽略其损失）</p><blockquote><p>定位误差（边框内有对象）</p><blockquote><p>$truth^r$：标注框位置；$b_{ijk}^r$：预测框位置</p></blockquote><p>置信度误差（边框内有对象）</p><blockquote><p>$I O U_{\text {truth }}^{k}$ ：Anchor与标注框的IOU； $b_{i j k}^{o}$：预测框置信度</p></blockquote><p>分类误差（边框内有对象）</p><blockquote><p>$truth^c$：标注框类别；$b_{ijk}^c$：预测框类别</p></blockquote></blockquote></li></ul><h2 id=stronger class=heading-element><span>Stronger</span>
<a href=#stronger class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://ethereon.github.io/netscope/#/gist/d08a41711e48cf111e330827b1279c31 target=_blank rel="external nofollow noopener noreferrer">可视化YOLOv2网络结构</a></p><p><a href=https://tensorspace.org/html/playground/yolov2-tiny_zh.html target=_blank rel="external nofollow noopener noreferrer">可视化YOLOv2-tiny</a></p><p><a href=https://www.cnblogs.com/YiXiaoZhou/p/7429481.html target=_blank rel="external nofollow noopener noreferrer">YOLO v2 损失函数源码分析</a></p><p><a href=https://github.com/pjreddie/darknet/blob/master/src/region_layer.c target=_blank rel="external nofollow noopener noreferrer">YOLO v2的官方Darknet实现</a></p><p><a href=https://github.com/allanzelener/YAD2K target=_blank rel="external nofollow noopener noreferrer">YOLO v2的Keras实现</a></p><p><a href=https://zhuanlan.zhihu.com/p/354111253 target=_blank rel="external nofollow noopener noreferrer">知乎：0目标检测那点儿事——更好更快的YOLO-V2</a></p><p><a href=https://zhuanlan.zhihu.com/p/354262769 target=_blank rel="external nofollow noopener noreferrer">知乎：1目标检测那点儿事——更好更快的YOLO-V2</a></p><p><a href=https://zhuanlan.zhihu.com/p/47575929 target=_blank rel="external nofollow noopener noreferrer">知乎：&lt;机器爱学习>YOLOv2 / YOLO9000 深入理解</a></p><p><a href=https://zhuanlan.zhihu.com/p/35325884 target=_blank rel="external nofollow noopener noreferrer">知乎：目标检测|YOLOv2原理与实现(附YOLOv3)</a></p><p><a href=https://www.cnblogs.com/wangguchangqing/p/10480995.html target=_blank rel="external nofollow noopener noreferrer">目标检测之YOLO V2 V3</a></p><h2 id=yolo-v3 class=heading-element><span>YOLO V3</span>
<a href=#yolo-v3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1804.02767 target=_blank rel="external nofollow noopener noreferrer">YOLOv3: An Incremental Improvement</a>
作者：<a href=https://pjreddie.com/ target=_blank rel="external nofollow noopener noreferrer">Joseph Redmon</a> ，<a href=https://homes.cs.washington.edu/~ali/index.html target=_blank rel="external nofollow noopener noreferrer">Ali Farhadi</a>
发表时间：(CVPR 2018)</p></blockquote><p>YOLOV3是单阶段目标检测算法YOLO系列的第三个版本，由华盛顿大学Joseph Redmon发布于2018年4月，广泛用于工业界。</p><p>改进了正负样本选取、损失函数、Darknet-53骨干网络，并引入了特征金字塔多尺度预测，显著提升了速度和精度。</p><h2 id=the-deal class=heading-element><span>The Deal</span>
<a href=#the-deal class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=bounding-box-predictiondirect-location-prediction class=heading-element><span>[Bounding Box Prediction](###Direct location prediction)</span>
<a href=#bounding-box-predictiondirect-location-prediction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>正负样本的匹配</p><p>预测框（每个GT仅分配一个Anchor负责预测）</p><blockquote><p>正例：与GT IOU最大</p><p>负例：IOU&lt;0.5</p><p>忽略：IOU>0.5但非最大</p></blockquote><h3 id=predictions-across-scales多尺度 class=heading-element><span>Predictions Across Scales多尺度</span>
<a href=#predictions-across-scales%e5%a4%9a%e5%b0%ba%e5%ba%a6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><table><thead><tr><th style=text-align:center></th><th style=text-align:center>输入</th><th style=text-align:center>grid cell</th><th style=text-align:center>Anchor</th><th style=text-align:center>预测框数</th><th style=text-align:center>输出张量的数据结构</th></tr></thead><tbody><tr><td style=text-align:center>YOLO V1</td><td style=text-align:center>$448\times448$</td><td style=text-align:center>$7\times7$</td><td style=text-align:center>0</td><td style=text-align:center>$7\times7\times2=98$</td><td style=text-align:center>$7\times7\times(5\times B+C)$</td></tr><tr><td style=text-align:center>YOLO V2</td><td style=text-align:center>$416\times416$</td><td style=text-align:center>$13\times13$</td><td style=text-align:center>5</td><td style=text-align:center>$13\times13\times5=845$</td><td style=text-align:center>$845\times(5+20)$</td></tr><tr><td style=text-align:center>YOLO V3</td><td style=text-align:center>$256\times256$</td><td style=text-align:center>$32\times32$，$16\times16$，$8\times8$</td><td style=text-align:center>3</td><td style=text-align:center>$4032$</td><td style=text-align:center>$4032\times(5+80)$</td></tr><tr><td style=text-align:center></td><td style=text-align:center>$416\times416$</td><td style=text-align:center>$52\times52$，$26\times26$，$13\times13$</td><td style=text-align:center>3</td><td style=text-align:center>$10647$</td><td style=text-align:center>$10647\times(5+80)$</td></tr></tbody></table><blockquote><p>Yolov3借鉴了<a href=https://arxiv.org/abs/1612.03144 target=_blank rel="external nofollow noopener noreferrer">FPN</a>特征图思想，小尺寸特征图用于检测大尺寸物体，而大尺寸特征图检测小尺寸物体。特征图的输出维度为 $N\times N\times[3\times(4+1+80)]$，$N\times N为$输出特征图格点数，一共3个Anchor框，每个框有4维预测框数值$t_x,t_y,t_w,t_h$ ，1维预测框置信度，80维物体类别数。</p></blockquote><h3 id=yolov3网络图 class=heading-element><span>yolov3网络图</span>
<a href=#yolov3%e7%bd%91%e7%bb%9c%e5%9b%be class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Object Detection/YOLO.assets/YOLO-V3_BackBone_Darknet53.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BackBone：Darknet53</div></center><center><img src="/images/Object Detection/YOLO.assets/YOLO-V3_网络图.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV3模型框架</div></center><blockquote><ul><li>Yolov3中，只有卷积层，通过调节卷积步长控制输出特征图的尺寸</li><li>concat操作与加和操作的区别：加和操作来源于ResNet思想，将输入的特征图，与输出特征图对应维度进行相加，即$y=f(x)+x$ ；而concat操作源于DenseNet网络的设计思路，将特征图按照通道维度直接进行拼接，例如的$8\times8\times16$特征图与$8\times8\times16$的特征图拼接后生成$8\times8\times32$的特征图。</li><li>上采样层(upsample)：作用是将小尺寸特征图通过插值等方法，生成大尺寸图像。例如使用最近邻插值算法，将$8\times8$的图像变换为$16\times16$。上采样层不改变特征图的通道数。</li></ul></blockquote><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOV3_2.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV3</div></center><center><img src="/images/Object Detection/YOLO.assets/YOLO-V3_训练过程.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV3训练过程</div></center><center><img src="/images/Object Detection/YOLO.assets/YOLO-V3_测试过程.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV3测试过程</div></center><h2 id=损失函数-1 class=heading-element><span>损失函数</span>
<a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2>$$
\begin{equation} \begin{split}
{loss} &= \sum_{i=0}^{K^2}\sum_{j=0}^{M}\mathbb{I}_{i,j}^{obj}\cdot (2-w_i\cdot h_i)(-x_i log(\hat x_i)-(1-x_i)log(1-\hat x_i))\\
&+\quad\sum_{i=0}^{K^2}\sum_{j=0}^{M}\mathbb{I}_{i,j}^{obj}\cdot (2-w_i\cdot h_i)(-y_i log( \hat y_i)-(1-y_i)log(1-\hat y_i))\\
&+\quad\ \sum_{i=0}^{K^2}\sum_{j=0}^{M}\mathbb{I}_{i,j}^{obj}\cdot (2-w_i\cdot h_i)[(w_i-\hat w_i)^2+(h_i-\hat h_i)^2]\\
&- \quad \sum_{i=0}^{K^2}\sum_{j=0}^{M}\mathbb{1}_{i,j}^{obj}\cdot[C_ilog(\hat C_i)+(1-C_i)log(1-\hat C_i)]\\
&- \quad \sum_{i=0}^{K^2}\sum_{j=0}^{M}\mathbb{1}_{i,j}^{noobj}\cdot[C_ilog(\hat C_i)+(1-C_i)log(1-\hat C_i)]\\
&-\quad \sum_{i=0}^{K^2}\sum_{j=0}^{M}\mathbb{1}_{i,j}^{obj}\cdot\sum_{c\in classes}[p_i(c)log(\hat p_i(c))+(1-p_i(c))log(1-\hat p_i(c))] \\
\end{split}\end{equation}
$$<p>一个是目标框位置$x,y,w,h$（左上角和长宽）带来的误差，又分为$x,y$带来的BCE Loss以及$w,h$带来的MSE Loss。</p><blockquote><p>K：grid size；M：Anchor box；$\mathbb{I}_{i,j}^{obj}$表示如果在$i,j$处的box有目标，则为1，否则为0；w 和 h 分别是ground truth 的宽和高</p><p>带$\hat x$号代表预测值；不带的表示标签</p></blockquote><p>一个是目标置信度带来的误差，也就是obj带来的loss（BCE Loss）</p><blockquote><p>$\mathbb{I}_{i,j}^{noobj}$：是否为负样本</p></blockquote><p>最后一个是类别带来的误差，也就是class带来的loss（类别数个BCE Loss）。</p><blockquote><p>$BCE=-\hat c_ilog(c_i)-(1-\hat c_i)log(1-c_i)$：二元交叉熵损失函数(Binary Cross Entropy)；$\hat c_i$标签值(非0即1)；$ c_i$预测值(0-1之间)</p></blockquote><h2 id=拓展阅读-2 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.youtube.com/watch?v=MPU2HistivI" target=_blank rel="external nofollow noopener noreferrer">YOLOV3目标检测Demo视频</a></p><p><a href=https://pjreddie.com/darknet/yolo/ target=_blank rel="external nofollow noopener noreferrer">YOLOv3官网</a></p><p><a href=https://github.com/pjreddie/darknet target=_blank rel="external nofollow noopener noreferrer">darknet github</a></p><p><strong>代码复现</strong></p><blockquote><p>Ultralytics公司：https://github.com/ultralytics/yolov3</p><p><a href=https://github.com/qqwweee/keras-yolo3 target=_blank rel="external nofollow noopener noreferrer">https://github.com/qqwweee/keras-yolo3</a></p><p><a href=https://github.com/bubbliiiing/yolo3-pytorch target=_blank rel="external nofollow noopener noreferrer">https://github.com/bubbliiiing/yolo3-pytorch</a></p><p>cvpods：https://github.com/Megvii-BaseDetection/cvpods/blob/master/cvpods/modeling/meta_arch/yolov3.py</p></blockquote><p><strong>博客</strong></p><blockquote><p><a href=https://zhuanlan.zhihu.com/p/143747206 target=_blank rel="external nofollow noopener noreferrer">知乎：深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5&amp;Yolox核心基础知识完整讲解</a></p><p><a href=https://zhuanlan.zhihu.com/p/40332004 target=_blank rel="external nofollow noopener noreferrer">知乎：近距离观察YOLOv3</a></p><p><a href=https://zhuanlan.zhihu.com/p/76802514 target=_blank rel="external nofollow noopener noreferrer">知乎：Yolo三部曲解读——Yolov3</a></p><p><a href=https://blog.csdn.net/nan355655600/article/details/106246355 target=_blank rel="external nofollow noopener noreferrer">Netron可视化YOLOV3网络结构</a></p><p><a href=https://blog.csdn.net/pikaqiu_n95/article/details/109008425 target=_blank rel="external nofollow noopener noreferrer">yolov3实现理论</a></p><p><a href=https://blog.csdn.net/leviopku/article/details/82660381 target=_blank rel="external nofollow noopener noreferrer">yolo系列之yolo v3【深度解析】</a></p><p><a href=https://blog.csdn.net/qq_37541097/article/details/81214953 target=_blank rel="external nofollow noopener noreferrer">YOLO v3网络结构分析</a></p><p><a href=https://github.com/thisiszhou/SexyYolo target=_blank rel="external nofollow noopener noreferrer">B站工程师Algernon鉴黄YOLO</a></p><p><a href=https://blog.csdn.net/qq_34795071/article/details/92803741 target=_blank rel="external nofollow noopener noreferrer">损失函数</a></p><p><a href=https://zhuanlan.zhihu.com/p/143106193 target=_blank rel="external nofollow noopener noreferrer">官方DarkNet YOLO V3损失函数完结版</a></p><p><a href=https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b target=_blank rel="external nofollow noopener noreferrer">What’s new in YOLO v3?</a></p></blockquote><p><a href=https://www.jiangdabai.com/vcat/%E3%80%8A30%E5%A4%A9%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B target=_blank rel="external nofollow noopener noreferrer">结构解析</a></p><h2 id=yolo-v4 class=heading-element><span>YOLO V4</span>
<a href=#yolo-v4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2004.10934 target=_blank rel="external nofollow noopener noreferrer">YOLOv4: Optimal Speed and Accuracy of Object Detection</a>
作者：Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao
发表时间：(CVPR 2020)</p><p><a href=https://github.com/AlexeyAB/darknet target=_blank rel="external nofollow noopener noreferrer">原始代码</a></p><p><a href=https://github.com/Tianxiaomo/pytorch-YOLOv4 target=_blank rel="external nofollow noopener noreferrer">YoloV4-pytorch代码</a></p></blockquote><h2 id=introduction-1 class=heading-element><span>Introduction</span>
<a href=#introduction-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li>提出了一种实时、高精度的目标检测模型。 它是可以使用1080Ti 或 2080Ti 等通用 GPU 来训练快速和准确的目标检测器；</li><li>在检测器训练阶段，验证了一些最先进的 Bag-of-Freebies 和 Bag-of-Specials 方法的效果；</li><li>对 SOTA 方法进行改进，使其效率更高，更适合单 GPU 训练，包括 <a href=https://arxiv.org/abs/2002.05712 target=_blank rel="external nofollow noopener noreferrer">CBN</a>，<a href=https://arxiv.org/abs/1803.01534 target=_blank rel="external nofollow noopener noreferrer">PAN</a> 和 <a href=https://arxiv.org/abs/1807.06521 target=_blank rel="external nofollow noopener noreferrer">SAM</a> 等。</li></ul><h2 id=related-work class=heading-element><span>Related work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=bag-of-freebies class=heading-element><span>Bag of freebies</span>
<a href=#bag-of-freebies class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>只改变训练策略或只增加训练成本，不影响推理成本的方法；白给的提高精度（赠品）</p></blockquote><h4 id=data-augmentation-数据增强 class=heading-element><span>Data Augmentation 数据增强</span>
<a href=#data-augmentation-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p>增加输入图片的可变性；更高的鲁棒性。</p><p>像素级调整；保留调整区域内的所有原始像素信息。</p><blockquote><p>photometric distortions 光照畸变</p><blockquote><p>brightness, contrast,hue, saturation, and noise of an image亮度、对比度、色调、饱和度和噪声</p></blockquote><p>geometric distortions 几何畸变</p><blockquote><p>random scaling, cropping, flipping, and ro-tating 随机缩放、裁剪、翻转和旋转</p></blockquote></blockquote><p>模拟对象遮挡</p><blockquote><p><a href=https://arxiv.org/abs/1708.04896 target=_blank rel="external nofollow noopener noreferrer">random erase</a> 随机擦除</p><p><a href=https://arxiv.org/abs/1708.04552 target=_blank rel="external nofollow noopener noreferrer">CutOut</a> ：随机屏蔽输入的方形区域的简单正则化技术填充0像素值</p><p><a href=https://arxiv.org/abs/1811.02545 target=_blank rel="external nofollow noopener noreferrer">hide-and-seek</a>：训练图像中随机隐藏patches，当最具区别性的内容被隐藏时，迫使网络寻找其他相关内容</p><p><a href=https://arxiv.org/abs/2001.04086 target=_blank rel="external nofollow noopener noreferrer">grid mask</a>：通过生成1个和原图相同分辨率的mask,然后将该mask和原图相乘得到一个GridMask增强后的图像。</p><p>正则化</p><blockquote><p><a href=https://jmlr.org/papers/v15/srivastava14a.html target=_blank rel="external nofollow noopener noreferrer">DropOut</a>：随机删除减少神经元的数量，使网络变得更简单</p><p><a href=http://proceedings.mlr.press/v28/wan13.html target=_blank rel="external nofollow noopener noreferrer">DropConnect</a></p><p><a href=https://arxiv.org/abs/1810.12890 target=_blank rel="external nofollow noopener noreferrer">DropBlock</a>：将Cutout应用到每一个特征图。并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程线性的增加这个比率；可应用于网络的每一层；不同组合，灵活</p></blockquote></blockquote><p>图像融合</p><blockquote><p><a href=https://arxiv.org/abs/1710.09412 target=_blank rel="external nofollow noopener noreferrer">MixUp</a>：使用两个图像以不同的系数比率进行乘法和叠加，然后用这些叠加的比率调整标签</p><p><a href="https://arxiv.org/abs/1905.04899?context=cs.CV" target=_blank rel="external nofollow noopener noreferrer">CutMix</a>：把Mixup和Cutout结合，切割一块patch并且粘贴上另外一张训练图片相同地方的patch，对应的label也按照patch大小的比例进行混合</p></blockquote><p>风格迁移</p></blockquote><h4 id=类别不平衡 class=heading-element><span>类别不平衡</span>
<a href=#%e7%b1%bb%e5%88%ab%e4%b8%8d%e5%b9%b3%e8%a1%a1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p><strong>Two stage：RCNN &mldr;</strong></p><blockquote><p><a href=https://ieeexplore.ieee.org/document/655648 target=_blank rel="external nofollow noopener noreferrer">hard negative example mining</a>：用初始的正负样本(一般是正样本+与正样本同规模的负样本的一个子集)训练分类器, 然后再用训练出的分类器对样本进行分类, 把其中负样本中错误分类的那些样本(hard negative)放入负样本集合, 再继续训练分类器, 如此反复, 直到达到停止条件(比如分类器性能不再提升).</p><p><a href=https://arxiv.org/abs/1604.03540 target=_blank rel="external nofollow noopener noreferrer">online hard example mining</a>：自动地选择难分辨样本来进行训练</p></blockquote><p><strong>One stage：SSD，yolo&mldr;</strong></p><blockquote><p><a href=https://arxiv.org/abs/1708.02002 target=_blank rel="external nofollow noopener noreferrer">Focal Loss</a></p></blockquote><h4 id=one-hot难表达类别之间的关联 class=heading-element><span>One-hot难表达类别之间的关联</span>
<a href=#one-hot%e9%9a%be%e8%a1%a8%e8%be%be%e7%b1%bb%e5%88%ab%e4%b9%8b%e9%97%b4%e7%9a%84%e5%85%b3%e8%81%94 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p><a href=https://arxiv.org/abs/1708.02002 target=_blank rel="external nofollow noopener noreferrer">label smoothing</a>(Inception V3)：将硬标签转化为软标签进行训练，可以使模型更具有鲁棒性</p><p><a href=https://arxiv.org/abs/1703.00551 target=_blank rel="external nofollow noopener noreferrer">knowledge distillation</a>：引入<strong>知识蒸馏</strong>的概念并用于设计标签细化网络</p></blockquote><h4 id=bbox-regression class=heading-element><span>BBox Regression</span>
<a href=#bbox-regression class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><blockquote><ul><li>重叠面积</li><li>中心点距离</li><li>长宽比</li></ul></blockquote><p>发展历程：<a href=https://arxiv.org/abs/1608.01471 target=_blank rel="external nofollow noopener noreferrer">IOU_Loss</a>(2016)-><a href=https://arxiv.org/abs/1902.09630 target=_blank rel="external nofollow noopener noreferrer">GIOU_Loss</a>(2019)-><a href=https://arxiv.org/abs/1911.08287 target=_blank rel="external nofollow noopener noreferrer">DIOU_Loss</a>(2020)-><a href=https://arxiv.org/abs/1911.08287 target=_blank rel="external nofollow noopener noreferrer">CIOU_Loss</a>(2020)</p><p><strong>IOU_Loss</strong></p><blockquote><center><img src="/images/Object Detection/YOLO.assets/IOU_Loss.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">IOU_Loss</div></center><p>A：预测框与真实框的交集；B：预测框与真实框的并集</p><p>$IOU=\frac{A}{B}$</p><p>$IOU_{Loss}=1-IOU$ :考虑了预测BBox面积和ground truth BBox面积的重叠面积</p><center><img src="/images/Object Detection/YOLO.assets/IOU_Loss_q.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">IOU_Loss_q</div></center><p>Q1：即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。</p><p>Q2：即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。</p></blockquote><p><strong>GIOU_Loss</strong></p><blockquote><center><img src="/images/Object Detection/YOLO.assets/GIOU_Loss.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">GIOU_Loss</div></center><p>$GIOU=IOU-\frac{|C-B|}{|C|}$；C:两框的最小外接矩形；差集=C-并集B</p><p>$GIOU_{Loss}=1-GIOU$ :增加了相交尺度的衡量方式</p><center><img src="/images/Object Detection/YOLO.assets/GIOU_Loss_q.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">GIOU_Loss_q</div></center><p>Q：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。</p></blockquote><p><strong>DIOU_Loss</strong></p><blockquote><center><img src="/images/Object Detection/YOLO.assets/DIOU_Loss.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DIOU_Loss</div></center><p>$DIOU=IOU-\frac{{Distance_2}^2}{Distance_C^2}$；Distance_C：C的对角线距离；Distance_2：两个框的两个中心点的欧氏距离$DIOU_Loss=1-DIOU$ ：考虑了重叠面积和中心点距离；当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。</p><center><img src="/images/Object Detection/YOLO.assets/DIOU_Loss_q.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DIOU_Loss_q</div></center><p>Q：目标框包裹预测框；预测框的中心点的位置都是一样的</p></blockquote><p><strong>CIOU_Loss</strong></p><blockquote><p>$CIOU=IOU-\frac{{Distance_2}^2}{Distance_C^2}-\frac{v^2}{(1-IOU)+v}$ $v=\frac{4}{\pi^2}(arctan\frac{w^{gt}}{h^{gt}}-arctan\frac{w^{p}}{h^{p}})^2$ : gt表示目标框的宽高；p表示预测框的宽高</p><p>$CIOU_{Loss}=1-CIOU$：同时考虑到重叠面积和中心点之间的距离以及长宽比</p></blockquote></blockquote><h3 id=bag-of-specials class=heading-element><span>Bag of specials</span>
<a href=#bag-of-specials class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>少量增加了推理成本，却显著提升性能的插件模块和后处理方法；不免费，但很实惠（特价）</p></blockquote><h4 id=enlarging-receptive-field-扩大感受野 class=heading-element><span>Enlarging Receptive Field 扩大感受野</span>
<a href=#enlarging-receptive-field-%e6%89%a9%e5%a4%a7%e6%84%9f%e5%8f%97%e9%87%8e class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p><a href=https://arxiv.org/abs/1406.4729 target=_blank rel="external nofollow noopener noreferrer">SPP</a> ：SPP将SPM集成到CNN使用max-pooling操作而不是bag-of-word运算；</p><blockquote><p>源于<a href=https://ieeexplore.ieee.org/document/1641019 target=_blank rel="external nofollow noopener noreferrer">SPM</a></p><blockquote><p>将特征图分割成几个d×d相等大小的块，其中d可以是{1,2,3,…}，从而形成空间金字塔，然后提取bag-of-word特征。</p></blockquote><p>[YOLOV3](###YOLO V3)改进版SPP模块：将SPP模块修改为融合$k×k$池化核的最大池化输出，其中$k = {1,5,9,13}$，步长等于1。</p><blockquote><p>一个相对较大的$k×k$有效地增加了backbone的感受野</p></blockquote></blockquote><p><a href=https://arxiv.org/abs/1606.00915 target=_blank rel="external nofollow noopener noreferrer">ASPP</a> ：和改进版SPP模块区别是主要由原来的步长1、核大小为$k×k$的最大池化到几个$3×3$核，缩放比例为$k$，步长1的空洞卷积。</p><p><a href=https://arxiv.org/abs/1711.07767 target=_blank rel="external nofollow noopener noreferrer">RFB</a> ：几个$k×k$核，缩放比例为$k$，步长1的空洞卷积</p></blockquote><h4 id=attention-mechanism-注意力机制 class=heading-element><span>Attention Mechanism 注意力机制</span>
<a href=#attention-mechanism-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>channel-wise attention</p><blockquote><p><a href=https://arxiv.org/abs/1709.01507 target=_blank rel="external nofollow noopener noreferrer">SE</a></p></blockquote><p>point-wise attention</p><blockquote><p><a href=https://arxiv.org/abs/1807.06521 target=_blank rel="external nofollow noopener noreferrer">SAM</a></p><blockquote><p>SAM 对卷积层的输出特征图应用最大池化和平均池化。将这两个特征做concat操作来，然后在一个卷积层中传递，然后应用 sigmoid 函数，该函数将突出显示最重要的特征所在的位置。</p><center><img src="/images/Object Detection/YOLO.assets/YOLO-V4_SAM.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">SAM</div></center></blockquote></blockquote><h4 id=feature-integration-特征融合模块 class=heading-element><span>Feature Integration 特征融合模块</span>
<a href=#feature-integration-%e7%89%b9%e5%be%81%e8%9e%8d%e5%90%88%e6%a8%a1%e5%9d%97 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p><a href=https://arxiv.org/abs/1411.4038 target=_blank rel="external nofollow noopener noreferrer">skip connection</a> (FCN)</p><p><a href=https://arxiv.org/abs/1411.5752 target=_blank rel="external nofollow noopener noreferrer">hyper-column</a></p><p><a href=https://arxiv.org/abs/1811.04533 target=_blank rel="external nofollow noopener noreferrer">SFAM</a> :使用SE模块在多尺度串联的特征图上执行channel-wise级别的重新加权</p><p><a href=https://arxiv.org/abs/1911.09516 target=_blank rel="external nofollow noopener noreferrer">ASFF</a> :使用softmax作为point-wise级别重新加权，然后添加不同尺度的特征图</p><p><a href=https://arxiv.org/abs/1911.09070 target=_blank rel="external nofollow noopener noreferrer">BiFPN</a> :提出了多输入加权残差连接以执行按 scale-wise级别重新加权，然后添加不同尺度的特征图。</p></blockquote><h4 id=activation--function-激活函数 class=heading-element><span>Activation Function 激活函数</span>
<a href=#activation--function-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p>让梯度更有效地传播，同时不会造成太多额外的计算成本</p><p><a href=https://dl.acm.org/doi/10.5555/3104322.3104425 target=_blank rel="external nofollow noopener noreferrer">ReLU</a>：基本上解决梯度消失问题 traditional：$tanh，sigmoid$</p><p><a href=https://www.semanticscholar.org/paper/Rectifier-Nonlinearities-Improve-Neural-Network-Maas/367f2c63a6f6a10b3b64b8729d601e69337ee3cc target=_blank rel="external nofollow noopener noreferrer">LReLU</a> ，<a href=https://arxiv.org/abs/1502.01852 target=_blank rel="external nofollow noopener noreferrer">PReLU</a> ：解决输出小于零时ReLU的梯度为零的问题。</p><p><a href=https://arxiv.org/abs/1704.04861 target=_blank rel="external nofollow noopener noreferrer">ReLU6</a> (MobileNet)，<a href=https://arxiv.org/abs/1905.02244 target=_blank rel="external nofollow noopener noreferrer">hard-Swish</a> (MobileNet V3)：专为量化网络设计</p><p><a href=https://arxiv.org/abs/1706.02515 target=_blank rel="external nofollow noopener noreferrer">Scaled ExponentialLinear Unit (SELU)</a> ：self-normalizing 神经网络设计</p><p><a href=https://arxiv.org/abs/1710.05941 target=_blank rel="external nofollow noopener noreferrer">Swish</a>， <a href=https://arxiv.org/abs/1908.08681 target=_blank rel="external nofollow noopener noreferrer">Mish</a>：连续可微的激活函数</p><blockquote><p>Mish 的下界和上界为 [≈ -0.31,∞]。由于保留了少量的负面信息，Mish通过设计消除了**<a href=https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24 target=_blank rel="external nofollow noopener noreferrer">Dying ReLU现象</a>**所必需的先决条件。较大的负偏差会导致 ReLu 函数饱和，并导致权重在反向传播阶段无法更新，从而使神经元无法进行预测。</p><p>Mish 属性有助于更好的表现力和信息流。由于在上面无界，Mish 避免了饱和，这通常会由于接近零的梯度而导致训练减慢。下界也是有利的，因为它会产生很强的正则化效果。</p></blockquote></blockquote><h4 id=post-processing--method---后处理方法 class=heading-element><span>Post-processing Method 后处理方法</span>
<a href=#post-processing--method---%e5%90%8e%e5%a4%84%e7%90%86%e6%96%b9%e6%b3%95 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p>用来过滤对同一物体预测不好的BBoxes，只保留响应较高的候选BBoxes</p><p><a href=https://arxiv.org/abs/1311.2524 target=_blank rel="external nofollow noopener noreferrer">Greedy NMS</a> (R-CNN)：增加分类置信度；由高到低顺序</p><p><a href=https://arxiv.org/abs/1704.04503 target=_blank rel="external nofollow noopener noreferrer">Soft NMS</a> ：考虑了对象的遮挡可能导致具有IoU得分的Greedy NMS中的置信度得分下降的问题</p><p><a href=https://arxiv.org/abs/1911.08287 target=_blank rel="external nofollow noopener noreferrer">DIOU NMS</a>：在soft NMS的基础上，在BBox筛选过程中加入中心点距离信息。</p><p>Anchor free里不使用NMS后处理：NMS都没有直接涉及提取特征图</p></blockquote><h2 id=methodology class=heading-element><span>Methodology</span>
<a href=#methodology class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>目的是在输入网络分辨率、卷积层数目、参数数量和每层输出个数之间找到最佳平衡</p><h4 id=selection-of-architecture class=heading-element><span>Selection of architecture</span>
<a href=#selection-of-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p><strong>检测器和分类器不同点</strong></p><ul><li>更大的输入网络尺寸（分辨率）——用于检测多个小尺寸目标</li><li>更多的层数——获得更大的感受野以便能适应网络输入尺寸的增加</li><li>更多参数——获得更大的模型容量以便在单个图像中检测多个大小不同的物体。</li></ul><p><strong>不同大小的感受野的影响</strong></p><ul><li>最大目标尺寸——允许观察到整个目标</li><li>最大网络尺寸——允许观察到目标周围的上下文</li><li>超出网络尺寸——增加图像像素点与最终激活值之间的连接数</li></ul><h4 id=selection-of-bof-and-bos class=heading-element><span>Selection of BoF and BoS</span>
<a href=#selection-of-bof-and-bos class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p>[<strong>Activations</strong>](####Activation Function 激活函数): ReLU, ==leaky-ReLU==, parametric-ReLU,ReLU6, SELU, ==Swish==, ==Mish==</p><blockquote><p>RRelu和SELU难训练；ReLU6是量化网络专用（排除选项）</p></blockquote><p>[<strong>Bounding box regression loss</strong>](####Post-processing Method 后处理方法): MSE, IoU, GIoU,CIoU, DIoU</p><blockquote><p>不用CIOU_nms：影响因子v包含标注框信息；前向推理没有标注框信息</p></blockquote><p>[<strong>Data augmentation</strong>](####Data Augmentation 数据增强l): CutOut, MixUp, CutMix，<font color=#ff4d00><strong>Mosaic</strong></font></p><p>[<strong>Regularization method</strong>](####Data Augmentation 数据增强l): DropOut, <a href=https://arxiv.org/abs/1605.07648 target=_blank rel="external nofollow noopener noreferrer">DropPath</a> ,<a href=https://arxiv.org/abs/1411.4280 target=_blank rel="external nofollow noopener noreferrer">Spatial DropOut</a> , DropBlock</p><blockquote><p>DropBlock最优</p></blockquote><p><strong>Normalization</strong> : <a href=https://arxiv.org/abs/1502.03167 target=_blank rel="external nofollow noopener noreferrer">BN</a>， <a href=https://arxiv.org/abs/1803.08904 target=_blank rel="external nofollow noopener noreferrer">CGBN or SyncBN</a>) ，<a href=https://arxiv.org/abs/1911.09737 target=_blank rel="external nofollow noopener noreferrer">FRN</a>，<a href=https://arxiv.org/abs/2002.05712 target=_blank rel="external nofollow noopener noreferrer">CBN</a>)</p><blockquote><p>一个GPU:排除SyncBN</p></blockquote><p><strong>Skip-connections</strong>: Residual connections, Weighted residual connections, Multi-input weighted residual connections(MiWRC), Cross stage partial connections (CSP)</p></blockquote><h4 id=additional-improvements class=heading-element><span>Additional improvements</span>
<a href=#additional-improvements class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><ul><li><p>引入了一种新的数据增强方法Mosaic和自对抗训练方法（Self-Adversarial Training，SAT）</p><blockquote><p><font color=#ff4d00><strong>Mosaic</strong></font>：随机裁剪4个训练图片，再拼接到1张图片(COCO数据集目标分布不均衡)</p><blockquote><p>丰富数据集</p><p>减少GPU</p><blockquote><p>归一化计算每层的4张不同图片计算激活统计信息</p><p>减少large mini-batch size的需求</p></blockquote><p>Augementation for small object dection 2019：界定大中小目标$(0-32；32-96；96-∞)$</p></blockquote><p><font color=#ff4d00><strong>Self-Adversarial Training (SAT) 自对抗训练</strong></font></p><blockquote><p>以2个forward backward stages的方式进行操作。在第一个阶段，神经网络改变的是原始图像而不是的网络权重。这样神经网络对其自身进行对抗性攻击，改变原始图像并创造出图像上没有目标的假象。在第2个阶段中，通过正常方式在修改的图像上进行目标检测对神经网络进行训练。</p></blockquote></blockquote></li><li><p>使用遗传算法选择最优超参数</p></li><li><p>修改的SAM、修改的PAN和Cross mini-Batch Normalization (CmBN)</p></li></ul><blockquote><table border=0><tr><td><img src="/images/Object Detection/YOLO.assets/YOLO-V4_Modified SAM.png"></td><td><img src="/images/Object Detection/YOLO.assets/YOLO-V4_Modified PAN.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Modified SAM</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Modified PAN</td></tr></table><p>SAM从spatial-wise attention修改为point-wise attention。</p><p>PAN的 shortcut connection改为concatenation。</p><center><img src="/images/Object Detection/YOLO.assets/YOLO-V4_CmBN.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Cross mnin-Batch Normalization</div></center>BN是对当前mini-batch进行归一化，<p>CBN是对当前以及当前往前数3个mini-batch的结果进行归一化，</p><p>CmBN 表示 CBN 修改版本,这仅在单个批次内的mini-batch之间收集统计信息。</p><blockquote><p>当batch size变小时，BN不会执行。标准差和均值的估计值受样本量的影响。样本量越小，就越不可能代表分布的完整性。</p></blockquote></blockquote><h4 id=yolo-v4-1 class=heading-element><span>YOLO V4</span>
<a href=#yolo-v4-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOV4_2.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV4_CSP网络图</div></center><blockquote><p><strong>Backbone</strong> ：CSPDarkNet53</p><blockquote><p>每个CSP模块前面的卷积核的大小都是$3\times3$，stride=2，起到下采样的作用。</p><p>因为Backbone有5个CSP模块，输入图像是$608\times608$，所以特征图变化的规律是：608->304->152->76->38->19</p><p>Cross Stage Partial Network 跨阶段局部网络：CSPNet</p><blockquote><p><a href=https://arxiv.org/abs/1911.11929 target=_blank rel="external nofollow noopener noreferrer">CSPNet: A New Backbone that can Enhance Learning Capability of CNN</a></p><p><a href=https://github.com/WongKinYiu/CrossStagePartialNetworks target=_blank rel="external nofollow noopener noreferrer">原始代码</a></p><p>CSP模块，解决网络优化中的<strong>梯度信息重复</strong></p><blockquote><p>将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并</p><blockquote><p>通过截断梯度流来防止过多的重复梯度信息。</p></blockquote><p>增强CNN学习能力,使得在轻量化的同时保持准确性</p><p>降低计算瓶颈</p><p>降低内存成本</p></blockquote></blockquote></blockquote><p><strong>Neck</strong>：<a href=https://arxiv.org/abs/1803.01534 target=_blank rel="external nofollow noopener noreferrer">PAN</a> ，[SPP](####Enlarging Receptive Field 扩大感受野)</p><blockquote><p>SPP模块：显著地增加了感受野，分离出最显著的上下文特征，并且几乎没有造成网络运行速度的降低。</p><blockquote><p>《<a href=https://arxiv.org/abs/1903.08589 target=_blank rel="external nofollow noopener noreferrer">DC-SPP-Yolo</a>》：主干网络采用SPP比单一的使用最大池化方式更加有效地增加主干特征的接收范围；可以显著分离上下文特征。</p></blockquote><p>FPN,自顶向下，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。传达强语义特征</p><p>PAN,自顶向上，传达强定位特征</p></blockquote><p><strong>Head</strong>：YOLOV3</p></blockquote><p><strong>使用技巧</strong></p><blockquote><ul><li><p>Bag of Freebies (BoF) for backbone</p><blockquote><p>CutMix和Mosaic数据增强，DropBlock正则化, 类标签平滑</p></blockquote></li><li><p>Bag of Specials (BoS) for backbone</p><blockquote><p>Mish激活函数，跨阶段部分连接(CSP)，多输入加权残差连接 (MiWRC)</p></blockquote></li><li><p>Bag of Freebies (BoF) for detector:</p><blockquote><p>CIoU损失函数, CmBN, DropBlock正则化，Mosaic数据增强，自对抗训练（SAT），Eliminate grid sensitivity，为每个真实标签使用多个anchor，<a href=https://arxiv.org/abs/1608.03983 target=_blank rel="external nofollow noopener noreferrer">Cosine annealing scheduler</a>，优化的超参数，随机的训练形状</p></blockquote></li><li><p>Bag of Specials (BoS) for detector:</p><blockquote><p>Mish激活函数，SPP模块，SAM模块，路径聚合模块（PAN）, DIoU-NMS</p></blockquote></li></ul></blockquote><h2 id=experiments-1 class=heading-element><span>Experiments</span>
<a href=#experiments-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=实验设置 class=heading-element><span>实验设置</span>
<a href=#%e5%ae%9e%e9%aa%8c%e8%ae%be%e7%bd%ae class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p><strong>ImageNet图像分类实验</strong></p><blockquote><p>训练步骤：8,000,000</p><p>batch size=128；mini-batch size=32</p><p>多项式衰减调度策略初始学习率=0.1</p><p>warm-up步骤=1,000</p><p>动量因子=0.9；衰减权重=0.005</p><p>均使用1080 Ti或2080 Ti GPU进行训练</p></blockquote><p><strong>MS COCO目标检测实验</strong></p><blockquote><p>训练步骤：500,500</p><p>batch size=64执行多尺度训练；mini-batch size=8或者4</p><p>步阶衰减学习率调度策略，初始学习率=0.01，分别在40万步和45万步上乘以系数0.1</p><p>动量因子=0.9；衰减权重=0.0005</p><p>遗传算法使用YOLOv3-SPP训练GIoU损失，并搜索300个epoch的最小5k集</p><blockquote><p>搜索学习率=0.00261，动量=0.949，IoU阈值= 0.213， loss normalizer 0.07。</p></blockquote></blockquote><h3 id=不同技巧对分类器和检测器训练的影响 class=heading-element><span>不同技巧对分类器和检测器训练的影响</span>
<a href=#%e4%b8%8d%e5%90%8c%e6%8a%80%e5%b7%a7%e5%af%b9%e5%88%86%e7%b1%bb%e5%99%a8%e5%92%8c%e6%a3%80%e6%b5%8b%e5%99%a8%e8%ae%ad%e7%bb%83%e7%9a%84%e5%bd%b1%e5%93%8d class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>分类器训练的BoF-backbone (Bag of Freebies)包括CutMix和Mosaic数据增强、类别标签smoothing。</p><center><img src="/images/Object Detection/YOLO.assets/YOLO-V4_Influence of BOF and Mish on the clasffier.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Influence of BOF and Mish on the clasffie</div></center>检测器消融实验：<center><img src="/images/Object Detection/YOLO.assets/YOLO-V4_Ablation Studies of BOF.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Ablation Studies of BOF</div></center>S：消除grid灵敏度，在YOLOv3通过方程$b_x=\sigma(t_x)+c_x;b_y=\sigma(t_y)+c_y$计算对象坐标，其中$c_x,c_y$始终为整数，因此，当$b_x$值接近$c_x$或$c_x+1$时需要极高的$t_x$绝对值。我们通过将sigmoid乘以超过1.0的因子来解决此问题，从而消除了没有检测到目标格子的影响。
M：Mosaic数据增强
IT：IoU阈值——如果IoU(ground truth, anchor) > IoU阈值，为一个ground truth使用多个anchor
GA：遗传算法
LS：类别标签smoothing
CBN：CmBN
CA：Cosine annealing scheduler——余弦退火衰减法;上升的时候使用线性上升，下降的时候模拟cos函数下降。执行多次。
DM：Dynamic mini-batch size——采用随机训练形状时，对于小分辨率的输入自动增大mini-batch的大小
OA：最优化Anchors
当使用SPP、PAN和SAM时，检测器获得最佳性能。<center><img src="/images/Object Detection/YOLO.assets/YOLO-V4_Ablation Studies of BOS.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Ablation Studies of BOS</div></center><h3 id=不同backbone和预训练权重对检测器训练的影响 class=heading-element><span>不同backbone和预训练权重对检测器训练的影响</span>
<a href=#%e4%b8%8d%e5%90%8cbackbone%e5%92%8c%e9%a2%84%e8%ae%ad%e7%bb%83%e6%9d%83%e9%87%8d%e5%af%b9%e6%a3%80%e6%b5%8b%e5%99%a8%e8%ae%ad%e7%bb%83%e7%9a%84%e5%bd%b1%e5%93%8d class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>CSPDarknet53比CSPResNeXt50更适合于做检测器的backbone</p><h3 id=不同的mini-batch-size对检测器训练的影响 class=heading-element><span>不同的mini-batch size对检测器训练的影响</span>
<a href=#%e4%b8%8d%e5%90%8c%e7%9a%84mini-batch-size%e5%af%b9%e6%a3%80%e6%b5%8b%e5%99%a8%e8%ae%ad%e7%bb%83%e7%9a%84%e5%bd%b1%e5%93%8d class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>训练时加入BoF和BoS后mini-batch大小几乎对检测器性能没有任何影响</p><blockquote><p>不再需要使用昂贵的GPU来进行训练;一个即可</p></blockquote><h2 id=拓展阅读-3 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://zhuanlan.zhihu.com/p/342570549 target=_blank rel="external nofollow noopener noreferrer">知乎：YOLOv4 介绍及其模型优化方法</a></p><p><a href=https://zhuanlan.zhihu.com/p/143747206 target=_blank rel="external nofollow noopener noreferrer">知乎：深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5&amp;Yolox核心基础知识完整讲解</a></p><p><a href=https://blog.csdn.net/qq_37541097/article/details/123229946 target=_blank rel="external nofollow noopener noreferrer">YOLOv4网络详解</a></p><p><a href=https://aijishu.com/a/1060000000109128 target=_blank rel="external nofollow noopener noreferrer">YOLOv4重磅发布，五大改进，二十多项技巧实验，堪称最强目标检测万花筒</a></p><p><a href=https://cloud.tencent.com/developer/article/1649322 target=_blank rel="external nofollow noopener noreferrer">项目实践YOLO V4万字原理详细讲解并训练自己的数据集</a></p><p><a href=https://www.cnblogs.com/makefile/p/activation-function.html target=_blank rel="external nofollow noopener noreferrer">激活函数(ReLU, Swish, Maxout)</a></p><p><a href=https://blog.ailemon.net/2020/09/21/yolov4-paper-details-interpretation/ target=_blank rel="external nofollow noopener noreferrer">YOLOv4论文详细解读</a></p><p><a href=https://blog.csdn.net/weixin_44791964/article/details/106533581 target=_blank rel="external nofollow noopener noreferrer">睿智的目标检测32——TF2搭建YoloV4目标检测平台</a></p><p><a href=https://zhuanlan.zhihu.com/p/159209199 target=_blank rel="external nofollow noopener noreferrer">YOLO V4 — 损失函数解析</a></p><p><a href=https://thoughts.teambition.com/share/5ffc5fee86465a0046842175 target=_blank rel="external nofollow noopener noreferrer">004.YOLO-V4（yolo系列）</a></p><p><a href=https://becominghuman.ai/explaining-yolov4-a-one-stage-detector-cdac0826cbd7 target=_blank rel="external nofollow noopener noreferrer">Explanation of YOLO V4 a one stage detector</a></p><h2 id=scaled-yolov4 class=heading-element><span>Scaled-YOLOv4</span>
<a href=#scaled-yolov4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html target=_blank rel="external nofollow noopener noreferrer">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></p><p>作者：Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao</p><p>发表时间：(CVPR 2021)</p><p><a href=https://github.com/WongKinYiu/ScaledYOLOv4 target=_blank rel="external nofollow noopener noreferrer">source code - Pytorch (use to reproduce results)</a></p></blockquote><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOV4_scaled_L.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">scaled YOLOV4_L网络图</div></center><h2 id=拓展阅读-4 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://sh-tsang.medium.com/review-scaled-yolov4-scaling-cross-stage-partial-network-51e3c515b0a7 target=_blank rel="external nofollow noopener noreferrer">Review — Scaled-YOLOv4: Scaling Cross Stage Partial Network</a></p><p><a href=https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-4-scaled-yolov4-c8c361b4f33f target=_blank rel="external nofollow noopener noreferrer">YOLO演進 — 4 — Scaled-YOLOv4</a></p><h2 id=yolo-v5 class=heading-element><span>YOLO V5</span>
<a href=#yolo-v5 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p><a href=https://github.com/ultralytics/yolov5 target=_blank rel="external nofollow noopener noreferrer">原始代码</a></p><p>6.1</p></blockquote><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOV5_L_2.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV5_L_2网络图</div></center><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOV5_L.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOV5_L网络图</div></center><p>数据增强</p><blockquote><p>data/hyps/hyp.scratch-high.yaml配置</p><p>Mosaic</p><p>copy paste：不同目标复制粘贴拼接</p><p>Random affine</p><p>MixUp</p><p>Albumentations 数据增强库</p><p>Augment HSV</p><p>Random horizontal flip</p></blockquote><p>训练策略</p><blockquote><p>Multi-scale training (0.5~1.5x)</p><p>AutoAnchor (For training custom data)</p><p>Warmup and Cosine LR scheduler</p><p>EMA (Exponential Moving Average)</p><p>Mixed precision</p><p>Evolve hyper-parameters</p></blockquote><h2 id=损失计算 class=heading-element><span>损失计算</span>
<a href=#%e6%8d%9f%e5%a4%b1%e8%ae%a1%e7%ae%97 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>Classes loss, 分类损失，采用的是BCE loss, 注意只计算正样本的分类损失。</p><p>Objectness loss, obj损失，采用的依然是BCE loss,注意这里的ob指的是网络预测的目标边界框与GT Box的CIoU。这里计算的是所有样本的obj损失。</p>$$
Loss = \lambda_1L_{cls} + \lambda_2L_{obj} + \lambda_3L_{loc}
$$<h3 id=平衡不同尺度损失 class=heading-element><span>平衡不同尺度损失</span>
<a href=#%e5%b9%b3%e8%a1%a1%e4%b8%8d%e5%90%8c%e5%b0%ba%e5%ba%a6%e6%8d%9f%e5%a4%b1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3>$$
L_{obj} = 4.0\cdot L_{obj}^{small}+1.0 L_{obj}^{medum}+0.4 L_{obj}^{large}
$$<h2 id=拓展阅读-5 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-5 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://www.bilibili.com/video/BV1T3411p7zR target=_blank rel="external nofollow noopener noreferrer">YOLOv5网络详解</a></p><p><strong>YOLOv5 教程</strong></p><ul><li><a href=https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data target=_blank rel="external nofollow noopener noreferrer">训练自定义数据</a> 🚀推荐的</li><li><a href=https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results target=_blank rel="external nofollow noopener noreferrer">获得最佳训练结果的提示</a> ☘️ 推荐的</li><li><a href=https://github.com/ultralytics/yolov5/issues/1289 target=_blank rel="external nofollow noopener noreferrer">权重和偏差记录</a> 🌟新的</li><li><a href=https://github.com/ultralytics/yolov5/issues/4975 target=_blank rel="external nofollow noopener noreferrer">用于数据集、标签和主动学习的 Roboflow</a> 🌟新的</li><li><a href=https://github.com/ultralytics/yolov5/issues/475 target=_blank rel="external nofollow noopener noreferrer">多 GPU 训练</a></li><li><a href=https://github.com/ultralytics/yolov5/issues/36 target=_blank rel="external nofollow noopener noreferrer">PyTorch 集线器</a> ⭐新的</li><li><a href=https://github.com/ultralytics/yolov5/issues/251 target=_blank rel="external nofollow noopener noreferrer">TFLite、ONNX、CoreML、TensorRT 导出</a> 🚀</li><li><a href=https://github.com/ultralytics/yolov5/issues/303 target=_blank rel="external nofollow noopener noreferrer">测试时间增强 (TTA)</a></li><li><a href=https://github.com/ultralytics/yolov5/issues/318 target=_blank rel="external nofollow noopener noreferrer">模型合奏</a></li><li><a href=https://github.com/ultralytics/yolov5/issues/304 target=_blank rel="external nofollow noopener noreferrer">模型修剪/稀疏</a></li><li><a href=https://github.com/ultralytics/yolov5/issues/607 target=_blank rel="external nofollow noopener noreferrer">超参数演化</a></li><li><a href=https://github.com/ultralytics/yolov5/issues/1314 target=_blank rel="external nofollow noopener noreferrer">冻结层的迁移学习</a> ⭐新的</li><li><a href=https://github.com/ultralytics/yolov5/issues/6998 target=_blank rel="external nofollow noopener noreferrer">架构总结</a> ⭐新的</li></ul><h2 id=yolox class=heading-element><span>YOLOX</span>
<a href=#yolox class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2107.08430 target=_blank rel="external nofollow noopener noreferrer">YOLOX: Exceeding YOLO Series in 2021</a></p><p>作者：Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun</p><p>发表时间：(CVPR 2021)</p><p><a href=https://github.com/Megvii-BaseDetection/YOLOX target=_blank rel="external nofollow noopener noreferrer">原始代码</a></p><p>Anchor-Free</p><p>和yolov5的v5.0不同的是head部分</p></blockquote><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOX_L_2.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOX_L_2网络图</div></center><center><img src="/images/Object Detection/YOLO.assets/网络图_YOLOX_L.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">YOLOX_L网络图</div></center><h2 id=拓展阅读-6 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://zhuanlan.zhihu.com/p/397993315 target=_blank rel="external nofollow noopener noreferrer">知乎：深入浅出Yolo系列之Yolox核心基础完整讲解</a></p><p><a href=https://www.bilibili.com/video/BV1JW4y1k76c target=_blank rel="external nofollow noopener noreferrer">B站：YoloX网络详解</a></p><h2 id=yolo-v7 class=heading-element><span>YOLO V7</span>
<a href=#yolo-v7 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2207.02696 target=_blank rel="external nofollow noopener noreferrer">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a></p><p>作者：Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao</p><p>发表时间：( 2022)</p><p><a href=https://github.com/WongKinYiu/yolov7 target=_blank rel="external nofollow noopener noreferrer">官方源码</a></p></blockquote></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-04 18:22:27">更新于 2023-06-04&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/ data-title=YOLO data-hashtags="Deep Learning,目标检测"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/ data-title=YOLO><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/ class=post-tag title="标签 - 目标检测">目标检测</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/light-weight/distilling-knowledge-/ class=post-nav-item rel=prev title="Distilling Knowledge "><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Distilling Knowledge </a><a href=/posts/deeplearning/object-detection/ssd/ class=post-nav-item rel=next title=SSD>SSD<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>