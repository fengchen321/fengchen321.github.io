<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>SENet - fengchen</title><meta name=author content="fengchen"><meta name=description content="SENet"><meta name=keywords content='Deep Learning,图像分类模型'><meta itemprop=name content="SENet"><meta itemprop=description content="SENet"><meta itemprop=datePublished content="2023-06-02T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-02T18:22:27+08:00"><meta itemprop=wordCount content="2638"><meta itemprop=keywords content="Deep Learning,图像分类模型"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/image-classification/senet/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="SENet"><meta property="og:description" content="SENet"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-02T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-02T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="图像分类模型"><meta name=twitter:card content="summary"><meta name=twitter:title content="SENet"><meta name=twitter:description content="SENet"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/senet/ title="SENet - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/vggnet/ title=VGGNet><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/ title=ResNet><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/image-classification/senet/index.md title="SENet - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"SENet","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/senet\/"},"genre":"posts","keywords":"Deep Learning, 图像分类模型","wordcount":2638,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/senet\/","datePublished":"2023-06-02T18:22:27+08:00","dateModified":"2023-06-02T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"SENet"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>SENet</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-02 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-02>2023-06-02</time></span>&nbsp;<span title="2638 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2700 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 6 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#squeeze-and-excitation-blocks>Squeeze-and-Excitation blocks</a></li><li><a href=#model-and-computational-complexity>Model and computational complexity</a></li><li><a href=#ablation-study>Ablation study</a></li><li><a href=#role-of-se-blocks>Role of SE blocks</a></li><li><a href=#训练细节>训练细节</a></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#selective-kernel-convolution>Selective Kernel Convolution</a></li><li><a href=#network-architecture>Network Architecture</a></li><li><a href=#ablation-studies>Ablation Studies</a></li><li><a href=#拓展阅读-1>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=senet class=heading-element><span>SENet</span>
<a href=#senet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1709.01507 target=_blank rel="external nofollow noopener noreferrer">Squeeze-and-Excitation Networks</a>
作者：Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu
发表时间：(CVPR 2018)</p><p><a href=https://github.com/hujie-frank/SENet target=_blank rel="external nofollow noopener noreferrer">Official Code</a></p><p><a href=https://github.com/xmu-xiaoma666/External-Attention-pytorch#4-squeeze-and-excitation-attention-usage target=_blank rel="external nofollow noopener noreferrer">External-Attention-pytorch</a> <a href=https://github.com/moskomule/senet.pytorch target=_blank rel="external nofollow noopener noreferrer">senet.pytorch</a></p></blockquote><h2 id=squeeze-and-excitation-blocks class=heading-element><span>Squeeze-and-Excitation blocks</span>
<a href=#squeeze-and-excitation-blocks class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度</strong></p><center><img src="/images/Image Classification/SENet.assets/SE-pipeline.jpg"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Diagram of a Squeeze-and-Excitation building block.</div></center><blockquote>$$
u_c=v_c*X=\sum_{s=1}^{C'}v_c^s*x^s
$$<blockquote><p>输入：$X=[x^1,x^2,&mldr;,x^{C&rsquo;}]$</p><p>输出：$U=[u_1,u_2,&mldr;,u_C]$</p><p>$v_c=[v_c^1,v_c^2,&mldr;,v_c^{C&rsquo;}] $；$v_c^s$是一个二维空间内核，表示作用于 $X $的相应通道的 $v_c$的单个通道。</p><p>卷积核的集合：$V=[v_1,v_2,&mldr;,v_C]$</p></blockquote><p><strong>压缩（Squeeze）</strong>：经过（全局平均池化）压缩操作后特征图被压缩为1×1×C向量;也可以采用更复杂的策略</p><blockquote><p>卷积计算：参数量比较大</p><p>最大池化：可能用于检测等其他任务，输入的特征图是变化的，能量无法保持</p></blockquote></blockquote>$$
z_c=F_{sq}(u_c)=\frac{1}{H\times W}\sum_{i=1}^H\sum_{j=1}^Wu_c(i,j)
$$<blockquote>$$
s=F_{ex}(z,W)=\sigma(g(z,W)) =\sigma(W_2\delta(W_1z))
$$<p>$\delta$：ReLU；$\sigma$：sigmoid激活，$W_1\in R^{\frac{C}{r}\times C}$：降维层；$W_2\in R^{C \times\frac{C}{r}}$：升维层</p><blockquote><p>比直接用一个 Fully Connected 层的好处在于</p><p>1）具有更多的非线性，可以更好地拟合通道间复杂的相关性；</p><p>2）极大地减少了参数量和计算量</p><p>c可能很大，所以需要降维</p></blockquote><p><strong>scale操作</strong>：最后通过一个 Scale 的操作来将归一化后的权重加权到每个通道的特征上</p></blockquote>$$
\tilde x_c = F_{scale}(u_c,s_c)=s_cu_c
$$<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SELayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>channel</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=mi>16</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>SELayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>avg_pool</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>AdaptiveAvgPool2d</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>channel</span><span class=p>,</span> <span class=n>channel</span> <span class=o>//</span> <span class=n>reduction</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>channel</span> <span class=o>//</span> <span class=n>reduction</span><span class=p>,</span> <span class=n>channel</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>avg_pool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>b</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>x</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><table border=0><tr><td align=center><img src="/images/Image Classification/SENet.assets/SE-Inception-module.jpg"></td><td align=center><img src="/images/Image Classification/SENet.assets/SE-ResNet-module.jpg"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">SE-Inception-module</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">SE-ResNet-module</td></tr></table><h2 id=model-and-computational-complexity class=heading-element><span>Model and computational complexity</span>
<a href=#model-and-computational-complexity class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Image Classification/SENet.assets/SENet.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">SENet</div></center><blockquote><p>reduction为16.</p><p>$fc,[16,256]$：16为第一个全连接层的输出通道数；256为第二个全连接层的输出通道数；</p></blockquote>$$
\frac{2}{r}\sum_{s=1}^SN_s\cdot C_s^2
$$<p>$r$表示降维比；$S$：第几个stage；$C_s$ 表示输出通道的维度；$N_s $表示第$s$个stage的重复块的数量</p><h2 id=ablation-study class=heading-element><span>Ablation study</span>
<a href=#ablation-study class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li><p>不同Reduction ratio也进行了消融实验。</p></li><li><p>Squeeze Operator不同操作(如Max,Avg)也进行了消融实验。</p></li><li><p>Excitation Operator不同激活函数操作(如ReLU,Tanh,Sigmoid)也进行了消融实验。</p></li><li><p>SE block在不同stage也进行了消融实验。</p></li><li><p>Integration strategy进行消融实验。</p><blockquote><center><img src="/images/Image Classification/SENet.assets/SENet_SE_variants.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">SE block integration designs explored in the ablation study</div></center><center><img src="/images/Image Classification/SENet.assets/SENet_SE_variants_result.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Effect of different SE block integration strategies with ResNet-50 on
ImageNet</div></center><p>SE的三种变体：SE 单元在<strong>分支聚合之前应用</strong>产生的性能改进对其位置相当稳健</p><blockquote><p>如果对 Addition 后主支上的特征进行重标定，由于在主干上存在 0~1 的 scale 操作，在网络较深 BP 优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。</p></blockquote><center><img src="/images/Image Classification/SENet.assets/SENet_SE_other_variants.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Effect of integrating SE blocks at the 3x3 convolutional layer of each
residual branch in ResNet-50 on ImageNet</div></center><p>另一种设计变体：将 SE 块移动到残差单元内，将其直接放在 3×3 卷积层之后。</p><blockquote><p>以更少的参数实现了可比的分类精度</p></blockquote></blockquote></li></ul><h2 id=role-of-se-blocks class=heading-element><span>Role of SE blocks</span>
<a href=#role-of-se-blocks class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>Effect of Squeeze</strong>：强调了挤压操作的重要，作为对比，它添加了相同数量的参数，删除了池化操作，用具有相同通道维度的相应 $1\times1 $卷积替换了两个 FC 层，即 NoSqueeze，其中激励输出保持空间维度作为输入。</p><center><img src="/images/Image Classification/SENet.assets/SENet_Effect of Squeeze.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Effect of Squeezet</div></center><p><strong>Role of Excitation</strong></p><center><img src="/images/Image Classification/SENet.assets/SENet_Role of Excitation.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Role of Excitation</div></center><blockquote>$$
SE\_5\_2：SE\_{stageID}\_{blockID}
$$<p>不同类别的分布在网络的早期层非常相似，表明特征通道的重要性很可能在早期由不同的类别共享</p><p>后面的层特征表现出更高水平的特异性</p><p>SE_5_2表现出一种有趣的趋向于饱和状态的趋势，大多数激活都接近于 1</p><p>SE_5_3 的网络末端（紧随其后的是分类器之前的全局池），在不同的类中出现了<strong>类似</strong>的模式</p><p>为网络提供重新校准方面不如之前的块重要，通过移除最后阶段的 SE 块，可以显着减少额外的参数计数，而性能只有<strong>边际损失</strong></p></blockquote><h2 id=训练细节 class=heading-element><span>训练细节</span>
<a href=#%e8%ae%ad%e7%bb%83%e7%bb%86%e8%8a%82 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li><p>每个瓶颈构建块的前$ 1\times1 $卷积通道的数量减半以降低模型的计算成本性能下降最小。</p></li><li><p>第一个$ 7 \times 7 $卷积层被三个连续的 $3 \times3 $卷积层替换。（Inception）</p></li><li><p>具有步长为2 的 $1 \times 1 $下采样卷积被替换为$ 3 \times 3$ 步长为2的 卷积以保留信息。</p></li><li><p>在分类层之前插入一个 dropout 层（dropout 比为 0.2）以减少过度拟合。</p></li><li><p>在训练期间使用了标签平滑正则化（。</p></li><li><p>在最后几个训练时期，所有 BN 层的参数都被冻结，以确保训练和测试之间的一致性。</p></li><li><p>使用 8 个服务器（64 个 GPU）并行进行训练，以实现大批量（2048 个）。初始学习率设置为 1.0</p></li></ul><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.youtube.com/watch?v=FUiUfD7bdqw" target=_blank rel="external nofollow noopener noreferrer">CV27 Momenta研发总监 孙刚 Squeeze and Excitation Networks上</a></p><p><a href="https://www.youtube.com/watch?v=-8nqA4F7XNU" target=_blank rel="external nofollow noopener noreferrer">CV27 Momenta研发总监 孙刚 Squeeze and Excitation Networks下</a></p><h2 id=sknet class=heading-element><span>SKNet</span>
<a href=#sknet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1903.06586 target=_blank rel="external nofollow noopener noreferrer">Selective Kernel Networks</a>
作者：Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang
发表时间：(CVPR 2019)</p><p><a href=https://github.com/implus/SKNet target=_blank rel="external nofollow noopener noreferrer">Official Code</a></p></blockquote><h2 id=selective-kernel-convolution class=heading-element><span>Selective Kernel Convolution</span>
<a href=#selective-kernel-convolution class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>用multiple scale feature汇总的information来channel-wise地指导如何分配侧重使用哪个kernel的表征</strong></p><p>一种非线性方法来聚合来自多个内核的信息，以实现神经元的自适应感受野大小</p><center><img src="/images/Image Classification/SENet.assets/SK-pipeline.jpg"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Selective Kernel Convolution</div></center><blockquote><p><strong>Split</strong>：生成具有不同内核大小的多条路径，这些路径对应于不同感受野(RF，receptive field) 大小的神经元</p><blockquote><p>$X\in R^{H&rsquo;\times W&rsquo;\times C&rsquo;} $</p><p>$\tilde F:X\to \tilde U \in R^{H\times W\times C} $ kernel size $3\times3$</p><p>$\hat F:X\to \hat U \in R^{H\times W\times C}$ kernel size $5\times5$：使用空洞卷积$3\times3$,空洞系数为2。</p></blockquote><p><strong>Fuse</strong>：聚合来自多个路径的信息，以获得选择权重的全局和综合表示。</p></blockquote>$$
U=\tilde U+\hat U\\
s_c=F_{gp}(U_c)=\frac{1}{H\times W}\sum_{i=1}^H\sum_{j=1}^WU_c(i,j)\\
z=F_{fc}(s)=\delta(B(Ws)) 降维处理\\
$$<blockquote><p>$s\in R^c$；$\delta$：ReLU；$z\in R^{d\times1}$；$W\in R^{d\times C}$：批量归一化；</p><p>$d=max(C/r,L)$ L：d的最小值，本文设置32</p><p><strong>Select</strong>：根据选择权重聚合不同大小内核的特征图</p>$$
a_c=\frac{e^{A_cz}}{e^{A_cz}+e^{B_cz}}\\
b_c=\frac{e^{B_cz}}{e^{A_cz}+e^{B_cz}}\\
$$$$
V_c=a_c\cdot\tilde U_c + b_c\cdot \hat U_c\\\
a_c+b_c=1\\
V_c\in R^{H\times W}
$$<center><img src="/images/Image Classification/SENet.assets/SK-pipeline-3.jpg"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Selective Kernel Convolution三分支</div></center><p>$SK[M,G,r]\to SK[2,32,16]$</p><blockquote><p>M：确定要聚合的不同内核的选择数量</p><p>G：控制每条路径的基数的组号</p><p>r：reduction ratio</p></blockquote></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>init</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>OrderedDict</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SKAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>channel</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span><span class=n>kernels</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>5</span><span class=p>,</span><span class=mi>7</span><span class=p>],</span><span class=n>reduction</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span><span class=n>group</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span><span class=n>L</span><span class=o>=</span><span class=mi>32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d</span><span class=o>=</span><span class=nb>max</span><span class=p>(</span><span class=n>L</span><span class=p>,</span><span class=n>channel</span><span class=o>//</span><span class=n>reduction</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>convs</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>kernels</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>convs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>OrderedDict</span><span class=p>([</span>
</span></span><span class=line><span class=cl>                    <span class=p>(</span><span class=s1>&#39;conv&#39;</span><span class=p>,</span><span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>channel</span><span class=p>,</span><span class=n>channel</span><span class=p>,</span><span class=n>kernel_size</span><span class=o>=</span><span class=n>k</span><span class=p>,</span><span class=n>padding</span><span class=o>=</span><span class=n>k</span><span class=o>//</span><span class=mi>2</span><span class=p>,</span><span class=n>groups</span><span class=o>=</span><span class=n>group</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                    <span class=p>(</span><span class=s1>&#39;bn&#39;</span><span class=p>,</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channel</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                    <span class=p>(</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                <span class=p>]))</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>channel</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fcs</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([])</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>kernels</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>fcs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d</span><span class=p>,</span><span class=n>channel</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>softmax</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>Softmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>bs</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>conv_outs</span><span class=o>=</span><span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=c1>### split</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>conv</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>convs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>conv_outs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>feats</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>conv_outs</span><span class=p>,</span><span class=mi>0</span><span class=p>)</span><span class=c1>#k,bs,channel,h,w</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>### fuse</span>
</span></span><span class=line><span class=cl>        <span class=n>U</span><span class=o>=</span><span class=nb>sum</span><span class=p>(</span><span class=n>conv_outs</span><span class=p>)</span> <span class=c1>#bs,c,h,w</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>### reduction channel</span>
</span></span><span class=line><span class=cl>        <span class=n>S</span><span class=o>=</span><span class=n>U</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=c1>#bs,c</span>
</span></span><span class=line><span class=cl>        <span class=n>Z</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>S</span><span class=p>)</span> <span class=c1>#bs,d</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>### calculate attention weight</span>
</span></span><span class=line><span class=cl>        <span class=n>weights</span><span class=o>=</span><span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>fc</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>fcs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>weight</span><span class=o>=</span><span class=n>fc</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>weights</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>weight</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bs</span><span class=p>,</span><span class=n>c</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>))</span> <span class=c1>#bs,channel</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weights</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span><span class=mi>0</span><span class=p>)</span><span class=c1>#k,bs,channel,1,1</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weights</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention_weights</span><span class=p>)</span><span class=c1>#k,bs,channel,1,1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>### fuse</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span><span class=o>=</span><span class=p>(</span><span class=n>attention_weights</span><span class=o>*</span><span class=n>feats</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>V</span>  </span></span></code></pre></td></tr></table></div></div><h2 id=network-architecture class=heading-element><span>Network Architecture</span>
<a href=#network-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Image Classification/SENet.assets/SKNet.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">SKNet</div></center><h2 id=ablation-studies class=heading-element><span>Ablation Studies</span>
<a href=#ablation-studies class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li><p>The dilation D and group number</p><blockquote><center><img src="/images/Image Classification/SENet.assets/SKNet_DG.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">The dilation D and group number</div></center></blockquote></li><li><p>Combination of different kernels</p><blockquote><center><img src="/images/Image Classification/SENet.assets/SKNet_M.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">The dilation D and group number</div></center><p>k3 表示 3x3 conv，k5 表示 3x3 conv with 2 dilated，k7 表示 3x3 conv with 3 dilated。</p><p>Dilated 是一种在不改变参数数量的情况下扩大感受区域的方法，主要用于分割。</p><p>(1) 当路径 M 的数量增加时，识别误差通常会减小。</p><p>(2) 无论 M = 2 还是 3，基于 SK 注意力的多路径聚合总是比简单聚合方法（朴素基线模型）实现更低的 top-1 误差。</p><p>(3) 使用 SK attention，模型从 M = 2 到 M = 3 的性能增益是微不足道的（top-1 error 从 20.79% 下降到 20.76%）。为了更好地权衡性能和效率，M = 2 是首选</p></blockquote></li></ul><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://zhuanlan.zhihu.com/p/59690223 target=_blank rel="external nofollow noopener noreferrer">SKNet——SENet孪生兄弟篇</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-02 18:22:27">更新于 2023-06-02&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/senet/ data-title=SENet data-hashtags="Deep Learning,图像分类模型"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/senet/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/senet/ data-title=SENet><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/ class=post-tag title="标签 - 图像分类模型">图像分类模型</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/image-classification/vggnet/ class=post-nav-item rel=prev title=VGGNet><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>VGGNet</a><a href=/posts/deeplearning/image-classification/resnet/ class=post-nav-item rel=next title=ResNet>ResNet<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024 - 2025</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>