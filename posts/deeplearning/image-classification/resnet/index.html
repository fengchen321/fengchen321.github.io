<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>ResNet - fengchen</title><meta name=author content="fengchen"><meta name=description content="ResNet"><meta name=keywords content='Deep Learning,图像分类模型'><meta itemprop=name content="ResNet"><meta itemprop=description content="ResNet"><meta itemprop=datePublished content="2023-06-02T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-02T18:22:27+08:00"><meta itemprop=wordCount content="10759"><meta itemprop=keywords content="Deep Learning,图像分类模型"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="ResNet"><meta property="og:description" content="ResNet"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-02T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-02T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="图像分类模型"><meta name=twitter:card content="summary"><meta name=twitter:title content="ResNet"><meta name=twitter:description content="ResNet"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/ title="ResNet - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/senet/ title=SENet><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/inception/ title=Inception><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/index.md title="ResNet - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"ResNet","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/resnet\/"},"genre":"posts","keywords":"Deep Learning, 图像分类模型","wordcount":10759,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/resnet\/","datePublished":"2023-06-02T18:22:27+08:00","dateModified":"2023-06-02T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"ResNet"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>ResNet</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-02 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-02>2023-06-02</time></span>&nbsp;<span title="10759 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 10800 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 22 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#residual-block>Residual block</a></li></ul></li><li><a href=#related-work>Related Work</a></li><li><a href=#deep-residual-learning>Deep Residual Learning</a></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#resnext-1>ResNeXt</a></li><li><a href=#拓展阅读-1>拓展阅读</a></li></ul><ul><li><a href=#split-attention-networks>Split-Attention Networks</a><ul><li><a href=#split-attention-block>Split-Attention Block</a></li><li><a href=#featuremap-group>featuremap group</a></li><li><a href=#split-attention-in-cardinal-groups><strong>Split Attention in Cardinal Groups</strong></a></li><li><a href=#radix-major-split-attention-block><strong>Radix-major Split-Attention Block</strong></a></li></ul></li><li><a href=#network-and-training>Network and Training</a></li><li><a href=#ablation-study>Ablation Study</a></li><li><a href=#拓展阅读-2>拓展阅读</a></li></ul><ul><li><a href=#motivation动机>Motivation动机</a></li><li><a href=#densenet-1>DenseNet</a><ul><li><a href=#稠密块dense-block>稠密块（dense block）</a></li><li><a href=#过渡层transition-layer>过渡层（transition layer）</a></li><li><a href=#model>Model</a></li></ul></li><li><a href=#拓展阅读-3>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-4>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-5>拓展阅读</a></li></ul><ul><li><a href=#basline-implemention>Basline Implemention</a></li><li><a href=#efficient-training>Efficient Training</a><ul><li><a href=#large-batch-training>Large-batch training</a></li><li><a href=#low-precision-training>Low-precision training</a></li></ul></li><li><a href=#model-tweaks>Model Tweaks</a></li><li><a href=#training-refinements>Training Refinements</a></li><li><a href=#拓展阅读-6>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-7>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-8>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=resnet class=heading-element><span>ResNet</span>
<a href=#resnet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1512.03385 target=_blank rel="external nofollow noopener noreferrer">Deep Residual Learning for Image Recognition</a></p><blockquote><p><a href=https://github.com/KaimingHe/deep-residual-networks target=_blank rel="external nofollow noopener noreferrer">代码地址</a></p><p><a href=https://github.com/Cadene/pretrained-models.pytorch target=_blank rel="external nofollow noopener noreferrer">pytorch版</a></p></blockquote><p>作者：<a href=http://kaiminghe.com/ target=_blank rel="external nofollow noopener noreferrer">Kaiming He</a> ，Xiangyu Zhang ，Shaoqing Ren ，Jian Sun，Microsoft Research</p><p>发表时间：(CVPR 2016)</p><p><a href=http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html target=_blank rel="external nofollow noopener noreferrer">CVPR论文主页</a></p></blockquote><p>微软亚洲研究院提出的深度残差网络ResNet，获得2015年ImageNet图像分类、定位、检测，MS COCO竞赛检测、分割五条赛道的冠军，通过引入残差连接，有效解决深层网络训练时的退化问题，可以通过加深网络大大提升性能。</p><p>ResNet在ILSVRC-2015图像分类竞赛中获得了top-5误差3.57%的冠军成绩，在图像分类任务上首次超过人类能力。ResNet常用于迁移学习和fine-tuning微调的特征提取的基模型。</p><p>提出残差学习结构解决深网络的退化问题和训练问题。</p><h2 id=introduction class=heading-element><span>Introduction</span>
<a href=#introduction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>Question: 简单叠加神经网络层可以吗？</p><p>Phenomenon：</p><p>明显的梯度消失/爆炸问题，难以收敛——正则化，适当的权重初始化+Batch Normalization可以加快网络收敛</p><p>模型退化问题凸显，准确率饱和</p><ul><li>网络退化：深层网络在训练集和测试集上的表现都不如浅层网络</li><li>模型退化问题并非过拟合导致，增加深度导致训练集错误率提升</li><li>深层网络不能比浅层网络错误率更高——identity mapping恒等映射</li></ul><h3 id=residual-block class=heading-element><span>Residual block</span>
<a href=#residual-block class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><ul><li>残差路径如何设计？</li><li>shortcut路径如何设计？</li><li>Residual Block之间怎么连接？</li></ul></blockquote><center><img src="/images/Image Classification/ResNet.assets/ResNet_Residual block.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Residual block</div></center><p>残差模块</p><blockquote><p>过去：</p><ul><li>直接拟合$H(x)$</li></ul><p>现在：</p><ul><li>拟合残差$F(x)=H(x)-x$</li></ul><p>shortcut connection：短路连接/捷径连接</p><ul><li>既没有引入额外参数，也没有增加计算复杂度</li><li>打破了网络对称性，提升网络表征能力</li></ul><p>identity mapping：恒等映射</p><p>$\bigoplus$为element-wise addition，要求参与运算的$F(x)$和$x$的尺寸要相同</p></blockquote><p>残差网络</p><ul><li>易于优化收敛</li><li>解决退化问题</li><li>可以很深</li></ul><h2 id=related-work class=heading-element><span>Related Work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>残差表示( Residual Representations)：</p><ul><li><p>有效的浅层表示方法:</p><ul><li><p>VLAD( vector of locally aggregated descriptors）</p></li><li><p>Fisher Vector: Probabilistic version of VLAD</p></li></ul></li><li><p>编码残差向量比编码原始向量表现更好</p></li></ul><p>捷径连接( shortcut Connections)：</p><ul><li>MLP——通过线性层将输入连接到输出</li><li>从中间层直接连接到辅助分类器</li><li>GoogLeNet——Inception Layer</li><li>Highway Networks——门控函数扮演残差角色，门控参数由学习得到</li><li>Residual Learning——提高信息流效率</li></ul><h2 id=deep-residual-learning class=heading-element><span>Deep Residual Learning</span>
<a href=#deep-residual-learning class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>传统多层网络难以拟合恒等映射</p><p>如果恒等映射已经最优，残差模块只需要拟合零映射</p><p>后面的网络只拟合前面网络的输出与期望函数的残差。</p><p>[残差块](###Residual block)：$y=F(x,{W_i})+x$</p><ul><li>$F(x,{W_i})$：需要学习的残差映射，维度与$x$一致</li><li>$x$：自身输入</li><li>$F+x$：跳跃连接，逐一加和,最后输岀经过激活函数ReLU</li><li>没有额外参数,不增加复杂度</li><li>$F$包含两个或两个以上网络层，否则表现为线性层$y=W_1x+x$</li><li>如果卷积层后加BN层，则不需要偏置项（期望为0）</li></ul><p>残差分支出现下采样(虚线表示)</p><blockquote><p>shortcut分支第一个卷积层步长都为2</p><center><img src="/images/Image Classification/ResNet.assets/ResNet_optionB.png" height=300 width=400><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ResNet_optionB</div></center></blockquote><ul><li>对多出来的通道padding补零填充</li><li>用$1\times1$卷积升维</li></ul><p>普通残差模块,用于ResNet-18/34</p><p>bottleneck残差模块,用于ResNet-50/101/152</p><blockquote><p>$1\times1$卷积,先降维后升维</p><p>减少参数量与计算量</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/ResNet_block_bottleneck.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">block和bottleneck</div></center><p>Plain Network(普通无残差网络)</p><center><img src="/images/Image Classification/ResNet.assets/ResNet_网路图.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ResNet-34与34-layer plain net和VGG对比</div></center><p>ResNet</p><center><img src="/images/Image Classification/ResNet.assets/ResNet_网络图2.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ResNet</div></center><ul><li>ResNet中，所有的Residual Block都没有pooling层，<strong>降采样是通过conv的stride实现的</strong>；</li><li>分别在conv3_1、conv4_1和conv5_1 Residual Block，降采样1倍，同时feature map数量增加1倍，如图中虚线划定的block；</li><li><strong>通过Average Pooling得到最终的特征</strong>，而不是通过全连接层；</li><li>每个卷积层之后都紧接着BatchNorm layer。</li></ul><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>ResNet解决退化问题机理</strong></p><ul><li><p>深层梯度回传顺畅</p><ul><li>恒等映射这一路的梯度是1，把深层梯度注入底层，防止梯度消失。</li></ul></li><li><p>类比其它机器学习模型</p><ul><li>集成学习 boosting，每一个弱分类器拟合“前面的模型与GT之差”</li><li>长短时记忆神经网络LSTM的遗忘门。</li><li>Relu激活函数。</li></ul></li><li><p>传统线性结构网络难以拟合“恒等映射”</p><ul><li>skip connection可以让模型自行选择要不要更新</li><li>弥补了高度非线性造成的不可逆的信息损失。( MobileNet v2)</li></ul></li><li><p>ResNet反向传播传回的梯度相关性好</p><ul><li><p><a href=https://arxiv.org/abs/1702.08591 target=_blank rel="external nofollow noopener noreferrer">The Shattered Gradients Problem: If resnets are the answer, then what is the question?</a></p><blockquote><p>网络加深,相邻像素回传回来的梯度相关性越来越低，最后接近白噪声但相邻像素之间具有局部相关性，相邻像素的梯度也应该局部相关。相邻像素不相关的白噪声梯度只意味着随机扰动，并无拟合。
ResNet梯度相关性衰减从$\frac{1}{2^L}$加为$\frac{1}{\sqrt L}$。保持了梯度相关性。</p></blockquote></li></ul></li><li><p>ResNet相当于几个浅层网络的集成</p><ul><li><p><a href=https://arxiv.org/abs/1605.06431 target=_blank rel="external nofollow noopener noreferrer">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a></p><blockquote><p>$2^n$个潜在路径(类似 dropout)
测试阶段去掉某几个残差块，几乎不影响性能</p></blockquote></li></ul></li><li><p>skip connection可以实现不同分辨率特征的组合</p><blockquote><p>FPN、 DenseNet</p></blockquote></li><li><p><a href=https://sci-hub.do/https://ink.springer.com/article/10.1007/s40304-017-0103-z target=_blank rel="external nofollow noopener noreferrer">从非线性动力学系统用离散微分方程解释(鄂维南院士)</a></p><blockquote><p>ResNet数学本质是用微分方程的积分曲线去拟合系统的目标函数；构造了一个平滑的解空间流形，在这个平滑的流形上更容易找到解。
残差网络相当于不同长度的神经网络组成的组合函数；残差模块相当于一个差分放大器</p></blockquote></li></ul><p><a href="https://www.youtube.com/watch?v=C6tLw-rPQ2o" target=_blank rel="external nofollow noopener noreferrer">CVPR2016何恺明汇报（2016年6月27日）</a></p><p><a href=http://image-net.org/challenges/LSVRC/2015/ target=_blank rel="external nofollow noopener noreferrer">ILSVRC2015竞赛</a></p><p><a href=http://cocodataset.org/#detection-2015 target=_blank rel="external nofollow noopener noreferrer">COCO2015检测与分割竞赛</a></p><p><a href=https://www.jianshu.com/p/f71ba99157c7 target=_blank rel="external nofollow noopener noreferrer">论文翻译</a></p><p><a href=https://news.hexun.com/2019-04-22/196906796.html target=_blank rel="external nofollow noopener noreferrer">孙剑首个深度学习博士张祥雨：3年看1800篇论文，28岁掌舵旷视基础模型研究</a></p><p>PreResNet：<a href=https://arxiv.org/abs/1603.05027 target=_blank rel="external nofollow noopener noreferrer">Identity Mappings in Deep Residual Networks-2016</a></p><blockquote><p><a href=https://github.com/KaimingHe/resnet-1k-layers. target=_blank rel="external nofollow noopener noreferrer">代码地址</a></p></blockquote><h2 id=resnext class=heading-element><span>ResNeXt</span>
<a href=#resnext class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1611.05431 target=_blank rel="external nofollow noopener noreferrer">Aggregated Residual Transformations for Deep Neural Networks</a></p><p>作者：Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, <a href=http://kaiminghe.com/ target=_blank rel="external nofollow noopener noreferrer">Kaiming He</a></p><p>发表时间：(CVPR 2017)</p><p><a href=https://github.com/miraclewkf/ResNeXt-PyTorch target=_blank rel="external nofollow noopener noreferrer">pytorch代码</a></p></blockquote><p><strong>设计block遵循以下两个规则</strong>：</p><ul><li><p>如果输出相同 size 的 spatial map, 那么，block 的 hyper-parameters (即 width 和 filter size) 相同</p></li><li><p>feature map大小缩减一半，通道数增一倍。</p><blockquote><p>这个规则保证了每个block的计算复杂度几乎一致！</p></blockquote></li></ul><h2 id=resnext-1 class=heading-element><span>ResNeXt</span>
<a href=#resnext-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Image Classification/ResNet.assets/ResNeXt_resnet.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)</div></center><center><img src="/images/Image Classification/ResNet.assets/ResNeXt_block.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ResNeXt_block:A layer is denoted as (# input channels, filter size, # output channels)</div></center><blockquote><p>三种等价</p><p>b和Inception V3类似，但b是同构</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/ResNeXt_网络图.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ResNeXt</div></center><blockquote><p>ResNeXt-50（32x4d）：32指进入网络的第一个<strong>ResNeXt基本结构的分组数量C</strong>（即<strong>cardinality基数</strong>）为32，4d表示depth即每一个分组的通道数为4（所以第一个基本结构输入通道数为128）</p></blockquote><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac target=_blank rel="external nofollow noopener noreferrer">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</a></p><p><a href=https://zhuanlan.zhihu.com/p/78019001 target=_blank rel="external nofollow noopener noreferrer">知乎：薰风读论文：ResNeXt 深入解读与模型实现</a></p><p><a href=https://arxiv.org/abs/1805.00932 target=_blank rel="external nofollow noopener noreferrer">Exploring the Limits of Weakly Supervised Pretraining-ECCV-2018</a></p><blockquote><p>在 2019 年，facebook 通过弱监督学习研究了该系列网络在 ImageNet 上的精度上限，为了区别之前的 ResNeXt 网络，该系列网络的后缀为 wsl，其中 wsl 是弱监督学习（weakly-supervised-learning）的简称。为了能有更强的特征提取能力，研究者将其网络宽度进一步放大，其中最大的 ResNeXt101_32x48d_wsl 拥有 8 亿个参数，将其在 9.4 亿的弱标签图片下训练并在 ImageNet-1k 上做 finetune，最终在 ImageNet-1k 的 top-1 达到了 85.4%。Fix-ResNeXt 中，作者使用了更大的图像分辨率，针对训练图片和验证图片数据预处理不一致的情况下做了专门的 Fix 策略，并使得 ResNeXt101_32x48d_wsl 拥有了更高的精度，由于其用到了 Fix 策略，故命名为 Fix-ResNeXt101_32x48d_wsl。</p></blockquote><h2 id=resnest class=heading-element><span>ResNeSt</span>
<a href=#resnest class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2004.08955 target=_blank rel="external nofollow noopener noreferrer">ResNeSt: Split-Attention Networks</a></p><p>作者：<a href=https://hangzhang.org/ target=_blank rel="external nofollow noopener noreferrer">Hang Zhang</a>, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R. Manmatha, <a href=https://github.com/mli target=_blank rel="external nofollow noopener noreferrer">Mu Li</a>, Alexander Smola</p><p>发表时间：(2020)</p><p><a href=https://github.com/zhanghang1989/ResNeSt target=_blank rel="external nofollow noopener noreferrer">Official Code</a></p></blockquote><h2 id=split-attention-networks class=heading-element><span>Split-Attention Networks</span>
<a href=#split-attention-networks class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=split-attention-block class=heading-element><span>Split-Attention Block</span>
<a href=#split-attention-block class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p><strong>featuremap group</strong> and <strong>split attention</strong> operations</p><p>ResNeSt中每个块将特征图沿着channel维度划分为几个组（groups）和更细粒度的子组（splits），每个组的特征表示是由其splits的表示的加权组合来确定的（根据全局上下文信息来确定权重），将得到的这个单元称之为 Split-Attention block</p></blockquote><h3 id=featuremap-group class=heading-element><span>featuremap group</span>
<a href=#featuremap-group class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>借鉴了ResNeXt网络的思想，将输入分为<strong>K个，每一个记为Cardinal1-k</strong> ，然后又将每个Cardinal拆分成<strong>R个，每一个记为Split1-r</strong>，所以总共有<strong>G=KR</strong>个组</p><center><img src="/images/Image Classification/ResNet.assets/ResNeSt_featuremap_group.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">ResNeSt Block</div></center></blockquote><h3 id=split-attention-in-cardinal-groups class=heading-element><span><strong>Split Attention in Cardinal Groups</strong></span>
<a href=#split-attention-in-cardinal-groups class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Image Classification/ResNet.assets/ResNeSt_Spilt_Attention_unit.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Split-Attention within a cardinal group</div></center>一个cardinal group的组合表示可以通过多个splits按元素求和进行融合来得到，第k个cardinal group为$\hat U^k$；
$$
\hat U^k=\sum_{j=R(k-1)+1}^{Rk}U_j\\
s_c^k=F_{gp}(\hat U_c^k)=\frac{1}{H\times W}\sum_{i=1}^H\sum_{j=1}^W\hat U_c^k(i,j)\\
$$<p>$\hat U^k\in R^{H\times W\times C/K}$；$s^k\in R^{C/K}$；$k\in1,2,&mldr;,K$；$H、W $和$ C $是block输出特征图的大小</p>$$
a_i^k(c) =
\begin{cases}
\frac{\exp(G_i^c(s^k))}{\sum_{j=1}^R \exp(G_i^c(s^k))} & \text{if } R > 1 \\
      \frac{1}{1 + \exp(-G_i^c(s^k))} & \text{if } R = 1
  \end{cases}
$$<blockquote><p>G：注意力权重函数G是两个全连接层(Dense)外加relu激活函数；如果R=1的话就是对该Cardinal中的所有通道视为一个整体</p></blockquote>$$
V_c^k=\sum_{i=1}^R a_i^k(c)U_{R(k-1)+1}\\
V =Concat\{V^1,V^2,...,V^K\}
$$<blockquote><p>$\hat V^k\in R^{H\times W\times C/K}$</p></blockquote><h3 id=radix-major-split-attention-block class=heading-element><span><strong>Radix-major Split-Attention Block</strong></span>
<a href=#radix-major-split-attention-block class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>转换成这一形式是为了便于使用标准的CNN进行加速（像group convolution， group fully connectd layer等）</p><p>在 channel 维度上被分为 cardinal 个不同的组，每个组叫 cardinal groups。可以把这个 cardinal group 继续分成 radix 个小组。这样每个组都一个 cardinal 的序号，和 radix 的序号。</p><p><a href=https://github.com/zhanghang1989/ResNeSt/blob/master/tests/test_radix_major.py target=_blank rel="external nofollow noopener noreferrer">代码测试</a></p><blockquote><center><img src="/images/Image Classification/ResNet.assets/ResNeSt_Radix_major.png" width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Radix-major implementation of ResNeSt block</div></center></blockquote><p>展示出从 cardinality-major 到 radix-major 的变化过程</p><table border=0><tr><td align=center><img src="/images/Image Classification/ResNet.assets/ResNeSt_radix_major0.png"></td><td align=center><img src="/images/Image Classification/ResNet.assets/ResNeSt_radix_major1.png"></td><td align=center><img src="/images/Image Classification/ResNet.assets/ResNeSt_radix_major2.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">cardinality-major</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">中间变换</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">radix-major</td></tr></table><blockquote><p><strong>Channel/group shuffling equivariant:</strong></p><blockquote><p><strong>全局平均池化层</strong>：把 channel 维度任意打乱，再通过全局平均池化层，然后再把 channel <strong>顺序还原</strong></p><p><strong>批量归一化 (Batch Normalization)</strong>：打乱 channel，然后再还原，保证 BN 层的 gamma 和 beta 也相应调整顺序</p><p><strong>分组卷积</strong></p></blockquote><p><strong>1x1 卷积与全连接层</strong>：1x1 的分组卷积来实现多个并行的全连接层</p><p>If several consecutive modules are shuffling-equivariant, then the entire block is shuffling-equivariant.</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>functional</span> <span class=k>as</span> <span class=n>F</span>
</span></span><span class=line><span class=cl><span class=c1># https://github.com/zhanghang1989/ResNeSt/issues/66</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>rSoftMax</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>radix</span><span class=p>,</span> <span class=n>cardinality</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>radix</span> <span class=o>&gt;</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>radix</span> <span class=o>=</span> <span class=n>radix</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cardinality</span> <span class=o>=</span> <span class=n>cardinality</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>radix</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>cardinality</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>radix</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Splat</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=n>radix</span><span class=p>,</span> <span class=n>cardinality</span><span class=p>,</span> <span class=n>reduction_factor</span><span class=o>=</span><span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Splat</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>radix</span> <span class=o>=</span> <span class=n>radix</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cardinality</span> <span class=o>=</span> <span class=n>cardinality</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>channels</span> <span class=o>=</span> <span class=n>channels</span>
</span></span><span class=line><span class=cl>        <span class=n>inter_channels</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>channels</span><span class=o>*</span><span class=n>radix</span><span class=o>//</span><span class=n>reduction_factor</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>channels</span><span class=o>//</span><span class=n>radix</span><span class=p>,</span> <span class=n>inter_channels</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>cardinality</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>inter_channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>inter_channels</span><span class=p>,</span> <span class=n>channels</span><span class=o>*</span><span class=n>radix</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>cardinality</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rsoftmax</span> <span class=o>=</span> <span class=n>rSoftMax</span><span class=p>(</span><span class=n>radix</span><span class=p>,</span> <span class=n>cardinality</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span><span class=p>,</span> <span class=n>rchannel</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>radix</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>splited</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>rchannel</span><span class=o>//</span><span class=bp>self</span><span class=o>.</span><span class=n>radix</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>gap</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>splited</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>gap</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>gap</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>adaptive_avg_pool2d</span><span class=p>(</span><span class=n>gap</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>gap</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>gap</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>gap</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn1</span><span class=p>(</span><span class=n>gap</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>gap</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>gap</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>atten</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>gap</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>atten</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rsoftmax</span><span class=p>(</span><span class=n>atten</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>radix</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>atten</span><span class=p>,</span> <span class=n>rchannel</span><span class=o>//</span><span class=bp>self</span><span class=o>.</span><span class=n>radix</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>out</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>([</span><span class=n>att</span><span class=o>*</span><span class=n>split</span> <span class=k>for</span> <span class=p>(</span><span class=n>att</span><span class=p>,</span> <span class=n>split</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>attens</span><span class=p>,</span> <span class=n>splited</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>out</span> <span class=o>=</span> <span class=n>atten</span> <span class=o>*</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span></span></span></code></pre></td></tr></table></div></div><h2 id=network-and-training class=heading-element><span>Network and Training</span>
<a href=#network-and-training class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>Network Tweaks</strong></p><blockquote><ul><li><p><strong>平均下采样</strong>: 对于检测和分割任务，下采样过程对于保持空间信息非常重要，ResNeSt采用的是平均池化方法，使用$3\times3$的kernel来很好的保持空间信息。</p></li><li><p><strong>从<a href=https://arxiv.org/abs/1812.01187 target=_blank rel="external nofollow noopener noreferrer">ResNet-D</a>中学到的策略</strong>：用3个$3\times3$卷积替代一个$7\times7$卷积；加了一个$2\times2$的平均池化到skip connection里去。</p></li></ul></blockquote><p><strong>Training Strategy</strong></p><blockquote><ul><li>大型小批量分布式训练：$\eta =\frac{B}{256}\eta_{base}$； B 為为mini-batch size、base learning rate 设定为0.1；在前五個个epoch 使用<a href=https://arxiv.org/abs/1706.02677 target=_blank rel="external nofollow noopener noreferrer"> warm-up strategy</a> 逐渐增加 learning rate</li><li><a href=https://arxiv.org/abs/1512.00567 target=_blank rel="external nofollow noopener noreferrer">Label Smoothing</a></li><li><a href=https://arxiv.org/abs/1805.09501 target=_blank rel="external nofollow noopener noreferrer">Auto Augmentation</a></li><li><a href=https://arxiv.org/abs/1710.09412 target=_blank rel="external nofollow noopener noreferrer">Mixup Training</a></li><li>则化：可以选择dropout、DropBlock、L2正则化方法。</li></ul></blockquote><h2 id=ablation-study class=heading-element><span>Ablation Study</span>
<a href=#ablation-study class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><table border=0><tr><td align=center><img src="/images/Image Classification/ResNet.assets/ResNeSt_ablation_0.png"></td><td align=center><img src="/images/Image Classification/ResNet.assets/ResNeSt_ablation_1.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">breakdown of improvement</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">radix vs. cardinality under ResNeSt-fast setting</td></tr></table><blockquote><table><thead><tr><th><a href=https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md target=_blank rel="external nofollow noopener noreferrer">补充来源</a></th><th>setting</th><th>#P</th><th>GFLOPs</th><th>PyTorch</th><th>Gluon</th></tr></thead><tbody><tr><td>ResNeSt-50-fast</td><td>1s1x64d</td><td>26.3M</td><td>4.34</td><td>80.33</td><td>80.35</td></tr><tr><td>ResNeSt-50-fast</td><td>2s1x64d</td><td>27.5M</td><td>4.34</td><td>80.53</td><td>80.65</td></tr><tr><td>ResNeSt-50-fast</td><td>4s1x64d</td><td>31.9M</td><td>4.35</td><td>80.76</td><td>80.90</td></tr><tr><td>ResNeSt-50-fast</td><td>1s2x40d</td><td>25.9M</td><td>4.38</td><td>80.59</td><td>80.72</td></tr><tr><td>ResNeSt-50-fast</td><td>2s2x40d</td><td>26.9M</td><td>4.38</td><td>80.61</td><td>80.84</td></tr><tr><td>ResNeSt-50-fast</td><td>4s2x40d</td><td>30.4M</td><td>4.41</td><td>81.14</td><td>81.17</td></tr><tr><td>ResNeSt-50-fast</td><td>1s4x24d</td><td>25.7M</td><td>4.42</td><td>80.99</td><td>80.97</td></tr></tbody></table><p>2s2x40d ：radix=2, cardinality=2 and width=40</p></blockquote><h2 id=拓展阅读-2 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://www.bilibili.com/video/BV1PV411k7ch#reply3078900535 target=_blank rel="external nofollow noopener noreferrer">张航-ResNeSt：拆分注意力网络</a></p><p><a href=https://zhuanlan.zhihu.com/p/133805433 target=_blank rel="external nofollow noopener noreferrer">关于ResNeSt的点滴疑惑</a></p><p><a href=https://zhuanlan.zhihu.com/p/135220104 target=_blank rel="external nofollow noopener noreferrer">ResNeSt 实现有误？</a></p><p><a href=https://github.com/zhanghang1989/ResNeSt/issues/4 target=_blank rel="external nofollow noopener noreferrer">等价图片来源</a></p><p><a href=https://github.com/zhanghang1989/ResNeSt/issues/74 target=_blank rel="external nofollow noopener noreferrer">https://github.com/zhanghang1989/ResNeSt/issues/74</a></p><p><a href=https://github.com/zhanghang1989/ResNeSt/issues/4 target=_blank rel="external nofollow noopener noreferrer">https://github.com/zhanghang1989/ResNeSt/issues/4</a></p><p><a href=https://github.com/zhanghang1989/ResNeSt/issues/41 target=_blank rel="external nofollow noopener noreferrer">https://github.com/zhanghang1989/ResNeSt/issues/41</a></p><h2 id=densenet class=heading-element><span>DenseNet</span>
<a href=#densenet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1608.06993 target=_blank rel="external nofollow noopener noreferrer">Densely Connected Convolutional Networks</a></p><p>作者：Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger</p><p>发表时间：(CVPR 2017)</p><p><a href=https://github.com/liuzhuang13/DenseNet target=_blank rel="external nofollow noopener noreferrer">Official Code</a></p><p>稠密连接网络（DenseNet）在某种程度上是ResNet的逻辑扩展</p></blockquote><p>相比 ResNet 中的 bottleneck，dense-block 设计了一个更激进的密集连接机制，即互相连接所有的层，每个层都会接受其前面<strong>所有层</strong>作为其额外的输入。</p><p>DenseNet 将所有的 dense-block 堆叠，组合成了一个密集连接型网络。</p><p>密集的连接方式使得 DenseNet更容易进行梯度的反向传播，使得网络更容易训练。</p><h2 id=motivation动机 class=heading-element><span>Motivation动机</span>
<a href=#motivation%e5%8a%a8%e6%9c%ba class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>DenseNets 不是从极深或极宽的架构中汲取表征能力，而是通过特征重用来利用网络的潜力。</strong></p><p>Q:从输入层到输出层的信息路径（以及相反方向的梯度）变得很大，以至于它们可能在到达另一边之前就消失了。</p><blockquote><p>只需将每一层直接相互连接起来：解决了确保最大信息（和梯度）流动的问题。</p><p>每一层都可以直接访问损失函数和原始输入图像的梯度。</p><p>缺点：反向传播虽然容易，但是计算复杂</p></blockquote><p>Q：DenseNets 比等效的传统 CNN 需要更少的参数</p><blockquote><p>不需要学习冗余特征图：对于旧的特征图(feature-map)是不需要再去重新学习的</p><p>特征重用缺点：训练模型时RAM会爆炸</p><p>growth-rate不用设很大，所以减少许多参数。</p><blockquote><p>growth-rate：卷积层中卷积核的数量(k)，DenseNet：k=12</p><p>卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率</p></blockquote></blockquote><p><a href=https://arxiv.org/abs/1707.06990 target=_blank rel="external nofollow noopener noreferrer">Memory-Efficient Implementation of DenseNets</a></p><center><img src="/images/Image Classification/ResNet.assets/DenseNet_implementation.png" width=1000><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Memory-Efficient Implementation of DenseNets</div></center><h2 id=densenet-1 class=heading-element><span>DenseNet</span>
<a href=#densenet-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>假如我们有$L$层卷积神经网路，那就有$L$个(层与层之间的)连结。但是DenseNet设计成有$\frac{L(L+1)}{2}$个连结。</p><center><img src="/images/Image Classification/ResNet.assets/DenseNet_网络图_0.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DenseNet</div></center><h3 id=稠密块dense-block class=heading-element><span>稠密块（dense block）</span>
<a href=#%e7%a8%a0%e5%af%86%e5%9d%97dense-block class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。</p><h3 id=过渡层transition-layer class=heading-element><span>过渡层（transition layer）</span>
<a href=#%e8%bf%87%e6%b8%a1%e5%b1%82transition-layer class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>过渡层可以用来控制模型复杂度。通过$1\times1$卷积层来减小通道数，并使用步幅为2的<strong>平均汇聚层</strong>减半高和宽，从而进一步降低模型复杂度。</p><blockquote><p>为什么在过渡层使用平均汇聚层而不是最大汇聚层？</p><blockquote><p>参考：平均池化的特点是保留背景信息让每一个信息对最后的输出都有帮助，最大池化的特点是提取特征只保留特征最明显的信息，当我们费劲心力把不同层的信息叠在了一起以后用最大池化等于前面都做了无用功</p></blockquote></blockquote><h3 id=model class=heading-element><span>Model</span>
<a href=#model class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src="/images/Image Classification/ResNet.assets/DenseNet_网络图_1.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DenseNet</div></center><p>DenseNet-121是指网络总共有121层：(6+12+24+16)*2 + 3(transition layer) + 1(7x7 Conv) + 1(Classification layer) = 121。</p><p>DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。</p><p>类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。</p><p>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</p><p>与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。</p><h2 id=拓展阅读-3 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://amaarora.github.io/2020/08/02/densenets.html target=_blank rel="external nofollow noopener noreferrer">DenseNet Architecture Explained with PyTorch Implementation from TorchVision</a></p><p><a href=https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a target=_blank rel="external nofollow noopener noreferrer">Understanding and visualizing DenseNets</a></p><p><a href=https://zh.d2l.ai/chapter_convolutional-modern/densenet.html target=_blank rel="external nofollow noopener noreferrer">动手学深度学习：7.7. 稠密连接网络（DenseNet）</a></p><h2 id=dpn class=heading-element><span>DPN</span>
<a href=#dpn class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1707.01629 target=_blank rel="external nofollow noopener noreferrer">Dual Path Networks</a></p><p>作者：Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng</p><p>发表时间：(NIPS 2017)</p></blockquote><p>DPN,DPN 的全称是 Dual Path Networks，即双通道网络。</p><p>该网络是由 DenseNet 和 ResNet 结合的一个网络，利用残差网络的跳跃连接对特征进行复用，又可以利用密集连接路径持续探索新特征。</p><blockquote><p>DenseNet 把每一层的输出都拼接（concatenate）到其后每一层的输入上，从靠前的层级中提取到新的特征。 <strong>善于挖掘新特征，冗余度高</strong></p><p>ResNet 把输入直接加到（element-wise adding）卷积的输出上是对之前层级中已提取特征的复用。<strong>善于复用特征，冗余度低</strong></p></blockquote><p>$[W_1 \ W_2][X_1;X_2]=W_1X_1+W_2X_2$：<strong>如果两组conv，输出的filter个数是一样的，那么在input channel上concat是可以等价于分别两组conv求和的形式</strong></p><center><img src="/images/Image Classification/ResNet.assets/DPN_overview.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DPN_block</div></center><blockquote><p>(a) 残差网络。</p><p>(b) 密集连接的网络，其中每一层都可以访问所有先前微块的输出。在这里，为了与（a）中的微块设计保持一致，添加了一个 1×1 卷积层（下划线）。</p><p>(c) 通过在 (b) 中的微块之间共享相同输出的第一个 1×1 连接，密集连接的网络退化为残差网络。(c) 中的虚线矩形突出显示了残差单元。</p><p>(d) 双路径架构，DPN。</p><p>(e) 从实现的角度看(d)的等价形式，其中符号“~”表示拆分操作，“+”表示逐元素加法</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/DPN_overview_1.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">DPN</div></center><ul><li>$3\times3$ 的卷积层采用的是 group convolution</li><li>$1×1×256(+16) $中的 256 代表的是 ResNet 的通道数，16 代表的是 DenseNet 一层的输出通道数，将结果分成 256 和 16 两部分，256 的 element-wise 的加到 ResNet 通道，16 的 concat 到 DenseNet 通道，然后继续下一个 block，同样输出 256 + 16 个通道，重复操作。</li></ul><h2 id=拓展阅读-4 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://github.com/cypw/DPNs target=_blank rel="external nofollow noopener noreferrer">代码</a></p><p><a href=https://github.com/cypw/DPNs/tree/master/settings target=_blank rel="external nofollow noopener noreferrer">DPNS代码</a></p><p><a href=https://zhuanlan.zhihu.com/p/32702293 target=_blank rel="external nofollow noopener noreferrer">知乎：解读Dual Path Networks（DPN，原创）</a></p><p><a href=https://zhuanlan.zhihu.com/p/102944057 target=_blank rel="external nofollow noopener noreferrer">知乎：卷积神经网络学习路线（十五） | NIPS 2017 Dual Path Network</a></p><p><a href=http://vincentho.name/2018/12/11/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E2%80%94%E2%80%94-Dual-Path-Network/ target=_blank rel="external nofollow noopener noreferrer">【论文阅读】—— Dual Path Network</a></p><h2 id=hardnet class=heading-element><span>HarDNet</span>
<a href=#hardnet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1909.00948 target=_blank rel="external nofollow noopener noreferrer">HarDNet: A Low Memory Traffic Network</a></p><p>作者：Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin</p><p>发表时间：(ICCV 2019)</p><p><a href=https://github.com/PingoLH/Pytorch-HarDNet target=_blank rel="external nofollow noopener noreferrer">官方代码</a></p></blockquote><p>HarDNet,HarDNet（Harmonic DenseNet）是 2019 年由国立清华大学提出的一种全新的神经网络，在低 MAC 和内存流量的条件下实现了高效率。与 FC-DenseNet-103，DenseNet-264，ResNet-50，ResNet-152 和 SSD-VGG 相比，新网络的推理时间减少了 35%，36%，30%，32% 和 45%。使用了包括 Nvidia Profiler 和 ARM Scale-Sim 在内的工具来测量内存流量，并验证推理延迟确实与内存流量消耗成正比，并且所提议的网络消耗的内存流量很低。</p><p><strong>评价指标</strong></p><p>Nvidia profiler获取DRAM读/写的字节数。</p><p>ARM Scale Sim获取每个CNN框架的流量数据和推理次数。</p><p>Convolutional Input/Output (CIO)：每个卷积层的输入和输出尺寸之和。CIO是DRAM流量的近似处理。</p><p>MoC（MACs over CIO）。在MoC低于某个值时，CIO才会在推理时间中占主导地位。</p><p>对每一层的MoC施加一个软约束，以设计一个低CIO网络模型，并合理增加MACs。</p><p>首先减少来自DenseNet的大部分层连接，以降低级联损耗。然后，通过增加层的通道宽度来平衡输入/输出通道比率。</p><center><img src="/images/Image Classification/ResNet.assets/HarDNet_block.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Block</div></center><blockquote><p>采用的<strong>稀疏连接</strong>方式：当$k$能被$2^n$整除，让$k$层和$k-2^n$层相连，其中$n$为非负整数；并且还需满足$k-2^{n} \ge 0$。</p><p>$l$层初始化growth-rate k</p><blockquote><p>卷积层中卷积核的数量(k)，DenseNet：k=12</p></blockquote><p>m 用作低维压缩因子</p><p>通道数：$k\times m^n$，n是$l$除以$2^n$时的最大数</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/HarDNet_trainsition.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">(a) Inverted transition down module, (b) Depthwise-separable convolution for HarDNet</div></center><p>在HDB后连接一个1x1 conv层，作为trainsition。此外，设置HDB的深度为$L=2^n$，这样一个HDB的最后一层就有最大的通道数，梯度最多能传输$\text{log}L$层。为了缓解这种梯度消失，将一个HDB的输出设置为第L层和它前面所有奇数层的级联。当完成HDB以后，就可以丢弃从2至L-2的所有偶数层。当m=1.6-1.9时，这些偶数层的内存占用是奇数层的2至3倍。</p><center><img src="/images/Image Classification/ResNet.assets/HarDNet.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">HarDNet</div></center><blockquote><p>3x3, 64：64个输出通道的Conv3x3层</p><p>8,k=14,t=256：有8层的HDB，增长率k和一个有t个输出通道的trainsition过渡层 conv1x1</p><p>m ：低维压缩因子</p><p>所有层的Conv-BN-ReLU，而不是DenseNet中使用的BN-ReLU-Conv</p><blockquote><p>实现折叠批量标准化</p></blockquote></blockquote><p>HardNet-68 中每个 HDB 的专用增长率 k 提高了 CIO 效率。</p><blockquote><p>由于深度 HDB 具有更多的输入通道，因此更大的增长率有助于平衡层的输入和输出之间的通道比率，以满足对MoC 约束。</p></blockquote><p>对于层分布，没有集中在大多数 CNN 模型采用的 stride16 上，而是让 stride8 在 HardNet-68 中拥有最多的层，</p><blockquote><p>提高了局部特征学习，有利于小规模目标检测。相比之下，分类任务更多地依赖全局特征学习，因此专注于低分辨率可以获得更高的准确度和更低的计算复杂度</p></blockquote><h2 id=拓展阅读-5 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-5 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://zhuanlan.zhihu.com/p/257874749 target=_blank rel="external nofollow noopener noreferrer">HarDNet简析</a></p><h2 id=resnet_d class=heading-element><span>ResNet_D</span>
<a href=#resnet_d class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1812.01187 target=_blank rel="external nofollow noopener noreferrer">Bag of Tricks for Image Classification with Convolutional Neural Networks</a></p><p>作者：Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li</p><p>发表时间：(CVPR 2019)</p><p><a href=https://github.com/dmlc/gluon-cv target=_blank rel="external nofollow noopener noreferrer">Official Code</a></p></blockquote><h2 id=basline-implemention class=heading-element><span>Basline Implemention</span>
<a href=#basline-implemention class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li>预处理与数据增强<ul><li>随机sample并且转化为[0,255]之间的32位宽浮点数</li><li>随机Crop的高宽比在3/4到4/3；面积占比大小从8~100%；最后被Resize到[224,224]</li><li>50%概率水平翻转</li><li>缩放色调，饱和度和亮度，取[0.6,1.4]</li><li>加上PCA Noise</li><li>最后Normalize整个图片</li></ul></li><li>(对测试的时候,不做增强,首先对图片按照短边resize到256,再随机Crop到244,然后Normalize)</li><li>采用Xavier初始化</li><li>使用加Nesterov加速的SGD(NAG)<ul><li>batch-size: 256</li><li>共训练120 epoch</li><li>lr 0.1(30,60,90 epoch上除以10)</li></ul></li></ul><h2 id=efficient-training class=heading-element><span>Efficient Training</span>
<a href=#efficient-training class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=large-batch-training class=heading-element><span>Large-batch training</span>
<a href=#large-batch-training class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>与小批量训练的模型相比，使用大批量训练训练的模型的验证精度降低</p><p>如何解决：四种启发式方法，有助于扩大单机训练的批处理规模</p><blockquote><p>Linear scaling learning rate 线性缩放学习率。</p><blockquote><p>在小批量SGD中，梯度下降是一个随机过程，因为每批样本都是随机选取的。增加批处理大小并不会改变随机梯度的期望，但会降低其方差。换句话说，大的batch size会降低gradient中的noise。</p><p>随批大小线性增加学习率对ResNet-50训练有效。选取0.1作为批量大小为256的初始学习率，那么当批量大小为b时，我们将初始学习率提高到0.1 × b/256。</p></blockquote><p>learning rate warmup</p><blockquote><ul><li><p>一开始使用较小的学习率，然后在训练过程稳定时切换回初始学习率。</p></li><li><p>一种渐进的预热策略，将学习率从0线性增加到初始学习率。</p></li></ul></blockquote><p>Zero $\gamma$</p><blockquote><p>BN 的$\gamma$和$\beta$一般分别初始化为1和0</p><p>Zero $\gamma$：对位于残差块末端的所有BN层初始化$\gamma=0$。</p><p>因此，所有的残差块都只是返回它们的输入，模拟的网络，它的层数较少，在初始阶段更容易训练。Therefore, all residual blocks just return their inputs, mimics network that has less number of layers and is easier to train at the initial stage.</p></blockquote><p>No bias decay</p><blockquote><p>将权值衰减应用于卷积层和全连接层中的权值。其他参数，包括偏置和在BN层的γ和β，保持不正则化</p></blockquote></blockquote><h3 id=low-precision-training class=heading-element><span>Low-precision training</span>
<a href=#low-precision-training class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>将所有参数和激活存储在FP16中，并使用FP16计算梯度。同时，所有参数在FP32中都有一个副本，用于参数更新。此外，将一个标量乘以损失，以更好地将梯度范围对齐到FP16</p></blockquote><p><strong>结论</strong></p><center><img src="/images/Image Classification/ResNet.assets/resnet_vd_Efficient_Training.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Efficient_Training</div></center><p>与基线模型相比，1024批大小和FP16训练的模型甚至略微提高了0.5%的top-1精度。</p><p>仅通过线性缩放学习率将批量大小从256增加到1024会导致top-1准确率下降0.9%，而堆叠其余三个启发式方法可以弥补这一差距。训练结束时从FP32切换到FP16不会影响精度。</p><h2 id=model-tweaks class=heading-element><span>Model Tweaks</span>
<a href=#model-tweaks class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><table border=0><tr><td align=center><img src="/images/Image Classification/ResNet.assets/resnet_vd_model_0.png"></td><td align=center><img src="/images/Image Classification/ResNet.assets/resnet_vd_model_1.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Resnet50</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">ResnetB-C-D</td></tr></table><p>ResNet-B：前两个卷积的步长进行了切换</p><blockquote><p>Path A中的卷积忽略了四分之三的输入特征映射，因为它使用了一个跨步为2的内核大小1×1。</p></blockquote><p>ResNet-C：Input stem 的$7\times7$卷积替换成3个$3\times 3$</p><p>ResNet-B：ResNet-B基础上增加一个stride为2的2×2平均池化层，将$1\times1$卷积stride改为1</p><blockquote><p>Path B中的卷积忽略了四分之三的输入特征映射，因为它使用了一个跨步为2的内核大小1×1。</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/resnet_vd_model_2.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">model_conclusion</div></center><h2 id=training-refinements class=heading-element><span>Training Refinements</span>
<a href=#training-refinements class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>Cosine learning rate decay</strong></p><blockquote><p>$\eta_{t}=\frac{1}{2}\left(1+\cos \left(\frac{t \pi}{T}\right)\right) \eta $，其中是$\eta$初始化学习率。</p></blockquote><p><strong>label smoothing</strong></p><blockquote><p>通常分类任务中每张图片的标签是one hot形式的，也就是说一个向量在其对应类别索引上设置为1，其他位置为0，形如[0,0,0,1,0,0]。</p><p>label smoothing就是将类别分布变得平滑一点，即</p><p>$q_{i}=\left{\begin{array}{ll}{1-\varepsilon} & {\text { if } i=y} \ {\varepsilon /(K-1)} & {\text { otherwise }}\end{array}\right. $</p><p>其中$q_{i}$就代表某一类的ground truth，例如如果\(i==y\)，那么其最终真实值就是$1-\varepsilon$，其它位置设置为$\varepsilon /(K-1)$,而不再是。这里的$\varepsilon$=0.1</p></blockquote><p><strong>Knowledge Distillation</strong></p><blockquote><p>T=20</p></blockquote><p><strong>Mixup</strong></p><blockquote><p>每次随机抽取两个样本进行加权求和得到新的样本，标签同样做加权操作。公式中的$\lambda\in[0,1]$是一个随机数，服从$\text{Beta}(\alpha,\alpha)$分布。$\alpha=0.2$</p><p>$\begin{aligned} \hat{x} &=\lambda x_{i}+(1-\lambda) x_{j} \ \hat{y} &=\lambda y_{i}+(1-\lambda) y_{j} \end{aligned} $</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/resnet_vd_training_refinements.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">training_refinements</div></center><p>蒸馏在ResNet上工作得很好，然而，它在Inception-V3和MobileNet上不太好。</p><blockquote><p>可能解释是：教师模型不是来自于学生的同一家庭，因此在预测中分布不同，给模型带来了负面影响</p></blockquote><h2 id=拓展阅读-6 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://www.slideshare.net/DongminChoi6/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-review-cdm target=_blank rel="external nofollow noopener noreferrer">Bag of tricks for image classification with convolutional neural networks review [cdm]</a></p><p><a href=https://arxiv.org/abs/2110.00476 target=_blank rel="external nofollow noopener noreferrer">ResNet strikes back: An improved training procedure in timm</a> Top-1：80.4%</p><h2 id=res2net class=heading-element><span>Res2Net</span>
<a href=#res2net class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1904.01169 target=_blank rel="external nofollow noopener noreferrer">Res2Net: A New Multi-scale Backbone Architecture</a></p><p>作者：Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip Torr</p><p>发表时间：(TPAMI 2020)</p><p><a href=https://github.com/Res2Net target=_blank rel="external nofollow noopener noreferrer">官方源码</a></p></blockquote><p>通过在一个残差块中构筑类似残差分层的方式进行连接。Res2Net 可以在更细粒度级别表达多尺度特征，并且可以增加每层网络的感受野大小。</p><p>Res2Net 揭示了一个新的提升模型精度的维度，即 scale，其是除了深度、宽度和基数的现有维度之外另外一个必不可少的更有效的因素。</p><table border=0><tr><td align=center><img src="/images/Image Classification/ResNet.assets/Res2Net_Module.png"></td><td align=center><img src="/images/Image Classification/ResNet.assets/Res2Net_Module_1.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Res2Net_Module：s=4</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Res2Net_Module + group_conv + SE_block集成</td></tr></table><p>在$1\times1$卷积层后面，将特征图分为s个子集（$s$为尺度（scale）维度）</p><blockquote><p>原有的$n$通道$3\times3$滤波器替换为一系列有$w$通道的更小的滤波器组（避免损失，令$n = s × w$）;子集有着和原始特征图集相同的空间大小。</p></blockquote><p>每一组滤波器先从一组输入特征图中进行特征提取，然后与先前组生成的特征图和另一组输入的特征图一起被送到下一组卷积核进行处理。</p><blockquote><p>小滤波器组以类似于残差的模式被逐层连接，这样可以增加输出特征能表达的不同尺度的数量。</p></blockquote><p>最终，所有特征图将被拼接在一起并被送到一组$1\times1$的卷积核处进行信息融合。</p><p>忽略了第一个分组的卷积层：这也是一种特征复用的形式，减少参数并增加$s $的数量</p><h2 id=拓展阅读-7 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-7 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://mmcheng.net/res2net/ target=_blank rel="external nofollow noopener noreferrer">作者博客</a></p><h2 id=rednet class=heading-element><span>RedNet</span>
<a href=#rednet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2103.06255 target=_blank rel="external nofollow noopener noreferrer">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></p><p>作者：<a href=https://duoli.org/ target=_blank rel="external nofollow noopener noreferrer">Duo Li</a>, <a href=https://github.com/hujie-frank target=_blank rel="external nofollow noopener noreferrer">Jie Hu</a>, <a href="https://scholar.google.com/citations?user=DsVZkjAAAAAJ" target=_blank rel="external nofollow noopener noreferrer">Changhu Wang</a>, <a href=https://github.com/lxtGH target=_blank rel="external nofollow noopener noreferrer">Xiangtai Li</a>, <a href="https://scholar.google.com/citations?user=iHoGTt4AAAAJ" target=_blank rel="external nofollow noopener noreferrer">Qi She</a>, <a href=https://github.com/zh460045050 target=_blank rel="external nofollow noopener noreferrer">Lei Zhu</a>, <a href=http://tongzhang-ml.org/ target=_blank rel="external nofollow noopener noreferrer">Tong Zhang</a>, <a href=https://cqf.io/ target=_blank rel="external nofollow noopener noreferrer">Qifeng Chen</a></p><p>发表时间：(CVPR 2021)</p><p><a href=https://github.com/d-li14/involution target=_blank rel="external nofollow noopener noreferrer">官方源码</a></p></blockquote><p><strong>普通convolution</strong></p><blockquote><p>空间不变性（spatial-agnostic）</p><blockquote><p>平移等价性</p><p>大小一般3x3，偏小</p></blockquote><p>通道特异性（channel-specific）</p><blockquote><p>不同通道包含不同语义信息</p><p>不同通道的卷积核存在冗余</p></blockquote><p>希望具有：自适应长距离关系建模</p></blockquote><p>involution</p><blockquote><p>通道不变性（channel-agnostic）: kernel privatized for different positions</p><p>空间特异性（spatial-specific）: kernel shared across different channels</p><p>kernel：$H\in R^{H\times W \times K\times K \times G}$</p><p>#groups： G</p></blockquote><center><img src="/images/Image Classification/ResNet.assets/RedNet_involution.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">RedNet_involution</div></center><p>针对输入feature map的一个坐标点上的特征向量，先通过 $\phi$ (FC-BN-ReLU-FC)和reshape (channel-to-space)变换展开成kernel的形状，从而得到这个坐标点上对应的involution kernel，再和输入feature map上这个坐标点邻域的特征向量进行Multiply-Add得到最终输出的feature map。</p><center><img src="/images/Image Classification/ResNet.assets/RedNet_involution_1.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">RedNet_involution</div></center><p>在 ResNet的stem中（使用$3\times 3$或$7\times7$ involution进行分类或密集预测）和trunk（对所有任务使用$7\times7$ involution）位置中的所有bottleneck位置上替换掉了$3\times 3$卷积，但保留了所有的$1\times 1$卷积用于通道映射和融合。这些精心重新设计的实体联合起来，形成了一种新的高效 Backbone 网络，称为 RedNet。</p><h2 id=拓展阅读-8 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-8 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://www.linkresearcher.com/theses/6ba69226-7c28-4e8d-8fe2-bba6e9496587 target=_blank rel="external nofollow noopener noreferrer">超越卷积、自注意力机制：强大的神经网络新算子involution</a></p><p><a href=https://www.yuque.com/lart/papers/frxyq3#FVXRR target=_blank rel="external nofollow noopener noreferrer">论文笔记</a></p><h2 id=dcdc class=heading-element><span>DCDC</span>
<a href=#dcdc class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2211.06163 target=_blank rel="external nofollow noopener noreferrer">Dual Complementary Dynamic Convolution for Image Recognition</a></p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan%2C+L" target=_blank rel="external nofollow noopener noreferrer">Longbin Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin%2C+Y" target=_blank rel="external nofollow noopener noreferrer">Yunxiao Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+S" target=_blank rel="external nofollow noopener noreferrer">Shumin Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+J" target=_blank rel="external nofollow noopener noreferrer">Jie Chen</a></p><p>发表时间：( 2022)</p></blockquote><p>在本文中，我们新颖地将特征建模为局部空间自适应**(LSA)<strong>和全局位移不变</strong>[GSI]**部分的组合，然后提出了一个双分支双互补动态卷积算子来正确处理这两类特征，显着增强了代表能力。基于所提出的算子构建的 DCDC-ResNets 的性能明显优于 ResNet 基线和大多数最先进的动态卷积网络，同时具有更少的参数和 FLOP。我们还对目标检测、实例和全景分割等下游视觉任务进行了迁移实验，以评估模型的泛化能力，实验结果显示出显着的性能提升</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-02 18:22:27">更新于 2023-06-02&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/ data-title=ResNet data-hashtags="Deep Learning,图像分类模型"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/ data-title=ResNet><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/ class=post-tag title="标签 - 图像分类模型">图像分类模型</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/image-classification/senet/ class=post-nav-item rel=prev title=SENet><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>SENet</a><a href=/posts/deeplearning/image-classification/inception/ class=post-nav-item rel=next title=Inception>Inception<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>