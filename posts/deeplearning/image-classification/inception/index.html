<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Inception - fengchen</title><meta name=author content><meta name=description content="Inception"><meta name=keywords content='Deep Learning,图像分类模型'><meta itemprop=name content="Inception"><meta itemprop=description content="Inception"><meta itemprop=datePublished content="2023-06-02T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-02T18:22:27+08:00"><meta itemprop=wordCount content="6998"><meta itemprop=keywords content="Deep Learning,图像分类模型"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/image-classification/inception/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="Inception"><meta property="og:description" content="Inception"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-02T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-02T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="图像分类模型"><meta name=twitter:card content="summary"><meta name=twitter:title content="Inception"><meta name=twitter:description content="Inception"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/inception/ title="Inception - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/ title=ResNet><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/efficientnet/ title=EfficientNet><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Inception","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/inception\/"},"genre":"posts","keywords":"Deep Learning, 图像分类模型","wordcount":6998,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/inception\/","datePublished":"2023-06-02T18:22:27+08:00","dateModified":"2023-06-02T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"作者"},"description":"Inception"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Inception</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
Anonymous</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-02 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-02>2023-06-02</time></span>&nbsp;<span title="6998 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 7000 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 14 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related work</a></li><li><a href=#motivation-and-high-level-considerations>Motivation and High Level Considerations</a></li><li><a href=#googlenet>GoogLeNet</a></li><li><a href=#training-methodlogy>Training Methodlogy</a></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction-1>Introduction</a></li><li><a href=#normalization-via-mini-batch-statistics>Normalization via Mini-Batch Statistics</a></li><li><a href=#inception-v2-architecture>Inception V2 Architecture</a></li><li><a href=#拓展阅读-1>拓展阅读</a></li></ul><ul><li><a href=#general-design-principles通用设计原则建议>General Design Principles通用设计原则（建议）</a></li><li><a href=#factorizing-convolutions-with-large-filter-size卷积分解>Factorizing Convolutions with Large Filter Size卷积分解</a></li><li><a href=#utility-of-auxiliary-classifiers辅助分类器>Utility of Auxiliary Classifiers辅助分类器</a></li><li><a href=#efficient-grid-size-reduction高效下采样技巧>Efficient Grid Size Reduction高效下采样技巧</a></li><li><a href=#inception-v3>Inception V3</a></li><li><a href=#label-smoothing标签平滑>Label Smoothing标签平滑</a></li><li><a href=#拓展阅读-2>拓展阅读</a></li></ul><ul><li><a href=#inception-v4>Inception-V4</a></li><li><a href=#inception-resnet-v1>Inception-ResNet-V1</a></li><li><a href=#inception-resnet-v2>Inception-ResNet-V2</a></li><li><a href=#scaling-of-the-residuals>Scaling of the Residuals</a></li><li><a href=#拓展阅读-3>拓展阅读</a></li></ul><ul><li><a href=#the-cxeption-architecture>The Cxeption architecture</a></li><li><a href=#effect-of-an-intermediate-activation-after-pointwise-convolutions>Effect of an intermediate activation after pointwise convolutions</a></li><li><a href=#拓展阅读-4>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><p>策略：<font color=#f12c60><strong>split-transform-merge</strong></font></p><h2 id=inceptionv1googlenet class=heading-element><span>InceptionV1（GoogLeNet）</span>
<a href=#inceptionv1googlenet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1409.4842 target=_blank rel="external nofollow noopener noreferrer">Going Deeper with Convolutions</a></p><p>作者：Christian Szegedy，Wei Liu，Yangqing Jia，Pierre Sermanet，Scott Reed，Dragomir Anguelov，Dumitru Erhan，Vincent Vanhoucke，Andrew Rabinovich</p><p>发表时间：(CVPR 2015)</p></blockquote><p>GoogLeNet深度卷积神经网络结构，及其后续变种Inception-V1、Inception-V2-Inception-V3、Inception-V4。</p><p>使用Inception模块，引入并行结构和不同尺寸的卷积核，提取不同尺度的特征，将稀疏矩阵聚合为较为密集的子矩阵，大大提高计算效率，降低参数数量。加入辅助分类器，实现了模型整合、反向传播信号放大。</p><p>GoogLeNet在ILSVRC-2014图像分类竞赛中获得了top-5误差6.7%的冠军成绩。</p><h2 id=introduction class=heading-element><span>Introduction</span>
<a href=#introduction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>启发文献</p><ul><li><p><a href=https://arxiv.org/abs/1312.4400 target=_blank rel="external nofollow noopener noreferrer">Network In Network</a></p><blockquote><p>$1\times1$卷积降维-升维</p><p>Global Average pooling层取代全连接层</p></blockquote></li><li><p><a href=https://arxiv.org/abs/1310.6343 target=_blank rel="external nofollow noopener noreferrer">Provable Bounds for Learning Some Deep Representations</a></p><blockquote><p>用稀疏、分散的网络取代以前庞大密集臃肿的网络</p></blockquote></li></ul><h2 id=related-work class=heading-element><span>Related work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://ieeexplore.ieee.org/document/6795724 target=_blank rel="external nofollow noopener noreferrer">LeNet</a>，<a href=http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf target=_blank rel="external nofollow noopener noreferrer">AlexNet</a>，<a href=https://arxiv.org/abs/1311.2901 target=_blank rel="external nofollow noopener noreferrer">ZFNet</a>，<a href=https://arxiv.org/abs/1312.4400 target=_blank rel="external nofollow noopener noreferrer">NiN</a>，<a href=https://arxiv.org/abs/1312.6229 target=_blank rel="external nofollow noopener noreferrer">overfeat</a></p><h2 id=motivation-and-high-level-considerations class=heading-element><span>Motivation and High Level Considerations</span>
<a href=#motivation-and-high-level-considerations class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>提高模型性能的传统方法：</p><ul><li>增加深度（层数）</li><li>增加宽度（卷积核个数）适用于大规模标注好的数据集</li></ul><p>产生的问题：</p><ul><li><p>标注成本高</p></li><li><p>计算效率问题</p><blockquote><p>两个相连卷积层,两层同步增加卷积核个数，计算量将平方增加
如果很多权重训练后接近0，这部分计算就被浪费掉了</p></blockquote></li></ul><h2 id=googlenet class=heading-element><span>GoogLeNet</span>
<a href=#googlenet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>原始Inception模块通道数越来越多，计算量爆炸。</p><center><img src="/images/Image Classification/Inception.assets/GoogLeNet_原始Inception.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">原始Inception</div></center><p>每个 <strong>Inception</strong> 结构有 4 个分支，主要包含 1x1, 3x3, 5x5 卷积核和 max pooling 操作的步长为1，以保持输出特征层的尺寸与卷积核输出尺寸一致。1x1 卷积核的作用是降维，以避免 cancatenation 操作导致特征层过深，并减少网络参数.</p><center><img src="/images/Image Classification/Inception.assets/GoogLeNet_优化Inception.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">优化Inception</div></center>太过密集压缩的嵌入向量不便于模型处理；只在$3\times3$、$5\times5$卷积层之前用$1\times1$降维。<center><img src="/images/Image Classification/Inception.assets/GoogLeNet_网络结构图1.png" /></center><center><img src="/images/Image Classification/Inception.assets/GoogLeNet_网络结构图2.png" /><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">GoogLeNet</div></center><blockquote><p>#$3\times3$reduce：$3\times3$卷积之前的$1\times1$卷积，其他reduce同理</p></blockquote><blockquote><p>pool proj：池化后的$1\times1$卷积</p><p>所有卷积使用relu激活函数</p><p>GAP：全局平均池化 一个channel用一个平均值代表取代全连接层，减少参数量。</p><blockquote><p>便于fine-tune迁移学习</p><p>提升了0.6%的TOP1准确率</p></blockquote></blockquote><p>原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。</p><p>Stem Network:Conv-Pool-2x Conv-Pool（底层先用普通卷积层，后面用9个Inception模块叠加）</p><p>Classifier output(removed expensive FC layers!)</p><p>Auxiliary classification outputs to inject additional gradient at lower layers</p><blockquote><p>(AvgPool-1x1Conv-FC-FC-Softmax)</p><p>在4a和4d后面加辅助分类层</p><blockquote><p>改善梯度消失</p><p>正则化</p><p>让浅层也能学习到区分特征</p><blockquote><p>其实没太大用处，在v2/v3版本去掉</p></blockquote></blockquote></blockquote><p>训练时损失函数：$L=L_{最后}+0.3\times L_{辅1}+0.3\times L_{辅2}$</p><p>测试阶段：去掉辅助分类器</p><h2 id=training-methodlogy class=heading-element><span>Training Methodlogy</span>
<a href=#training-methodlogy class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>数据并行：一个batch均分k份，让不同节点前向和反向传播，再由中央param sever优化更新权重</p><p>asynchronous stochastic gradient descent：异步随机梯度下降</p><p>图像增强</p><blockquote><p>裁剪为原图8%-100%之间，宽高比3/4和4/3之间；</p><p>等概率使用不同插值方法（双线性，区域，最近邻，三次函数）</p></blockquote><p>裁剪：</p><blockquote><p>将原图缩放为短边长度256，288，320，352的四个尺度</p><p>每个尺度裁剪出左中右（或上中下）三张小图</p><p>每张小图取四个角和中央的五张$224\times224$的patch以及每张小图缩放为$224\times224$，共6个patch同时取镜像</p><p>$4\times3\times6\times2=144$个patch</p></blockquote><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.youtube.com/watch?v=ySrj_G5gHWI" target=_blank rel="external nofollow noopener noreferrer">ILSVRC2014竞赛汇报</a></p><p><a href=https://my.oschina.net/u/876354/blog/1637819 target=_blank rel="external nofollow noopener noreferrer">博客</a></p><p>Hebbian原则理解</p><blockquote><p>1、网络更容易过拟合，当数据集不全的时候，过拟合更容易发生，于是我们需要为网络feed大量的数据，但是制作样本集本身就是一件复杂的事情。
2、大量需要更新的参数就会导致需要大量的计算资源，而当下即使硬件快速发展，这样庞大的计算也是很昂贵的</p><p>解决以上问题的根本方法就是把全连接的网络变为稀疏连接（卷积层其实就是一个稀疏连接），当某个数据集的分布可以用一个稀疏网络表达的时候就可以通过分析某些激活值的相关性，将相关度高的神经元聚合，来获得一个稀疏的表示。
这种方法也呼应了Hebbian principle，一个很通俗的现象，先摇铃铛，之后给一只狗喂食，久而久之，狗听到铃铛就会口水连连。这也就是狗的“听到”铃铛的神经元与“控制”流口水的神经元之间的链接被加强了，而Hebbian principle的精确表达就是如果两个神经元常常同时产生动作电位，或者说同时激动（fire），这两个神经元之间的连接就会变强，反之则变弱（neurons that fire together, wire together）</p></blockquote><p><a href=https://dgschwend.github.io/netscope/#/preset/googlenet target=_blank rel="external nofollow noopener noreferrer">可视化GoogLeNet</a></p><h2 id=inceptionv2bn-inception class=heading-element><span>InceptionV2（BN-Inception）</span>
<a href=#inceptionv2bn-inception class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1502.03167 target=_blank rel="external nofollow noopener noreferrer">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><p>作者：Sergey Ioffe, Christian Szegedy</p><p>发表时间：(ICML 2015)</p></blockquote><h2 id=abstract class=heading-element><span>Abstract</span>
<a href=#abstract class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>训练慢和困难：<strong>internal covariate shift</strong> (ICS)</p><blockquote><p>低学习率；参数初始化</p></blockquote><p>Batch Normalization</p><blockquote><p>加快模型训练速度；加速收敛</p><blockquote><p>可以使用更高的学习率；参数初始化</p><p>和当前最好的分类网络相比训练步骤降低14倍</p></blockquote><p>具有一定正则化作用</p><blockquote><p>在某些情况，减少Dropout的使用</p></blockquote><p>使模型效果更好（并不是所有模型用了BN 就会更好）</p><blockquote><p>top-5：4.9%；</p><p>test error：4.8%</p><p>超过了人工评分的准确性。</p></blockquote></blockquote><h2 id=introduction-1 class=heading-element><span>Introduction</span>
<a href=#introduction-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>使用mini-batch</strong></p><blockquote><p>小批量的损失梯度是对训练集上梯度的估计，其质量随着批量大小的增加而提高。</p><p>现代计算平台提供的并行性，批处理的计算比单个示例的m次计算效率要高得多。</p></blockquote><p><strong>internal covariate shift</strong>：在深度学习网络的训练过程中网络内部结点的分布变化称为内部协变量偏移</p><blockquote><p>每一层数据的微小变化都会随着网络一层一层的传递而被逐渐放大。</p></blockquote><h2 id=normalization-via-mini-batch-statistics class=heading-element><span>Normalization via Mini-Batch Statistics</span>
<a href=#normalization-via-mini-batch-statistics class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>白化（Whitening）:对输入数据分布进行变换</p><blockquote><ul><li>使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；</li><li>去除特征之间的相关性。</li></ul><blockquote><ul><li><p>白化过程计算成本太高</p></li><li><p>白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力</p></li></ul></blockquote></blockquote><p>Batch Normalization：简化白化</p><blockquote><p>单独对每个特征标准化参数，使其具有零均值和单位方差。</p><p>引入了两个可学习的参数$\gamma$与$\beta$，这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换。</p><blockquote><p>对全连接层，作用在特征维</p><p>对卷积层，作用在通道维</p></blockquote></blockquote><p>如果batch size为$m$，则在前向传播过程中，网络中每个节点都有$m$个输出，Batch Normalization就是对该层每个节点的这$m$个输出进行归一化再输出，具体计算方式如下：</p><center><img src="/images/Image Classification/Inception.assets/InceptionV2_BN.png" /><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BN</div></center><blockquote><ul><li><strong>Standardization</strong>：首先对$m$个$x$进行 Standardization，得到 zero mean unit variance的分布$\hat x$。</li><li><strong>scale and shift</strong>：然后再$ \hat x$对进行scale and shift，缩放并平移到新的分布$y$，具有新的均值$\beta$方差$\gamma$。</li></ul></blockquote>$$
y_i^{(b)}=BN_{(x_i)^{(b)}}=\gamma \cdot (\frac{x_i^{(b)}-\mu(x_i)}{\sqrt {\sigma(x_i)^2+\epsilon}})+\beta
$$<blockquote><blockquote><p>$\mu$和$\sigma$为该行的均值和标准差，$\epsilon$为防止除零引入的极小量（可忽略）</p><p>$\gamma$和$\beta$为scale和shift参数,以提高表现力</p></blockquote><center><img src="/images/Image Classification/Inception.assets/InceptionV2_BN反向传播.png" /><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BN反向传播</div></center><center><img src="/images/Image Classification/Inception.assets/InceptionV2_BN反向传播计算图.png" /><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">BN反向传播</div></center></blockquote><blockquote><p><a href=https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html target=_blank rel="external nofollow noopener noreferrer">Understanding the backward pass through Batch Normalization Layer</a></p></blockquote><p>训练阶段计算的是每一个batch的均值和方差，但是测试时用的是训练后的（指数加权平均）的均值和方差（吴恩达）</p><h2 id=inception-v2-architecture class=heading-element><span>Inception V2 Architecture</span>
<a href=#inception-v2-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Image Classification/Inception.assets/InceptionV2_网络结构图.png" /><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">InceptionV2 网络结构图</div></center><p>与<a href=##GoogLeNet><strong>Inception V1</strong></a>对比</p><blockquote><p>Inception V1的$5\times 5$卷积被替换成两个$3\times 3$</p><blockquote><p>使网络的最大深度增加9层。增加了25%的参数，计算成本增加了约30%。</p></blockquote><p>Inception(3X)（特征图为$28\times28$）模块从2个变成3个。（Inception3a,b——>Inception3a,b,c）</p><p>模块内部有时使用平均池化，有时使用最大池化</p><p>模块3c, 4e的过滤器连接之前使用了stride-2卷积/池化层。</p><p>在第一层卷积层上采用深度乘子8的可分离卷积。</p><blockquote><p>减少了计算成本，同时增加了训练时的内存消耗。</p></blockquote></blockquote><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://blog.csdn.net/qq_37541097/article/details/104434557?spm=1001.2014.3001.5501" target=_blank rel="external nofollow noopener noreferrer">Batch Normalization详解以及pytorch实验</a></p><p><a href=https://www.cnblogs.com/shine-lee/p/11989612.html target=_blank rel="external nofollow noopener noreferrer">Batch Normalization详解</a></p><p><a href=https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html target=_blank rel="external nofollow noopener noreferrer">Understanding the backward pass through Batch Normalization Layer</a></p><p><a href=https://zhuanlan.zhihu.com/p/50444499 target=_blank rel="external nofollow noopener noreferrer">深入解读Inception V2之Batch Normalization（附源码）</a></p><p><a href=https://zhuanlan.zhihu.com/p/34879333 target=_blank rel="external nofollow noopener noreferrer">Batch Normalization原理与实战</a></p><p><a href=https://www.jianshu.com/p/aa5a13f99d91 target=_blank rel="external nofollow noopener noreferrer">理解Batch Normalization系列3——为什么有效及11个问题</a></p><p><a href=https://www.zhihu.com/question/283715823 target=_blank rel="external nofollow noopener noreferrer">Batch-normalized 应该放在非线性激活层的前面还是后面？</a></p><p><a href=https://arxiv.org/abs/1805.11604 target=_blank rel="external nofollow noopener noreferrer">How Does Batch Normalization Help Optimization?</a></p><blockquote><p><strong>BN层让损失函数更平滑</strong>。通过分析训练过程中每步梯度方向上步长变化引起的损失变化范围、梯度幅值的变化范围、光滑度的变化，认为添<strong>加BN层后，损失函数的landscape(loss surface)变得更平滑，相比高低不平上下起伏的loss surface，平滑loss surface的梯度预测性更好，可以选取较大的步长</strong>。</p><p>对比了标准VGG以及加了BN层的VGG每层分布随训练过程的变化，发现两者并无明显差异，认为BatchNorm并没有改善 <strong>Internal Covariate Shift</strong>。</p></blockquote><p><a href=https://www.reddit.com/r/MachineLearning/comments/8n4eot/r_how_does_batch_normalization_help_optimization/ target=_blank rel="external nofollow noopener noreferrer">How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)</a></p><p><a href=https://arxiv.org/abs/1612.04010 target=_blank rel="external nofollow noopener noreferrer">An empirical analysis of the optimization of deep network loss surfaces</a></p><blockquote><p><strong>BN更有利于梯度下降</strong>。绘制了VGG和NIN网络在有无BN层的情况下，loss surface的差异，包含初始点位置以及不同优化算法最终收敛到的local minima位置。<strong>没有BN层的，其loss surface存在较大的高原，有BN层的则没有高原，而是山峰，因此更容易下降。</strong></p></blockquote><h2 id=inceptionv3 class=heading-element><span>InceptionV3</span>
<a href=#inceptionv3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1512.00567 target=_blank rel="external nofollow noopener noreferrer">Rethinking the Inception Architecture for Computer Vision</a></p><blockquote><p><a href=https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py target=_blank rel="external nofollow noopener noreferrer">Pytorch官方Inception-V3</a></p></blockquote><p>作者：Szegedy, Christian, et al</p><p>发表时间：(CVPR 2016)</p></blockquote><p>本论文在<a href=https://arxiv.org/abs/1409.4842 target=_blank rel="external nofollow noopener noreferrer">GoogLeNet</a>和<a href=https://arxiv.org/abs/1502.03167 target=_blank rel="external nofollow noopener noreferrer">BN-Inception</a>的基础上，对Inception模块的结构、性能、参数量和计算效率进行了重新思考和重新设计。提出了Inception V2和Inception V3模型，取得了3.5%左右的Top-5错误率。</p><p>Inception V3具有强大的图像特征抽取和分类性能，是常用的迁移学习主干网络基模型。</p><h2 id=general-design-principles通用设计原则建议 class=heading-element><span>General Design Principles通用设计原则（建议）</span>
<a href=#general-design-principles%e9%80%9a%e7%94%a8%e8%ae%be%e8%ae%a1%e5%8e%9f%e5%88%99%e5%bb%ba%e8%ae%ae class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li><p>避免过度降维或收缩特征Bottleneck（避免过度的1 x 1卷积，特别是在网络浅层）</p><blockquote><p>feature map的长宽大小应该随网络加深缓慢减小</p><p>降维会造成各通道间的相关性信息丢失，仅反应了致密的嵌入信息</p></blockquote></li><li><p>独立的特征越多收敛越快（尽可能在分类层之前增加通道数）</p><blockquote><p>相互独立特征越多，输入的信息分解的越彻底</p><p>Hebbin原理</p></blockquote></li><li><p>大卷积核卷积之前可用1x1卷积降维（3x3或5x5卷积之前可先用1x1卷积降维，可保留相邻单元的强相关性）</p><blockquote><p>大尺度卷积：聚合空间信息大感受野</p><p>相邻感受野的卷积结果</p><blockquote><p>邻近单元的强相关性在降维过程中信息损失很少</p></blockquote></blockquote></li><li><p>均衡网络的宽度和深度</p><blockquote><p>两者同时提升，既可以提升性能，也能提升计算效率</p></blockquote></li></ul><h2 id=factorizing-convolutions-with-large-filter-size卷积分解 class=heading-element><span>Factorizing Convolutions with Large Filter Size卷积分解</span>
<a href=#factorizing-convolutions-with-large-filter-size%e5%8d%b7%e7%a7%af%e5%88%86%e8%a7%a3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>$5\times5$卷积分解成2个$3\times3$卷积；减少参数数量</p><blockquote><p>分解卷积是否会影响模型表达能力？</p><p>是否需保留第一层的非线性激活函数？</p><p>增加非线性可学习空间增强了</p><center><img src="/images/Image Classification/Inception.assets/InceptionV3_卷积分解0.png" width=400   height=300><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Inception Module A</div></center></blockquote><p>$3\times3$卷积分解成$3\times1$卷积和$1\times3$卷积非对称（空间可分离卷积）</p><blockquote><p>$n\times n$卷积分解成$n \times1$卷积和$1\times n$卷积</p><p>n越大，节省的运算量越大</p><center><img src="/images/Image Classification/Inception.assets/InceptionV3_卷积分解1.png" width=400 height=300><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Inception Module B</div></center><p>不对称卷积分解在靠前的层效果不好，适用于feature map尺寸在12-20之间</p><p>拓展滤波器组（加宽网络，升维）在最后分类层之前，用该模块拓展特征维度，生成高维稀疏特征。</p><center><img src="/images/Image Classification/Inception.assets/InceptionV3_卷积分解2.png" width=400 height=300><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Inception Module C</div></center></blockquote><h2 id=utility-of-auxiliary-classifiers辅助分类器 class=heading-element><span>Utility of Auxiliary Classifiers辅助分类器</span>
<a href=#utility-of-auxiliary-classifiers%e8%be%85%e5%8a%a9%e5%88%86%e7%b1%bb%e5%99%a8 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>提出辅助分类器并不能帮助模型更快收敛和更快的特征演化。</p><p>增加了BN层和Dropout层的辅助分类器可以起到正则化作用。</p><h2 id=efficient-grid-size-reduction高效下采样技巧 class=heading-element><span>Efficient Grid Size Reduction高效下采样技巧</span>
<a href=#efficient-grid-size-reduction%e9%ab%98%e6%95%88%e4%b8%8b%e9%87%87%e6%a0%b7%e6%8a%80%e5%b7%a7 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ul><li><p>先卷积再池化（计算量大）</p><center><img src="/images/Image Classification/Inception.assets/InceptionV3_先卷积再池化.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">先卷积再池化</div></center></li><li><p>步长为2的卷积（大量信息丢失，违反原则1）</p><center><img src="/images/Image Classification/Inception.assets/InceptionV3_步长为2的卷积.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">步长为2的卷积化</div></center></li></ul><h2 id=inception-v3 class=heading-element><span>Inception V3</span>
<a href=#inception-v3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p><a href=https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py target=_blank rel="external nofollow noopener noreferrer">Inception-v3 的 PyTorch 版本</a></p><p><a href=https://cloud.google.com/tpu/docs/inception-v3-advanced target=_blank rel="external nofollow noopener noreferrer">Google Cloud 上的 Inception-v3</a></p></blockquote><center><img src="/images/Image Classification/Inception.assets/InceptionV3_网络图.png"></center><center><img src="/images/Image Classification/Inception.assets/InceptionV3_网络图1.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">InceptionV3</div></center><p>figure5：$5\times5$卷积分解成2个$3\times3$卷积</p><p>figure6：空间卷积可分离卷积</p><p>figure7：拓展滤波器组</p><h2 id=label-smoothing标签平滑 class=heading-element><span>Label Smoothing标签平滑</span>
<a href=#label-smoothing%e6%a0%87%e7%ad%be%e5%b9%b3%e6%bb%91 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://arxiv.org/abs/1906.02629 target=_blank rel="external nofollow noopener noreferrer">When Does Label Smoothing Help?</a></p><p>标签平滑的目的是防止最大的 logit 变得比所有其他 logit 大得多</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>new_labels</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=err>—</span> <span class=n>ε</span><span class=p>)</span> <span class=o>*</span> <span class=n>one_hot_labels</span> <span class=o>+</span> <span class=n>ε</span> <span class=o>/</span> <span class=n>K</span></span></span></code></pre></td></tr></table></div></div><p>其中 ε 是 0.1，这是一个超参数，K 是 1000，这是类的数量。在分类器层观察到的一种dropout效应。</p><p>标签用one-hot独热编码</p><blockquote>$$
L=-\sum_{i=1}^{k}q_ilog{p_i}=-log{p_y}=-z_y+log{(\sum_{i=1}^k e^{z_i})}
$$<p>可能导致过拟合</p><p>它鼓励最大的逻辑单元与所有其它逻辑单元之间的差距变大，与有界限的梯度∂ℓ/∂zk相结合，这会降低模型的适应能力。</p>$$
z^* =
\begin{cases}
\log{\frac{(k-1)(1-\varepsilon)}{\varepsilon}} + \alpha & \text{if } i = y \\
    \alpha & \text{if } i \neq y
\end{cases}
$$</blockquote><h2 id=拓展阅读-2 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://cloud.google.com/tpu/docs/inception-v3-advanced target=_blank rel="external nofollow noopener noreferrer">在 Cloud TPU 上运行 Inception v3 的高级指南</a></p><p><a href=https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c target=_blank rel="external nofollow noopener noreferrer">博客</a></p><p><a href=https://stackoverflow.com/questions/39335535/label-smoothing-soft-targets-in-pandas target=_blank rel="external nofollow noopener noreferrer">Label Smooth的Pandas实现小例子</a></p><p><a href=https://www.zhihu.com/question/65339831 target=_blank rel="external nofollow noopener noreferrer">知乎：神经网络中的label smooth为什么没有火？</a></p><h2 id=inceptionv4 class=heading-element><span>InceptionV4</span>
<a href=#inceptionv4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1602.07261 target=_blank rel="external nofollow noopener noreferrer">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></p><p>作者：Szegedy C , Ioffe S , Vanhoucke V , et al.</p><p>发表时间：(AAAI 2017)</p></blockquote><p>提出了Inception-V4、Inception-ResNet-V1、Inception-ResNet-V2三个模型。</p><p>Inception-V4在Inception-V3的基础上进一步改进了Inception模块，提升了模型性能和计算效率。</p><p>Inception-V4没有使用残差模块，</p><p>Inception-ResNet将Inception模块和深度残差网络ResNet结合，提出了三种包含残差连接的Inception模块，残差连接显著加快了训练收敛速度。</p><p>Inception-ResNet-V2和Inception-V4的早期stem网络结构相同。</p><p>Inception-ResNet-V1和Inception-V3准确率相近，Inception-ResNet-V2和Inception-V4准确率相近。</p><p>经过模型集成和图像多尺度裁剪处理后，模型Top-5错误率降低至3.1%。</p><p>针对卷积核个数大于1000时残差模块早期训练不稳定的问题，提出了对残差分支幅度缩小的解决方案。</p><h2 id=inception-v4 class=heading-element><span>Inception-V4</span>
<a href=#inception-v4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>V：不使用padding</p><p>不加V：same padding</p><blockquote><p>如果padding设置为SAME，则说明输入图片大小和输出图片大小是一致的</p></blockquote></blockquote><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_网络图.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_stem.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">InceptionV4</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Stem主干网络</td></tr></table><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_ModuleA.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_ModuleB.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_ModuleC.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">InceptionV4_Module_A</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">InceptionV4_Module_B</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">InceptionV4_Module_C</td></tr></table><blockquote><p>模块A输出Grid Size：$35\times35$</p><p>模块B输出Grid Size：$17\times17$</p><p>模块C输出Grid Size：$8\times8$</p></blockquote><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_ReductionA.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_ReductionB.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">ReductionA</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">ReductionB</td></tr></table><h2 id=inception-resnet-v1 class=heading-element><span>Inception-ResNet-V1</span>
<a href=#inception-resnet-v1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>性能和InceptionV3相近</p></blockquote><p>带残差模块的Inception</p><blockquote><p>Inception之后使用不带激活函数的$1\times1$卷积：升维拓展filter bank ，匹配输入维度</p><p>在相加层之后不做BN，减少计算量。</p></blockquote><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v1_网络图.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v1_Stem.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v1</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Stem主干网络</td></tr></table><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v1_ModuleA.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v1_ModuleB.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v1_ModuleC.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v1_Module_A</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v1_Module_B</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v1_Module_C</td></tr></table><blockquote><p>模块A输出Grid Size：$35\times35$</p><p>模块B输出Grid Size：$17\times17$</p><p>模块C输出Grid Size：$8\times8$</p></blockquote><h2 id=inception-resnet-v2 class=heading-element><span>Inception-ResNet-V2</span>
<a href=#inception-resnet-v2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>Inception-ResNet-V1和Inception-ResNet-V2网络总体结构一样</p><p>Inception-ResNet-V2和InceptinV4主干网络一样</p><p>性能和InceptionV4相近</p></blockquote><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v1_网络图.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/InceptionV4_stem.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-V2</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Stem主干网络</td></tr></table><table border=0><tr><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v2_ModuleA.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v2_ModuleB.png"></td><td align=center><img src="/images/Image Classification/Inception.assets/Inception-ResNet-v2_ModuleC.png"></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v2_Module_A</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v2_Module_B</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Inception-ResNet-v2_Module_C</td></tr></table><blockquote><p>模块A输出Grid Size：$35\times35$</p><p>模块B输出Grid Size：$17\times17$</p><p>模块C输出Grid Size：$8\times8$</p></blockquote><h2 id=scaling-of-the-residuals class=heading-element><span>Scaling of the Residuals</span>
<a href=#scaling-of-the-residuals class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/Image Classification/Inception.assets/InceptionV4_Scaling of the Residuals.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">InceptionV4_Scaling of the Residuals</div></center><p>对残差块输出进行幅度减小</p><blockquote><p>在加法融合之前，对残差分支的结果乘以幅度缩小系数</p></blockquote><h2 id=拓展阅读-3 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-3 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://arxiv.org/pdf/1602.07261.pdf target=_blank rel="external nofollow noopener noreferrer">论文版本1</a></p><p><a href=https://pdfkul.com/inception-v4-inception-resnet-and-the-impact-of-residual-_59c079301723dd9a437a8853.html target=_blank rel="external nofollow noopener noreferrer">论文版本2</a></p><p><a href=https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14806 target=_blank rel="external nofollow noopener noreferrer">论文版本3</a></p><p><a href=https://blog.csdn.net/stesha_chen/article/details/82115429 target=_blank rel="external nofollow noopener noreferrer">Inception-V4和Inception-Resnet论文阅读和代码解析</a></p><p><a href=https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py target=_blank rel="external nofollow noopener noreferrer">Inception-V4源代码</a></p><p><a href=https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py target=_blank rel="external nofollow noopener noreferrer">Inception-ResNet-V2源代码</a></p><p><a href=https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc target=_blank rel="external nofollow noopener noreferrer">Inception英文综述博客</a></p><h2 id=xception class=heading-element><span>Xception</span>
<a href=#xception class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1610.02357v3 target=_blank rel="external nofollow noopener noreferrer">Xception: Deep Learning with Depthwise Separable Convolutions</a></p><p>作者：Francois Chollet</p><blockquote><p><a href=https://github.com/fchollet target=_blank rel="external nofollow noopener noreferrer">Github主页</a></p><p><a href=https://twitter.com/fchollet target=_blank rel="external nofollow noopener noreferrer">推特</a></p><p><a href="https://scholar.google.com/citations?user=VfYhf2wAAAAJ&amp;hl=en" target=_blank rel="external nofollow noopener noreferrer">谷歌学术</a></p><p><a href=https://www.linkedin.com/in/fchollet/ target=_blank rel="external nofollow noopener noreferrer">领英</a></p></blockquote><p>发表时间：(CVPR 2017)</p></blockquote><p>谷歌Xception，将深度可分离卷积引入Inception模块，实现长宽方向的空间信息和跨通道信息的完全解耦。X代表Extreme，极致。</p><p>在ImageNet数据集和JFT数据集两个大规模图像分类任务上，收敛速度、最终准确率都超过Inception V3。</p><p>Xception作者为深度学习框架Keras作者François Chollet。在Keras中可调用预训练的Xception模型作为迁移学习的骨干网络。</p><p>VGG：经典串行堆叠深度</p><p>Inception：拓展多分支宽度，分别处理（解耦）再整合汇总</p><p>在 DeeplabV3+ 中，作者将 Xception 做了进一步的改进，同时增加了 Xception 的层数，设计出了 Xception65 和 Xception71 的网络。</p><h2 id=the-cxeption-architecture class=heading-element><span>The Cxeption architecture</span>
<a href=#the-cxeption-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>假设：跨通道信息和长宽方向的空间信息可完全分离解耦</p><p>Xception与标准可分离卷积的区别</p><blockquote><p><a href=https://arxiv.org/abs/1704.04861 target=_blank rel="external nofollow noopener noreferrer">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> (2017)</p></blockquote><ul><li><p>顺序不同</p><center><img src="/images/Image Classification/Inception.assets\Xception_与深度可分离卷积的区别.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Xception与深度可分离卷积的区别</div></center></li><li><p>Xception中使用非线性激活函数ReLu</p></li></ul><blockquote><p>常规卷积：一个卷积核处理所有通道</p><p>深度可分离卷积：一个卷积核处理一个通道</p></blockquote><p>SeperableConv包含$1\times1$卷积+深度可分离卷积+合并</p><p>极限版本：每个$3\times3$卷积单独处理一个通道</p><center><img src="/images/Image Classification/Inception.assets\Xception_extreme version.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">extreme version of Inception module</div></center><h2 id=effect-of-an-intermediate-activation-after-pointwise-convolutions class=heading-element><span>Effect of an intermediate activation after pointwise convolutions</span>
<a href=#effect-of-an-intermediate-activation-after-pointwise-convolutions class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>非线性激活对空间-通道未解耦时有用</p><p>对$1\times1$卷积后的特征图，非线性激活会导致信息丢失，不利于后续的深度可分离卷积。</p><h2 id=拓展阅读-4 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-4 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://github.com/keras-team/keras/blob/master/keras/applications/xception.py target=_blank rel="external nofollow noopener noreferrer">Keras中的Xception预训练模型</a></p><p><a href=https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec target=_blank rel="external nofollow noopener noreferrer">博客</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-02 18:22:27">更新于 2023-06-02&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/inception/ data-title=Inception data-hashtags="Deep Learning,图像分类模型"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/inception/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/inception/ data-title=Inception><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/ class=post-tag title="标签 - 图像分类模型">图像分类模型</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/image-classification/resnet/ class=post-nav-item rel=prev title=ResNet><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>ResNet</a><a href=/posts/deeplearning/image-classification/efficientnet/ class=post-nav-item rel=next title=EfficientNet>EfficientNet<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/></a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>