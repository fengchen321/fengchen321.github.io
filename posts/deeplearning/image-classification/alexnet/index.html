<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>AlexNet - fengchen</title><meta name=author content="fengchen"><meta name=description content="AlexNet"><meta name=keywords content='Deep Learning,图像分类模型'><meta itemprop=name content="AlexNet"><meta itemprop=description content="AlexNet"><meta itemprop=datePublished content="2023-06-02T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-02T18:22:27+08:00"><meta itemprop=wordCount content="4094"><meta itemprop=keywords content="Deep Learning,图像分类模型"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="AlexNet"><meta property="og:description" content="AlexNet"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-02T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-02T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="图像分类模型"><meta name=twitter:card content="summary"><meta name=twitter:title content="AlexNet"><meta name=twitter:description content="AlexNet"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/ title="AlexNet - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/automl/ title=AutoML><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/light-weight/squeezenet/ title=SqueezeNet><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/index.md title="AlexNet - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"AlexNet","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/alexnet\/"},"genre":"posts","keywords":"Deep Learning, 图像分类模型","wordcount":4094,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/image-classification\/alexnet\/","datePublished":"2023-06-02T18:22:27+08:00","dateModified":"2023-06-02T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"AlexNet"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>AlexNet</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-02 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-02>2023-06-02</time></span>&nbsp;<span title="4094 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 4100 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 9 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#the-dataset>The Dataset</a></li><li><a href=#the-architecture>The Architecture</a><ul><li><a href=#relu激活函数>ReLu激活函数</a></li><li><a href=#多gpu模型并行>多GPU模型并行</a></li><li><a href=#lrn局部响应归一化>LRN局部响应归一化</a></li><li><a href=#overlapping-pooling>Overlapping Pooling</a></li><li><a href=#总体结构>总体结构</a></li></ul></li><li><a href=#reducing-overfiting>Reducing Overfiting</a><ul><li><a href=#data-augmentation数据增强>Data Augmentation数据增强</a></li><li><a href=#dropout-随机失活>Dropout 随机失活</a></li></ul></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-1>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=alexnet class=heading-element><span>AlexNet</span>
<a href=#alexnet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf target=_blank rel="external nofollow noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks</a> <a href=https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：<a href=http://www.cs.toronto.edu/~kriz/ target=_blank rel="external nofollow noopener noreferrer">Alex Krizhevsky</a>, <a href=http://www.cs.toronto.edu/~ilya/ target=_blank rel="external nofollow noopener noreferrer">Ilya Sutskever</a>, <a href=https://www.cs.toronto.edu/~hinton/ target=_blank rel="external nofollow noopener noreferrer">Hinton</a></p><p>发表时间：(NIPS 2012)</p><p><a href=https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html target=_blank rel="external nofollow noopener noreferrer">论文主页</a></p></blockquote><p>AlexNet是2012年ImageNet图像分类竞赛冠军，首次将深度学习和卷积神经网络用于大规模图像数据集分类，比之前的模型有巨大的性能飞跃，在ILSVRC-2012图像分类竞赛中获得了top-5误差15.3%的冠军成绩，远远优于第二名（top-5错误率为26.2%），在学术界和工业界引起巨大轰动，自此之后，计算机视觉开始广泛采用深度卷积神经网络，模型性能日新月异，并迁移泛化到目标检测、语义分割等其它计算机视觉任务。</p><p>AlexNet的作者之一Hinton因为在神经网络和计算机视觉的贡献，获得2019年图灵奖。</p><p>AlexNet采用了<strong>ReLU激活函数、双GPU模型并行、LRN局部响应归一化、重叠最大池化、数据增强、Dropout正则化</strong>等技巧。</p><p>AlexNet包含五个卷积层，池化层，Dropout层和三个全连接层，最终通过1000个输出神经元进行softmax分类。</p><h2 id=the-dataset class=heading-element><span>The Dataset</span>
<a href=#the-dataset class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>ImageNet</p></blockquote><p>网络对每张图片给出五个预测l类别结果概率从高到低</p><blockquote><p>Top1:概率最高的预测类别为正确标签
Top5:五个预测类别里包含正确标莶</p></blockquote><h2 id=the-architecture class=heading-element><span>The Architecture</span>
<a href=#the-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=relu激活函数 class=heading-element><span>ReLu激活函数</span>
<a href=#relu%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>在AlexNet中用的非线性非饱和函数是$f=max(0,x)$，即ReLU。实验结果表明，要将深度网络训练至training error rate达到25%的话，ReLU只需5个epochs的迭代，但tanh单元需要35个epochs的迭代，用ReLU比tanh快6倍。</p><h3 id=多gpu模型并行 class=heading-element><span>多GPU模型并行</span>
<a href=#%e5%a4%9agpu%e6%a8%a1%e5%9e%8b%e5%b9%b6%e8%a1%8c class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>为提高运行速度和提高网络运行规模，作者采用双GPU的设计模式。并且规定<strong>GPU只能在特定的层进行通信交流</strong>。其实就是每一个GPU负责一半的运算处理。作者的实验数据表示，two-GPU方案会比只用one-GPU跑半个上面大小网络的方案，在准确度上提高了1.7%的top-1和1.2%的top-5。值得注意的是，虽然one-GPU网络规模只有two-GPU的一半，但其实这两个网络其实并非等价的。</p><p>由反向传播原理，显存中不仅存储模型参数还需存储正向传播时每一层batch的中间结果。batch size越大，占显存越大。</p><p>双GPU(全参数)的训练时间比单GPU(半参数)更短；单GPU(半参数)模型中最后一个卷积层和全连接层数量和双GPU(全参数)模型相同，因此“半参数”并非真的只有一半的参数。</p><h3 id=lrn局部响应归一化 class=heading-element><span>LRN局部响应归一化</span>
<a href=#lrn%e5%b1%80%e9%83%a8%e5%93%8d%e5%ba%94%e5%bd%92%e4%b8%80%e5%8c%96 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3>$$
b_{x,y}^i=a_{x,y}^i/(k+\alpha \sum_{j=max(0，i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^j)^2)^\beta
$$<p>$a_{x,y}^i$：代表在feature map中第$i$个通道上$(x,y)$位置上的值；k常数防止分母为0；</p><p>N：feature map 通道数（本层卷积核个数）n：表示相邻的几个卷积核。</p><p>$(k,\alpha,\beta,n)=(0,1,1,N)$代表普通沿所有通道归一化；</p><p>$(k,\alpha,\beta,n)=(2,10^{-4},0.75,5)$AlexNet所用参数他们的值是在验证集上实验得到的。</p><p>这种归一化操作实现了某种形式的横向抑制(兴奋的神经元对周围神经元有抑制作用)。</p><p>卷积核矩阵的排序是随机任意，并且在训练之前就已经决定好顺序。这种LRN形成了一种侧向抑制机制。</p><h3 id=overlapping-pooling class=heading-element><span>Overlapping Pooling</span>
<a href=#overlapping-pooling class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>池层是相同卷积核领域周围神经元的输出。池层被认为是由空间距离s个像素的池单元网格的组成。也可以理解成以大小为步长对前面卷积层的结果进行分块，对块大小为的卷积映射结果做总结。Pooling单元在总结提取特征的时候，其输入会受到相邻pooling单元的输入影响，也就是提取出来的结果可能是有重复的(对max pooling而言)。而且，实验表示使用带交叠的Pooling的效果比的传统要好，在top-1和top-5上分别提高了0.4%和0.3%，在训练阶段有避免过拟合的作用。</p><blockquote><p>后来的paper不采用这种方法</p></blockquote><h3 id=总体结构 class=heading-element><span>总体结构</span>
<a href=#%e6%80%bb%e4%bd%93%e7%bb%93%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p><a href=https://www.bilibili.com/video/BV1p7411T7Pc target=_blank rel="external nofollow noopener noreferrer">网络结构详解</a></p><p>conv1&ndash;>ReLu&ndash;>Pool&ndash;>LRN;conv2&ndash;>ReLu&ndash;>Pool&ndash;>LRN;conv3&ndash;>ReLu;conv4&ndash;>ReLu;conv5&ndash;>ReLu&ndash;>Pool</p></blockquote><center><img src="/images/Image Classification/AlexNet.assets/AlexNet_Architecture0.png" /></center><center><img src="/images/Image Classification/AlexNet.assets/AlexNet_Architecture.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">AlexNet网络结构</div></center><ol><li><p>输入数据为$227\times227\times3$图像，通过Conv1，卷积核为，$11\times11$，卷积核个数为96个，步长为4，Padding为2，输出特征图$55\times55\times96$。$N=(W-F+2P)/s+1=[227-11+(2\times2)]/4+1=55$</p><blockquote><p>输入数据为$224\times224\times3$图像，通过Conv1，卷积核为$11\times11$，卷积核个数为96个，步长为4，Padding为[1:2]（左上各补1个0，右下各补2个0）；输出特征图$55\times55\times96$。$N=(W-F+2P)/s+1=[224-11+(1+2)]/4+1=55$</p><blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>tuple</span><span class=p>:(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 1代表上下方各补一行零</span>
</span></span><span class=line><span class=cl><span class=c1># 2代表左右两侧各补两列零</span>
</span></span><span class=line><span class=cl><span class=n>nn</span><span class=o>.</span><span class=n>ZeroPad2d</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># 左侧补一列，右侧补两列((z))</span>
</span></span><span class=line><span class=cl><span class=c1># 上方补一行，下方补两行</span></span></span></code></pre></td></tr></table></div></div></blockquote></blockquote></li><li><p>Maxpooling1滑动窗口$3\times3\times96$,步长为2，padding：0 ；输出特征图$27\times27\times96$。$N=(W-F+2P)/s+1=[55-3+0)]/2+1=27$</p><blockquote><p>池化操作只改变特征图大小，不改变深度。</p></blockquote></li><li><p>通过Conv2，卷积核为$5\times5$，卷积核个数为256个，步长为1，Padding为2，输出特征图$27\times27\times256。$$N=(W-F+2P)/s+1=[27-5+(2\times2)]/1+1=27$</p></li><li><p>Maxpooling2滑动窗口$3\times3\times256$,步长为2，padding：0 ；输出特征图$13\times13\times256$。$N=(W-F+2P)/s+1=[27-3+0)]/2+1=13$</p></li><li><p>通过Conv3，卷积核为$3\times3$，卷积核个数为384个，步长为1，Padding为1，输出特征图$13\times13\times384$ 。
$N=(W-F+2P)/s+1=[13-3+(2\times1)]/1+1=13$</p></li><li><p>通过Conv4，卷积核为$3\times3$，卷积核个数为384个，步长为1，Padding为1，输出特征图$13\times13\times384$ 。
$N=(W-F+2P)/s+1=[13-3+(2\times1)]/1+1=13$</p></li><li><p>通过Conv5，卷积核为$3\times3$，卷积核个数为256个，步长为1，Padding为1，输出特征图$13\times13\times256$ 。
$N=(W-F+2P)/s+1=[13-3+(2\times1)]/1+1=13$</p></li><li><p>Maxpooling3滑动窗口$3\times3\times256$,步长为2，padding：0 ；输出特征图$6\times6\times256$。$N=(W-F+2P)/s+1=[13-3+0)]/2+1=6$</p></li><li><p>FC6：$6\times6\times256$进行扁平化处理成为$1\times 9216$,用一个维度为$9216\times4096$矩阵完成输入输出的全连接，输出$1\times 4096$</p></li><li><p>FC7：用一个维度为$4096\times4096$矩阵完成输入输出的全连接，输出$1\times 4096$</p></li><li><p>FC8：用一个维度为$4096\times1000$矩阵完成输入输出的全连接，输出$1\times 1000$</p></li></ol><h2 id=reducing-overfiting class=heading-element><span>Reducing Overfiting</span>
<a href=#reducing-overfiting class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=data-augmentation数据增强 class=heading-element><span>Data Augmentation数据增强</span>
<a href=#data-augmentation%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%ba class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>针对==位置==：</p><ul><li><p>训练阶段：随机地从$256\times256$的原始图像中截取$224\times224$大小的区域(水平翻转及镜像),相当于增加了$2*(256-224)^2=2048$倍的数据量。</p><blockquote><p>如果没有数据增强,仅靠原始的数据量,参数众多的CNN会陷入过拟合中,使用了数据增强后可以大大减轻过拟合,提升泛化能力。</p></blockquote></li><li><p>测试阶段：取图片的四个角加中间共5个位置,并进行左右翻转,一共获得10张图片,对他们进行预测并对10次结果求均值</p></li></ul><p>针对==颜色==：</p><ul><li>对图像的RGB数据进行PCA处理,并对主成分做一个标准差为0.1的高斯扰动,增加些噪声,（修改RGB通道像素值）这个 Trick可以让错误率再下降1%。</li></ul><h3 id=dropout-随机失活 class=heading-element><span>Dropout 随机失活</span>
<a href=#dropout-%e9%9a%8f%e6%9c%ba%e5%a4%b1%e6%b4%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>随机：dropout probability (eg: p=0.5)</p><p>失活：weight = 0</p></blockquote><ul><li>训练阶段：每一个batch随机失活一半的神经元（将神经元输出设置为0）阻断该神经元的前向-反向传播。</li><li>预测阶段：保留所有神经元，预测结果乘以0.5 。</li></ul><p>Dropout减少过拟合的理由</p><ul><li>模型集成 p=0.5意味着$2^n$个共享权重的潜在网络</li><li>记忆随即抹去</li><li>减少神经元之间的联合依赖性</li><li>有性繁殖 每个基因片段都要与来自另一个随即个体的基因片段协同工作</li><li>数据增强 总可以找到一个图片使神经网络中间层结果与Dropout后相同 相当于增加了这张图片到数据集里</li><li>稀疏性</li><li>等价于正则项</li></ul><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://neurohive.io/en/popular-networks/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/ target=_blank rel="external nofollow noopener noreferrer">AlexNet – ImageNet Classification with Deep Convolutional Neural Networks</a></p><p><a href=https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac target=_blank rel="external nofollow noopener noreferrer">LRN与BN的区别</a></p><p><a href=https://code.google.com/archive/p/cuda-convnet/ target=_blank rel="external nofollow noopener noreferrer">AlexNet的CUDA代码实现</a></p><p><a href=https://stats.stackexchange.com/questions/283261/why-does-overlapped-pooling-help-reduce-overfitting-in-conv-nets target=_blank rel="external nofollow noopener noreferrer">重叠池化为何能防止过拟合</a></p><p><a href=https://cs.nyu.edu/~ylclab/data/norb-v1.0/ target=_blank rel="external nofollow noopener noreferrer">NORB数据集</a></p><p><a href=http://www.vision.caltech.edu/Image_Datasets/Caltech101/ target=_blank rel="external nofollow noopener noreferrer">Caltech-101数据集</a></p><p><a href=https://authors.library.caltech.edu/7694/ target=_blank rel="external nofollow noopener noreferrer">Caltech-256数据集</a></p><p><a href=https://www.kaggle.com/jessicali9530/caltech256 target=_blank rel="external nofollow noopener noreferrer">Caltech-256数据集（Kaggle)</a></p><p><a href=http://labelme.csail.mit.edu/ target=_blank rel="external nofollow noopener noreferrer">标注工具LabelMe（主页)</a></p><p><a href=https://github.com/wkentaro/labelme target=_blank rel="external nofollow noopener noreferrer">标注工具LabelMe（Github)</a></p><h2 id=zfnet class=heading-element><span>ZFNet</span>
<a href=#zfnet class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/1311.2901 target=_blank rel="external nofollow noopener noreferrer">Visualizing and Understanding Convolutional Networks</a>
作者：Matthew D Zeiler，Rob Fergus
发表时间：(CVPR 2013)</p></blockquote><p>纽约大学ZFNet，2013年ImageNet图像分类竞赛冠军模型。提出了一系列可视化卷积神经网络中间层特征的方法，并巧妙设置了对照消融实验，从各个角度分析卷积神经网络各层提取的特征及对变换的敏感性。</p><p>使用反卷积deconvnet，将中间层feature map投射重构回原始输入像素空间，便于可视化每个feature map捕获的特征。</p><p>改进AlexNet模型，减小卷积核尺寸，减小步长，增加卷积核，提出ZFNet。</p><blockquote><p>训练过程中不同层特征演化可视化。</p><p>图像平移、缩放、旋转敏感性分析。</p><p>图像局部遮挡敏感性分析（遮挡同一张狗脸图像的不同部位，分析结果变化）。</p><p>图像局部遮挡相关性敏感性分析（遮挡不同狗脸的同一部位，分析相关性）。</p><p>ZFNet在ImageNet2012图像分类竞赛结果。</p><p>模型迁移学习泛化到其它数据集的性能分析：Caltech-101、Caltech-256、PASCAL VOC2012。</p><p>去除全连接层和卷积层后模型性能分析。</p><p>模型各层特征对分类任务的有效性分析。</p></blockquote><center><img src="/images/Image Classification/AlexNet.assets/ZFNet_.png" /></center><center><img src="/images/Image Classification/AlexNet.assets/ZFNet_AlexNet.png"></center><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.youtube.com/watch?v=ghEmQSxT6tw" target=_blank rel="external nofollow noopener noreferrer">原作者讲解视频</a>（视频中有几页ppt播放顺序错误）</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-02 18:22:27">更新于 2023-06-02&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/ data-title=AlexNet data-hashtags="Deep Learning,图像分类模型"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/ data-title=AlexNet><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/ class=post-tag title="标签 - 图像分类模型">图像分类模型</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/image-classification/automl/ class=post-nav-item rel=prev title=AutoML><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>AutoML</a><a href=/posts/deeplearning/light-weight/squeezenet/ class=post-nav-item rel=next title=SqueezeNet>SqueezeNet<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>