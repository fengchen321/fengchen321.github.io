<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Deep Learning Paper - fengchen</title><meta name=author content="fengchen"><meta name=description content="Deep Learning Paper"><meta name=keywords content='Deep Learning'><meta itemprop=name content="Deep Learning Paper"><meta itemprop=description content="Deep Learning Paper"><meta itemprop=datePublished content="2023-06-01T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-01T18:22:27+08:00"><meta itemprop=wordCount content="1447"><meta itemprop=keywords content="Deep Learning"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/paper/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="Deep Learning Paper"><meta property="og:description" content="Deep Learning Paper"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-01T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-01T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Learning Paper"><meta name=twitter:description content="Deep Learning Paper"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/paper/ title="Deep Learning Paper - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/about/ title><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/image-classification/vggnet/ title=VGGNet><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/paper/index.md title="Deep Learning Paper - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Deep Learning Paper","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/paper\/"},"genre":"posts","keywords":"Deep Learning","wordcount":1447,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/paper\/","datePublished":"2023-06-01T18:22:27+08:00","dateModified":"2023-06-01T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"Deep Learning Paper"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Deep Learning Paper</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-01 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-01>2023-06-01</time></span>&nbsp;<span title="1447 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 1500 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 3 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#image-classification>Image Classification</a></li><li><a href=#object-detection>Object Detection</a><ul><li><a href=#dense-prediction-one-stage>Dense Prediction (one-stage)</a><ul><li><a href=#anchor-based>anchor based</a></li><li><a href=#anchor-free>anchor free</a></li></ul></li><li><a href=#sparse-prediction-two-stage>Sparse Prediction (two-stage)</a><ul><li><a href=#anchor-based-1>anchor based</a></li><li><a href=#anchor-free-1>anchor free</a></li></ul></li><li><a href=#neck>Neck</a><ul><li><a href=#additional--blocks>Additional blocks</a></li><li><a href=#path-aggregation-blocks>Path-aggregation blocks</a></li></ul></li></ul></li><li><a href=#image-segmentation>Image Segmentation</a></li><li><a href=#轻量化cnn>轻量化CNN</a></li><li><a href=#gan>GAN</a></li></ul></nav></div></div><div class=content id=content><h2 id=paper class=heading-element><span>Paper</span>
<a href=#paper class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h2 id=image-classification class=heading-element><span>Image Classification</span>
<a href=#image-classification class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><font face="Noto Serif SC" color=#ff0000><strong>ALexNet</strong></font>：<a href=http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf target=_blank rel="external nofollow noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks</a> (NIPS 2012)</p><p><font face="Noto Serif SC" color=#8d1eff><strong>ZFNet</strong></font>：<a href=https://arxiv.org/abs/1311.2901 target=_blank rel="external nofollow noopener noreferrer">Visualizing and Understanding Convolutional Networks</a> (ECCV 2014)</p><p><font face="Noto Serif SC" color=#ff0000><strong>GoogLeNet</strong></font>：<a href=https://arxiv.org/abs/1409.4842 target=_blank rel="external nofollow noopener noreferrer">Going Deeper with Convolutions</a> (CVPR 2015)</p><blockquote><blockquote><p><a href=https://arxiv.org/abs/1312.4400 target=_blank rel="external nofollow noopener noreferrer">Network In Network</a> $1\times1$卷积</p><p><a href=https://arxiv.org/abs/1310.6343 target=_blank rel="external nofollow noopener noreferrer">Provable Bounds for Learning Some Deep Representations</a> 用稀疏、分散的网络取代以前庞大密集臃肿的网络</p></blockquote><p><font face="Noto Serif SC" color=#ff0000><strong>InceptionV2</strong></font>：<a href=https://arxiv.org/abs/1502.03167 target=_blank rel="external nofollow noopener noreferrer">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> (ICML 2015)</p><p><font face="Noto Serif SC" color=#ff0000><strong>InceptionV3</strong></font>：<a href=https://arxiv.org/abs/1512.00567 target=_blank rel="external nofollow noopener noreferrer">Rethinking the Inception Architecture for Computer Vision</a> (CVPR 2016)</p><p><font face="Noto Serif SC" color=#ff0000><strong>InceptionV4</strong></font>：<a href=https://arxiv.org/abs/1602.07261 target=_blank rel="external nofollow noopener noreferrer">Inception-ResNet and the Impact of Residual Connections on Learning</a> (AAAI 2017)</p><p><font face="Noto Serif SC" color=#ff0000><strong>Xception</strong></font>：<a href=https://arxiv.org/abs/1610.02357v3 target=_blank rel="external nofollow noopener noreferrer">Xception: Deep Learning with Depthwise Separable Convolutions</a> (CVPR 2017)</p></blockquote><p><font face="Noto Serif SC" color=#ff0000><strong>VGGNet</strong></font>：<a href=https://arxiv.org/abs/1409.1556 target=_blank rel="external nofollow noopener noreferrer">Very Deep Convolutional Networks for Large-Scale Visual Recognition</a> (ICLR 2015)</p><p><font face="Noto Serif SC" color=#ff0000><strong>ResNet</strong></font>：<a href=https://arxiv.org/abs/1512.03385 target=_blank rel="external nofollow noopener noreferrer">Deep Residual Learning for Image Recognition</a>(CVPR 2016)</p><blockquote><p>ResNeXt：<a href=https://arxiv.org/abs/1611.05431 target=_blank rel="external nofollow noopener noreferrer">ggregated Residual Transformations for Deep Neural Networks-2017</a></p><p>DenseNet：<a href=https://arxiv.org/abs/1608.06993 target=_blank rel="external nofollow noopener noreferrer">Densely Connected Convolutional Networks</a></p></blockquote><h2 id=object-detection class=heading-element><span>Object Detection</span>
<a href=#object-detection class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=dense-prediction-one-stage class=heading-element><span>Dense Prediction (one-stage)</span>
<a href=#dense-prediction-one-stage class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=anchor-based class=heading-element><span>anchor based</span>
<a href=#anchor-based class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p><strong>SSD</strong>：<a href=https://arxiv.org/abs/1512.02325 target=_blank rel="external nofollow noopener noreferrer">SSD: Single Shot MultiBox Detector</a> (ECCV 2016)</p><p><font face="Noto Serif SC" color=#ff0000><strong>YOLO</strong></font>：<a href=https://arxiv.org/abs/1506.02640 target=_blank rel="external nofollow noopener noreferrer">You Only Look Once:Unified, Real-Time Object Detection</a> (CVPR 2016)</p><blockquote><p><font face="Noto Serif SC" color=#ff0000><strong>YOLOV2</strong></font>：<a href=https://arxiv.org/abs/1612.08242 target=_blank rel="external nofollow noopener noreferrer">YOLO9000: Better, Faster, Stronger</a> (CVPR 2017)</p><p><font face="Noto Serif SC" color=#ff0000><strong>YOLOV3</strong></font>：<a href=https://arxiv.org/abs/1804.02767 target=_blank rel="external nofollow noopener noreferrer">YOLOv3: An Incremental Improvement </a>(CVPR 2018)</p><p><strong>YOLOV4</strong>：<a href=https://arxiv.org/abs/2004.10934 target=_blank rel="external nofollow noopener noreferrer">YOLOv4: Optimal Speed and Accuracy of Object Detection</a> (CVPR 2020)</p><blockquote><p><strong>Scaled-YOLOv4</strong>：<a href=https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html target=_blank rel="external nofollow noopener noreferrer">Scaled-YOLOv4: Scaling Cross Stage Partial Network</a> (CVPR 2021)</p><p><a href=https://arxiv.org/abs/1608.01471 target=_blank rel="external nofollow noopener noreferrer">IOU_Loss</a>(2016)-><a href=https://arxiv.org/abs/1902.09630 target=_blank rel="external nofollow noopener noreferrer">GIOU_Loss</a>(2019)-><a href=https://arxiv.org/abs/1911.08287 target=_blank rel="external nofollow noopener noreferrer">DIOU_Loss</a>(2020)-><a href=https://arxiv.org/abs/1911.08287 target=_blank rel="external nofollow noopener noreferrer">CIOU_Loss</a>(2020)</p></blockquote><p><strong>YOLOX</strong>：<a href=https://arxiv.org/abs/2107.08430 target=_blank rel="external nofollow noopener noreferrer">YOLOX: Exceeding YOLO Series in 2021</a></p><p><strong>YOLOV5</strong>：</p><blockquote><p><a href=https://arxiv.org/abs/2110.13675 target=_blank rel="external nofollow noopener noreferrer">Alpha-IoU:A Family of Power Intersection over Union Losses for Bounding Box Regression</a> （NIPS 2021）</p></blockquote></blockquote><p><strong>RetinaNet</strong>：<a href=https://arxiv.org/abs/1708.02002 target=_blank rel="external nofollow noopener noreferrer">Focal Loss for Dense Object Detection</a> (ICCV 2017)</p><h4 id=anchor-free class=heading-element><span>anchor free</span>
<a href=#anchor-free class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p><strong>CornerNet</strong>：CornerNet: Detecting Objects as Paired Keypoints](<a href=https://arxiv.org/abs/1808.01244 target=_blank rel="external nofollow noopener noreferrer">https://arxiv.org/abs/1808.01244</a>) (ECCV 2018)</p><blockquote><p><a href=https://arxiv.org/abs/1904.08900 target=_blank rel="external nofollow noopener noreferrer">CornerNet-Lite: Efficient Keypoint Based Object Detection</a> (BMVC 2020)</p></blockquote><p><strong>CenterNet</strong>：<a href=https://arxiv.org/abs/1904.08189 target=_blank rel="external nofollow noopener noreferrer">CenterNet: Keypoint Triplets for Object Detection</a> （ICCV 2019)</p><p><strong>MatrixNe</strong>t：<a href=https://arxiv.org/abs/1908.04646 target=_blank rel="external nofollow noopener noreferrer">Matrix Nets: A New Deep Architecture for Object Detection</a>（ICCV 2019)</p><p><strong>FCOS</strong>：<a href=https://arxiv.org/abs/1904.01355 target=_blank rel="external nofollow noopener noreferrer">FCOS: Fully Convolutional One-Stage Object Detection</a> (ICCV 2019)</p><h3 id=sparse-prediction-two-stage class=heading-element><span>Sparse Prediction (two-stage)</span>
<a href=#sparse-prediction-two-stage class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=anchor-based-1 class=heading-element><span>anchor based</span>
<a href=#anchor-based-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p><strong>R-CNN</strong>：[<a href=https://arxiv.org/pdf/1311.2524.pdf target=_blank rel="external nofollow noopener noreferrer">Rich feature hierarchies for accurate object detection and semantic segmentation</a> (CVPR 2014)</p><blockquote><blockquote><p><a href=https://link.springer.com/article/10.1007/s11263-013-0620-5 target=_blank rel="external nofollow noopener noreferrer">Selective Search for Object Recognition</a>（IJCV 2012）</p><p>[<strong>Path-aggregation blocks-FPN</strong>](####Path-aggregation blocks)</p></blockquote><p>[<strong>Additional blocks-SPP</strong>](####Additional blocks)</p><p><strong>Fast R-CNN</strong>：<a href=https://arxiv.org/abs/1504.08083 target=_blank rel="external nofollow noopener noreferrer">Fast R-CNN</a> (ICCV 2015)</p><p><strong>Faster R-CNN</strong>：<a href=https://arxiv.org/abs/1506.01497 target=_blank rel="external nofollow noopener noreferrer">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a> (NIPS 2015)</p><p><strong>R-FCN</strong>：<a href=https://arxiv.org/abs/1605.06409 target=_blank rel="external nofollow noopener noreferrer">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a> (NIPS 2016)</p><p><strong>Mask R-CNN</strong>：<a href=https://arxiv.org/abs/1703.06870 target=_blank rel="external nofollow noopener noreferrer">Mask R-CNN</a> (ICCV 2017)</p><p><strong>Libra R-CNN</strong>: <a href=https://arxiv.org/abs/1904.02701 target=_blank rel="external nofollow noopener noreferrer">Libra R-CNN: Towards Balanced Learning for Object Detection</a> (CVPR 2019)</p><p><strong>Sparse R-CNN</strong>：<a href=https://arxiv.org/abs/2011.12450 target=_blank rel="external nofollow noopener noreferrer">Sparse R-CNN: End-to-End Object Detection with Learnable Proposals</a> (CVPR 2021)</p></blockquote><h4 id=anchor-free-1 class=heading-element><span>anchor free</span>
<a href=#anchor-free-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p><strong>RepPoints</strong>：<a href=https://arxiv.org/abs/1904.11490 target=_blank rel="external nofollow noopener noreferrer">RepPoints: Point Set Representation for Object Detection</a> (ICCV 2019)</p><h3 id=neck class=heading-element><span>Neck</span>
<a href=#neck class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h4 id=additional--blocks class=heading-element><span>Additional blocks</span>
<a href=#additional--blocks class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p><strong>SPP</strong>：<a href=https://arxiv.org/abs/1406.4729 target=_blank rel="external nofollow noopener noreferrer">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a> (TPAMI 2015)</p><p><strong>ASPP</strong>：<a href=https://arxiv.org/abs/1606.00915 target=_blank rel="external nofollow noopener noreferrer">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a> (TPAMI 2017)</p><p><strong>RFB</strong>：<a href=https://arxiv.org/abs/1711.07767 target=_blank rel="external nofollow noopener noreferrer">Receptive Field Block Net for Accurate and Fast Object Detection</a> (ECCV 2018)</p><p><strong>SAM</strong>：<a href=https://arxiv.org/abs/1807.06521 target=_blank rel="external nofollow noopener noreferrer">CBAM: Convolutional Block Attention Module</a> (ECCV 2018)</p></blockquote><h4 id=path-aggregation-blocks class=heading-element><span>Path-aggregation blocks</span>
<a href=#path-aggregation-blocks class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><blockquote><p><strong>FPN</strong>：<a href=https://arxiv.org/abs/1612.03144 target=_blank rel="external nofollow noopener noreferrer">Feature Pyramid Networks for Object Detection</a> (CVPR 2017)</p><p><strong>PAN</strong>：<a href=https://arxiv.org/abs/1803.01534 target=_blank rel="external nofollow noopener noreferrer">Path Aggregation Network for Instance Segmentation</a> (CVPR 2018)</p><p><strong>NAS-FPN</strong>：<a href=https://arxiv.org/abs/1904.07392 target=_blank rel="external nofollow noopener noreferrer">NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection</a> (CVPR 2019)</p><p><strong>BiFPN</strong>：<a href=https://arxiv.org/abs/1911.09070 target=_blank rel="external nofollow noopener noreferrer">EfficientDet: Scalable and Efficient Object Detection</a> (CVPR 2020)</p><p><strong>ASFF</strong>：<a href=https://arxiv.org/abs/1911.09516 target=_blank rel="external nofollow noopener noreferrer">Learning Spatial Fusion for Single-Shot Object Detection</a> (2019)</p><p><strong>SFAM</strong>： <a href=https://arxiv.org/abs/1811.04533 target=_blank rel="external nofollow noopener noreferrer">M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</a> (AAAI 2019)</p></blockquote><h2 id=image-segmentation class=heading-element><span>Image Segmentation</span>
<a href=#image-segmentation class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h2 id=轻量化cnn class=heading-element><span>轻量化CNN</span>
<a href=#%e8%bd%bb%e9%87%8f%e5%8c%96cnn class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>SqueezeNet</strong>：<a href=https://arxiv.org/abs/1602.07360 target=_blank rel="external nofollow noopener noreferrer">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</a> (2016)</p><blockquote><p><a href=https://arxiv.org/pdf/1803.10615.pdf target=_blank rel="external nofollow noopener noreferrer">SqueezeNext: Hardware-Aware Neural Network Design</a> (2018)</p></blockquote><p><strong>MobileNet</strong>：<a href=https://arxiv.org/abs/1704.04861 target=_blank rel="external nofollow noopener noreferrer">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> (2017)</p><blockquote><p><strong>MobileNetV2</strong>：<a href=https://arxiv.org/abs/1801.04381 target=_blank rel="external nofollow noopener noreferrer">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> (2018)</p><p><strong>MobileNetV3</strong>：<a href=https://arxiv.org/abs/1905.02244 target=_blank rel="external nofollow noopener noreferrer">Searching for MobileNetV3</a> (2019)</p><blockquote><p><a href=https://arxiv.org/abs/1807.11626 target=_blank rel="external nofollow noopener noreferrer">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a> (CVPR 2019)</p></blockquote></blockquote><p><strong>ShuffleNet</strong>：<a href=https://arxiv.org/abs/1707.01083 target=_blank rel="external nofollow noopener noreferrer">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a> (2017)</p><blockquote><p><strong>ShuffleNetV2</strong>：<a href=https://arxiv.org/abs/1807.11164 target=_blank rel="external nofollow noopener noreferrer">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a> (2018)</p></blockquote><p><strong>PeleeNet</strong>：<a href=https://arxiv.org/abs/1804.06882 target=_blank rel="external nofollow noopener noreferrer">Pelee: A Real-Time Object Detection System on Mobile Devices</a> (2018)</p><p><strong>Shift-A</strong>：<a href=https://arxiv.org/abs/1711.08141 target=_blank rel="external nofollow noopener noreferrer">Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions</a> (2018)</p><p><strong>GhostNet</strong>： <a href=https://arxiv.org/abs/1911.11907 target=_blank rel="external nofollow noopener noreferrer">GhostNet: More Features from Cheap Operations</a> (2020)</p><h2 id=gan class=heading-element><span>GAN</span>
<a href=#gan class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>GAN：<a href=https://arxiv.org/abs/1406.2661 target=_blank rel="external nofollow noopener noreferrer">Generative Adversarial Networks</a></p><h2 id=如何读论文 class=heading-element><span>如何读论文</span>
<a href=#%e5%a6%82%e4%bd%95%e8%af%bb%e8%ae%ba%e6%96%87 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p><a href="https://www.bilibili.com/video/BV1H44y1t75x?spm_id_from=333.999.0.0" target=_blank rel="external nofollow noopener noreferrer">李沐</a></p></blockquote><p>第一遍：关注标题和摘要；结论。实验部分和方法的图表；看看适不适合。海选</p><p>第二遍：全过一遍，图表、流程图具体到每个部分；相关文献圈出来。精选</p><p>第三遍：知道每句话，每段话在说什么，换位思考。脑补过程。重点研读</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-01 18:22:27">更新于 2023-06-01&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/paper/ data-title="Deep Learning Paper" data-hashtags="Deep Learning"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/paper/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/paper/ data-title="Deep Learning Paper"><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/image-classification/vggnet/ class=post-nav-item rel=next title=VGGNet>VGGNet<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024 - 2025</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>