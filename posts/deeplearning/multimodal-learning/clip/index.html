<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>CLIP - fengchen</title><meta name=author content><meta name=description content="CLIP"><meta name=keywords content='Deep Learning,多模态学习'><meta itemprop=name content="CLIP"><meta itemprop=description content="CLIP"><meta itemprop=datePublished content="2023-06-08T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-08T18:22:27+08:00"><meta itemprop=wordCount content="6134"><meta itemprop=keywords content="Deep Learning,多模态学习"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="CLIP"><meta property="og:description" content="CLIP"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-08T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-08T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="多模态学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="CLIP"><meta name=twitter:description content="CLIP"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/ title="CLIP - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/ title=VILT><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/albef/ title=ALBEF><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"CLIP","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/multimodal-learning\/clip\/"},"genre":"posts","keywords":"Deep Learning, 多模态学习","wordcount":6134,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/multimodal-learning\/clip\/","datePublished":"2023-06-08T18:22:27+08:00","dateModified":"2023-06-08T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"作者"},"description":"CLIP"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>CLIP</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
Anonymous</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-08 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-08>2023-06-08</time></span>&nbsp;<span title="6134 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 6200 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 13 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#related-work>Related work</a></li><li><a href=#methods>Methods</a></li><li><a href=#推荐阅读>推荐阅读</a></li></ul><ul><li><a href=#methods-1>Methods</a></li></ul><ul><li><a href=#methods-2>Methods</a></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-1>拓展阅读</a></li></ul><ul><li><a href=#拓展阅读-2>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=clip class=heading-element><span>CLIP</span>
<a href=#clip class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2103.00020 target=_blank rel="external nofollow noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision</a> <a href=https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever</p><p>发表时间：(ICML 2021)</p><p><a href=https://github.com/openai/CLIP target=_blank rel="external nofollow noopener noreferrer">offical code</a> 代码只是可以用来做推理并没有开源</p><p>图片和文本之间的对比学习</p><p>CLIP：Con-trastive Language-Image Pre-training</p></blockquote><p>利用自然语言的这种监督信号去学习一个迁移性能好的视觉网络</p><blockquote><p>优点</p><ul><li><p>不需要再去标注数据</p></li><li><p>图片-文本对这种多模态特征适合zero-shot迁移学习</p><blockquote><p>单模态的对比学习：MoCo；单模态的掩码学习：MAE；只能学到视觉特征，很难zero-shot迁移学习</p></blockquote></li></ul><p>局限性：</p><ul><li>ResNet50打平手但是离SOTA还很远，扩大模型和数据集能提高预计资源$\times 1000$，代价太大</li><li>在有些数据集上的zero-shot效果也不好：细分类数据集，抽象概念</li><li>推理时，目标数据集out-of-distribution,CLIP泛化照样差</li><li>不能做成生成式模型（GPT）（对比学习的目标函数和生成式的目标函数结合）</li><li>数据利用不高效（数据大）减少数据用量：数据增强；自监督；伪标签</li><li>下游任务数据集测试调参带入偏见：创建一个用来测试各种各样的zero-shot的迁移能力的数据集</li><li>网上爬的未清洗，可能带有社会偏见</li><li>提供一些训练样本反而效果变差（Few Shot效果不好）</li></ul></blockquote><p>不使用ImageNet的训练集的情况下直接Zero-shot 做推理就获得和之前监督训练好ResNet50同样的效果</p><p>使用超大规模 web Image Text 数据集</p><h2 id=related-work class=heading-element><span>Related work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://arxiv.org/abs/1612.09161 target=_blank rel="external nofollow noopener noreferrer">Learning visual n-grams from web data</a>：和CLIP相似，没有transformer和大规模数据集，效果很差</p><p><a href=https://arxiv.org/abs/2006.06666 target=_blank rel="external nofollow noopener noreferrer">VirTex (CVPR 2021)</a> 自回归的预测方式去做模型的预训练</p><p><a href=https://arxiv.org/abs/2008.01392 target=_blank rel="external nofollow noopener noreferrer">ICMLM (ECCV 2020)</a> 用这种完形填空的方式去做预训练</p><p><a href=https://arxiv.org/abs/2010.00747 target=_blank rel="external nofollow noopener noreferrer">ConVIRT (MLHC 2022)</a> 和CLIP类似，只在医疗图像上做了实验</p><h2 id=methods class=heading-element><span>Methods</span>
<a href=#methods class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/MultiModal learning/CLIP.assets/CLIP.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">CLIP 模型总览图</div></center><p>(1) 模型的输入是一个图片和文字的配对；图片通过了一个图片编码器 <strong>Image Encoder</strong> 得到了一些特征 $I_1,I_2,&mldr;,I_N$；句子通过一个文本编码器 <strong>Text Encoder</strong> 得到一些文本的特征 $T_1,T_2,&mldr;,T_N$。</p><blockquote><p>正样本：对角线上文本和图片配对的元素 $N$</p><p>负样本：其他 $N^2-N$</p></blockquote><p>(2) prompt template 提示模板</p><p>把Image Net 里的1,000个类变成1000个句子；句子通过预训练好的文本编码器得到1,000个文本的特征</p><blockquote><p>如何变成句子？用物体类别去替代图里的 object 变成 <strong>A photo of a (object).</strong></p><p>为什么要prompt template ？只用一个单词去做 prompt 经常出现歧异性（不同语境下意思不同）。由于模型预训练时，图片和句子成对使用，推理时直接用类别单词得到的文本特征(distribution gap)，效果就会稍有下降。</p><p>prompt engineering ：为每个任务定制提示文本可以显着提高零样本性能（缩小解空间）</p><p>prompt ensemble：80个模板结果综合</p></blockquote><p>(3) Zero-shot</p><p>推理时，输入一张图片通过预训练好的图片编码器得到图片的特征 $I_1$，$I_1 $ 和所有的文本特征做cosine similarity (相似性比较)，得到文本特征最相似的句子$I_1T_3$。</p><p>摆脱了categorical label 的限制</p><blockquote><p>不论是训练还是推理，都不需要提前定好一个标签列表。</p><p>任意一张照片可以通过给模型输入不同的文本句子从而知道这张图片里到底有没有感兴趣的物体</p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/CLIP_implementation.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">Numpy-like pseudocode for the core of an implementation of CLIP.</div></center><ul><li><p>两个输入：一个是图片的输入；一个是文本的输入。通过编码器输出图像特征和文本特征</p></li><li><p>线性投射层 W 学习一下如何从单模态转变为多模态，再做一次 L2 归一化</p><blockquote><p>投射层 线性还是非线性 没太大关系（数据集大，多模态）</p><p>数据增强只使用随机裁剪</p></blockquote></li><li><p>计算consine similarity</p></li><li><p>交叉熵目标函数 一个是 Image loss；一个是 text loss； 把两个 loss 加起来取平均</p></li></ul><h2 id=推荐阅读 class=heading-element><span>推荐阅读</span>
<a href=#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://openai.com/blog/clip/ target=_blank rel="external nofollow noopener noreferrer">官方博客</a></p><p><a href="https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.880.my_history.page.click&amp;vd_source=d28e92983881d85b633a5acf8e46efaa" target=_blank rel="external nofollow noopener noreferrer">CLIP 论文精读</a></p><p><a href=https://github.com/orpatashnik/StyleCLIP target=_blank rel="external nofollow noopener noreferrer">style CLIP</a> (ICCV 2021)： CLIP + style GAN 想通过文字上的改变从而去引导图像生成</p><p><a href=https://arxiv.org/abs/2106.14843 target=_blank rel="external nofollow noopener noreferrer">CLIP draw</a> 不需要任何训练，CLIPDraw在矢量笔画上操作，而不是在像素图像上操作，使绘画偏向于更简单的人类可识别的形状。</p><p><a href=https://github.com/johanmodin/clifs target=_blank rel="external nofollow noopener noreferrer">视频检索</a>：CLIP模型把检索对象(一句话表示)变成文本特征，把视频里的每一帧都变成视觉上的特征，然后一帧一帧的去跟文本特征做对比然后挑出相似性最高的那一帧展现出来</p><p><a href=https://lilianweng.github.io/posts/2021-09-25-train-large/ target=_blank rel="external nofollow noopener noreferrer">How to Train Really Large Models on Many GPUs?</a></p><h2 id=lseg class=heading-element><span>LSeg</span>
<a href=#lseg class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2201.03546 target=_blank rel="external nofollow noopener noreferrer">Language-driven Semantic Segmentation</a> <a href=https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl</p><p>发表时间：(ICLR 2022)</p><p><a href=https://github.com/isl-org/lang-seg target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>CLIP做图像分割：像素级别的分类</p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/Lseg.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">Lseg overview</div></center><ul><li><p>模型的输入是一个图片和文字的配对；图片通过了一个图片编码器 <strong>Image Encoder</strong> 得到了一些密集特征$C\times \tilde H \times \tilde W$矩阵 ，各元素为$I_{11},I_{12},&mldr;,I_{\tilde H \tilde W}$；文本通过一个文本编码器 <strong>Text Encoder</strong> 得到一些文本的特征$N\times C$矩阵，各元素为 $T_1,T_2,&mldr;,T_N$。</p><blockquote><p>图片编码器：dpt的结构-vision Transformer + decoder</p><blockquote><p>decoder目的：把bottleneck feature慢慢upscale；特征维度$C$一般是512或者768</p><p>使用原始的ViT或者dit的预训练参数</p></blockquote><p>文本编码器：CLIP里的文本编码器</p></blockquote></li><li><p>图片特征和文本特征做点积得到$N\times \tilde H \times \tilde W$矩阵，各元素为$F_{11},F_{12},&mldr;,F_{\tilde H \tilde W}$；拿输出特征和最后的ground truth去做cross entropy loss</p></li><li><p>spetial regularization block 文本和视觉特征交互，加两个这种block效果最好</p></li></ul><p><strong>局限性</strong></p><blockquote><p>目标函数不是对比学习；也不是无监督学习的框架；依赖于手工标注的segametation mask</p></blockquote><h2 id=groupvit class=heading-element><span>GroupViT</span>
<a href=#groupvit class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2202.11094 target=_blank rel="external nofollow noopener noreferrer">GroupViT: Semantic Segmentation Emerges from Text Supervision</a> <a href=https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang</p><p>发表时间：(CVPR 2022)</p><p><a href=https://github.com/NVlabs/GroupViT target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>CLIP做图像分割：监督信号来自于文本</p></blockquote><p>为什么叫group?</p><blockquote><p>视觉做无监督分割经常就是用一类方法叫做grouping（一种自下而上的方式）</p><blockquote><p>类似于有一些聚类中心点，从这个点开始发散，把附近周围相似的点逐渐扩充成一个group，那这个group相当是一个segametation mask。</p></blockquote></blockquote><h2 id=methods-1 class=heading-element><span>Methods</span>
<a href=#methods-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>ViT + grouping block + 可学习的group tokens</strong></p><center><img src="/images/MultiModal learning/CLIP.assets/GroupViT_Pipeline.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">The Architecture and Training Pipeline of GroupViT</div></center><ul><li><p>图像编码器：<strong>Vision Transformer</strong>(12层Transformer layers)</p><blockquote><p>两部分输入</p><blockquote><ol><li><p>原始图像的patch embedding</p><blockquote><p>大小$224\times224$的图片，patch size选择$16\times16$；就有一个$14\times14=196$序列长度的一个序列
然后经过这个linear projection就得到了patch embedding，维度为$196\times384$(ViT small)</p></blockquote></li><li><p>可学习的<strong>group tokens</strong></p><blockquote><p>开始设的是$64\times384$：64个聚类中心；384为了保持维度和patch embedding进行拼接</p></blockquote></li></ol></blockquote></blockquote></li><li><p><strong>grouping block</strong> (6层Transfor Layer之后加了一个grouping block)</p><blockquote><p>类似于自注意力的方式先算一个相似度矩阵，用这个相似的矩阵去帮助原来的这个image token
做聚类中心的分配，从而完成了输入$(196+64)\times384$降到这个$64\times 384$</p><blockquote><p>合并成为更大的group，做一次聚类的分配</p><p>降低序列长度，模型的计算复杂度，训练时间相应的都减少了</p></blockquote><p>第9层Transformer Layer 之后又加了一次grouping block：$64\times 384$降到这个$8\times 384$</p></blockquote></li><li><p>文本编码器得到文本特在$z^T$；图像编码器输出$8\times 384$进行average pooling得到$1\times384$，在通过MLP得到图片特征$z^I$</p></li><li><p>后续和CLIP一样对比学习</p></li><li><p>zero shot推理</p><blockquote><p>给定一个图片首先经过group ViT 得到最后8个group Embedding</p><p>再把有可能这些标签通过这个文本编码器得到一系列的这个文本特征</p><p>计算这些图像的Group Embedding和这些文本的特征之间的相似度</p><p>局限性：最多只能检测到8类；没有很好的利用dense prediction的特性；CLIP 这种训练方式
没有办法学到这些背景类（语义太模糊）</p></blockquote></li></ul><h2 id=vild class=heading-element><span>VILD</span>
<a href=#vild class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2104.13921 target=_blank rel="external nofollow noopener noreferrer">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</a> <a href=https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui</p><p>发表时间：(ICLR 2022)</p><p><a href=https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild target=_blank rel="external nofollow noopener noreferrer">offical code</a></p></blockquote><h2 id=methods-2 class=heading-element><span>Methods</span>
<a href=#methods-2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/MultiModal learning/CLIP.assets/VILD_1.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">VILD</div></center><blockquote><p>(a) <strong>baseline</strong> 就是一个maskRCNN(定位+分类)</p><blockquote><p>两阶段的分类器：第一阶段RPN抽取 $N$个region Proposal ；第二阶段就是根据着$N$个 Proposal 通过detection head得到一些region embedding ，最后再通过一些分类头判断类别</p></blockquote><p>(b) <strong>ViLD-text</strong>：和a类似得到N个region embedding之后，和base category基类+背景类的text embedding去做点乘计算相似度，得到一个81维的向量，将这个向量做softmax，再去和ground truth做交叉熵，得到的结果即为ViLD的损失函数</p><blockquote><p>text embedding：经过CLIP的文本编码器得到的，不参与训练的。（类别通过prompt生成一个句子进入编码器输出）</p><p>在b中需要改动的参数有两处，一是图像处理模块，也即抽取图像特征的backbone需要训练；二是背景类的embedding。</p><blockquote><p>背景类：不在基础类里的所有别的类别</p></blockquote></blockquote><p>(c) <strong>ViLD-image</strong>：利用CLIP的图像编码器对自己的视觉backbone进行知识蒸馏，让backbone输出的region embedding 尽可能地靠近CLIP的image embedding</p><blockquote><p>一些抽好的Proposal 做一些resize的操作</p><p>c 中输入的是M个pre-computed proposal，和a、b不同（加快训练）</p><blockquote><p>预先把所有图像的proposal算出来，然后一次性扔到CLIP图像编码器中先抽好存到硬盘中，这样在训练的时候就直接把这些存好的embedding取出来就可以了。</p><p>损失函数：常用的L1 Loss。需要注意的是，作者在把一个proposal送入CLIP的图像编码器时，是将其1x和1.5x分别送入进行编码，最后再把这两个embedding加起来。</p></blockquote><p>损失函数：常用的L1 Loss</p></blockquote><p>(d) ViLD：ViLD-image和ViLD-text两个的合体</p><blockquote><p>左侧将N+M个proposal同时输入进目标检测框架，然后分开，n个Embedding去算cross entropy loss
然后m 个 precomputer embedding去算这个蒸馏的L_1 loss。</p><p>右侧为teacher网络，只有训练的时候用，测试的时候用不到。</p></blockquote></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/VILD_ensemble.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">VILD_ensemble</div></center><center><img src="/images/MultiModal learning/CLIP.assets/VILD.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">VILD</div></center><ul><li><p>训练阶段</p><blockquote><p>图片先通过一个RPN得到一些region Proposal 然后通过RoI Align 和一些Conv层得到一些region embedding $R_1,R_2$；</p><p>绿色的基础类先通过一个prompt然后通过文本编码器得到绿色的文本编码和$R_1,R_2$做点乘，再和ground truth做cross entropy loss；
把已经抽取好的region Proposal 通过CLIP model得到一些CLIP的iamge embedding $ I_1, I_2$；使用蒸馏计算$L_1$ loss 希望$R_1, R_2$呢尽可能的跟$I_1, I_2 $去接近</p></blockquote></li><li><p>推理阶段</p><blockquote><p>不论是基础类还是新类都通过prompt再通过这个文本编码器得到所有的这些text embedding；然后让Mask RCNN抽取的region embedding去和text embedding做相似度计算，计算结果最大的那个，就是模型输出的检测到的类型。</p></blockquote></li></ul><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://zhuanlan.zhihu.com/p/565836721 target=_blank rel="external nofollow noopener noreferrer">利用图像文本的知识蒸馏来进行开放词表目标检测</a></p><h2 id=glip class=heading-element><span>GLIP</span>
<a href=#glip class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2112.03857 target=_blank rel="external nofollow noopener noreferrer">Grounded Language-Image Pre-training</a> <a href=https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong</p><p>发表时间：(CVPR 2022)</p><p><a href=https://github.com/microsoft/GLIP target=_blank rel="external nofollow noopener noreferrer">offical code</a></p></blockquote><p>object detection 目标检测：给定图片，把bounding box 给找出来</p><p>phrase grounding：给定图片和文本，根据文本把物体找出来</p><blockquote><p>定位 loss 部分差不多</p><p>分类 loss 部分</p><blockquote><p>detection：它的标签是一个或者两个单词是one-hot的这种标签</p><blockquote><p>给定图片通过backbone得到$N\times D$的region embedding (n个bounding box，每个bounding box Embedding的维度是d)；通过$C\times D$矩阵的分类头；MNS把bounding box筛选一下，然后再去跟ground Truth 去算cross entropy loss</p></blockquote><p>Vision grounding：标签是一个句子。</p><blockquote><p>给定图片通过backbone得到了一些region feature；一个句子prompt通过文本编码器得到文本的embedding，进行相似度计算。（类似ViLD-text）</p></blockquote></blockquote><p>目标检测和Vision grounding 结合</p><blockquote><p>判断一下什么时候算是一个positive match；什么时候算是一个negative match</p></blockquote></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/GLIP.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">GLIP</div></center><ul><li>图片通过图像编码器得到一些region embedding；文本通过文本编码器得到一些text embedding</li><li>用Cross Attention啊把这个文本和图像的特征交互一下</li></ul><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h2 id=clipasso class=heading-element><span>CLIPasso</span>
<a href=#clipasso class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2202.05822 target=_blank rel="external nofollow noopener noreferrer">CLIPasso: Semantically-Aware Object Sketching</a> <a href=https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：<a href=https://yaelvi116.wixsite.com/mysite target=_blank rel="external nofollow noopener noreferrer">Yael Vinker</a>, <a href=https://pajouheshgar.github.io/ target=_blank rel="external nofollow noopener noreferrer">Ehsan Pajouheshgar</a>, <a href=https://jessica-bo.github.io/ target=_blank rel="external nofollow noopener noreferrer">Jessica Y. Bo</a>, <a href=https://roman-bachmann.github.io/ target=_blank rel="external nofollow noopener noreferrer">Roman Bachmann</a>, <a href=https://www.cs.tau.ac.il/~amberman/ target=_blank rel="external nofollow noopener noreferrer">Amit Haim Bermano</a>, <a href=https://danielcohenor.com/ target=_blank rel="external nofollow noopener noreferrer">Daniel Cohen-Or</a>, <a href=https://vilab.epfl.ch/zamir/ target=_blank rel="external nofollow noopener noreferrer">Amir Zamir</a>, <a href=https://faculty.idc.ac.il/arik/site/index.asp target=_blank rel="external nofollow noopener noreferrer">Ariel Shamir</a></p><p>发表时间：(SIGGRAPH 2022) (Best Paper Award)</p><p><a href=https://clipasso.github.io/clipasso/ target=_blank rel="external nofollow noopener noreferrer">主页介绍 + code</a></p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/CLIPasso.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">CLIPasso</div></center><p>贝兹曲线</p><blockquote><p>通过一系列的2维的点控制的一个曲线</p></blockquote><p>基于saliency的一个初始化的方式</p><blockquote><p>把图片扔给已经训练好的Vision Transformer，然后把最后一层的多头自注意力取加权平均做成了一个siliancy map；在这个siliancy map上去看哪些区域更显著，这些显著的区域上去采点。</p></blockquote><p>定义了这几个曲线，也就这里说的$S_1$到$S_N$就是n个笔画，通过光栅化器Rasterizer得到简笔画。</p><p><strong>Loss 选择</strong></p><blockquote><p>$L_s$ 基于语义性的目标函数：简笔画生成的特征和原始图像生成的特征尽可能的接近</p><p>$L_g$ 基于geometric的目标函数：resnet的 2 3 4各阶段特征拿出来算loss，而不是用最后的那个2048维的特征。</p><blockquote><p>保证最后生成的简笔画无论是在几何形状上，位置上跟原有的图像尽可能的一致；而且在语义信息上也能尽可能的保持一致</p></blockquote></blockquote><p><strong>局限性</strong></p><blockquote><p>图像有背景，效果就会大打折扣。必须是一个物体然后处在一个纯白色的背景上</p><blockquote><p>先把一张带背景的图片，把这个物体抠出来，背景是一个白色幕布的图片，扔给CLIPasso去生成简笔画(两阶段)</p></blockquote><p>初始化的笔画都是同时生成的而不是序列生成的（怎样才能一笔一画）</p><p>通过控制笔画数去控制图片的抽象程度 （手动&ndash;优化参数）</p></blockquote><h2 id=拓展阅读-2 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-2 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://distill.pub/2021/multimodal-neurons/ target=_blank rel="external nofollow noopener noreferrer">Multimodal Neurons in Artificial Neural Networks</a> 可视化分析 CLIP</p><h2 id=clip4clip class=heading-element><span>CLIP4Clip</span>
<a href=#clip4clip class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2104.08860 target=_blank rel="external nofollow noopener noreferrer">CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval</a> <a href=https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, Tianrui Li</p><p>发表时间：( 2021)</p><p><a href=https://github.com/ArrowLuo/CLIP4Clip target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>视频领域</p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/CLIP4Clip.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">CLIP4Clip</div></center><p>对含有时序的视频特征处理，假设10帧</p><blockquote><ul><li><p>10个图像的特征直接取平均 （没有考虑到这个时序的特性）</p><blockquote><p>一个是一个人逐渐的在坐下，另外一个是一个人逐渐的站起来；只是取一个这个平均的话，这两个动作无法区分</p></blockquote></li><li><p>late fusion： 最原始的lstm把这10个特征扔给一个lstm，把最后的输出拿出来 （时序建模：Transformer替代）</p></li><li><p>early fusion：把文本和这个图像帧的特征一起在学习</p></li></ul></blockquote><h2 id=actionclip class=heading-element><span>ActionCLIP</span>
<a href=#actionclip class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2109.08472 target=_blank rel="external nofollow noopener noreferrer">ActionCLIP: A New Paradigm for Video Action Recognition</a> <a href=https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Mengmeng Wang, Jiazheng Xing, Yong Liu</p><p>发表时间：( 2021)</p><p><a href=https://github.com/sallymmx/actionclip target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>动作识别</p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/ActionCLIP.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">ActionCLIP</div></center><p>视频的输入通过一个视频编码器得到一些特征，把标签当做文本给一个文本编码器得到一些文本的特征；去计算文本和图像之间的相似度；相似度矩阵和提前定义好的ground truth算一个loss。</p><p>把cross entropy loss换成KL divergence</p><center><img src="/images/MultiModal learning/CLIP.assets/Overview of ActionCLIP.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">Overview of ActionCLIP</div></center><h2 id=pointclip class=heading-element><span>PointCLIP</span>
<a href=#pointclip class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2112.02413 target=_blank rel="external nofollow noopener noreferrer">PointCLIP: Point Cloud Understanding by CLIP</a> <a href=https://www.semanticscholar.org/paper/PointCLIP%3A-Point-Cloud-Understanding-by-CLIP-Zhang-Guo/f3ce9ba3fcec362b70263a7ed63d9404975496a0 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li</p><p>发表时间：(CVPR 2022)</p><p><a href=https://github.com/zrrskywalker/pointclip target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>3D点云</p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/PointCLIP.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">PointCLIP</div></center><p>把3D点云投射到2D平面上变成了2D的深度图，扔给clip的视觉编码器得到视觉表征。</p><p>文本端通过prompt变成了句子point cloud depth Map of a 『CLASS』</p><h2 id=depthclip class=heading-element><span>DepthCLIP</span>
<a href=#depthclip class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2207.01077 target=_blank rel="external nofollow noopener noreferrer">Can Language Understand Depth?</a> <a href=https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Renrui Zhang, Ziyao Zeng, Ziyu Guo, Yafeng Li</p><p>发表时间：(CVPR 2022)</p><p><a href=https://github.com/adonis-galaxy/depthclip target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>用文本跨界估计深度</p></blockquote><center><img src="/images/MultiModal learning/CLIP.assets/DepthCLIP.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">DepthCLIP</div></center><p>把深度估计看成了一个分类问题，强制性的把深度距离分成了7大类</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-08 18:22:27">更新于 2023-06-08&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/ data-title=CLIP data-hashtags="Deep Learning,多模态学习"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/ data-title=CLIP><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0/ class=post-tag title="标签 - 多模态学习">多模态学习</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/multimodal-learning/vilt/ class=post-nav-item rel=prev title=VILT><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>VILT</a><a href=/posts/deeplearning/multimodal-learning/albef/ class=post-nav-item rel=next title=ALBEF>ALBEF<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/></a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>