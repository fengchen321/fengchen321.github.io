<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>VILT - fengchen</title><meta name=author content="fengchen"><meta name=description content="VILT"><meta name=keywords content='Deep Learning,多模态学习'><meta itemprop=name content="VILT"><meta itemprop=description content="VILT"><meta itemprop=datePublished content="2023-06-08T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-08T18:22:27+08:00"><meta itemprop=wordCount content="1847"><meta itemprop=keywords content="Deep Learning,多模态学习"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="VILT"><meta property="og:description" content="VILT"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-08T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-08T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="多模态学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="VILT"><meta name=twitter:description content="VILT"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/ title="VILT - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/ title=BYOL><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/ title=CLIP><link rel=alternate type=text/markdown href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/index.md title="VILT - fengchen"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"VILT","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/multimodal-learning\/vilt\/"},"genre":"posts","keywords":"Deep Learning, 多模态学习","wordcount":1847,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/multimodal-learning\/vilt\/","datePublished":"2023-06-08T18:22:27+08:00","dateModified":"2023-06-08T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fengchen"},"description":"VILT"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>VILT</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
fengchen</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-08 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-08>2023-06-08</time></span>&nbsp;<span title="1847 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 1900 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 4 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#relate-work>Relate work</a></li><li><a href=#methods>Methods</a><ul><li><a href=#whole-word-masking>Whole word masking</a></li><li><a href=#image-augmentation>Image Augmentation</a></li></ul></li><li><a href=#experiments>Experiments</a></li><li><a href=#future-work>Future work</a></li><li><a href=#推荐阅读>推荐阅读</a></li></ul></nav></div></div><div class=content id=content><h2 id=vilt class=heading-element><span>VILT</span>
<a href=#vilt class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2102.03334 target=_blank rel="external nofollow noopener noreferrer">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a> <a href=https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1 target=_blank rel="external nofollow noopener noreferrer"><img loading=lazy src="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount" alt=citation srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;query=citationCount&amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;size=large 2x" data-title=citation class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></a></p><p>作者：Wonjae Kim, Bokyung Son, Ildoo Kim</p><p>发表时间：(ICML 2021)</p><p><a href=https://github.com/dandelin/vilt target=_blank rel="external nofollow noopener noreferrer">offical code</a></p><p>第一个摆脱了目标检测的视觉文本模型</p></blockquote><h2 id=abstract class=heading-element><span>Abstract</span>
<a href=#abstract class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>Vision and Language Pre-training(VLP) 当前的工作主要集中在图像特征抽取上，一般来讲，图像特征抽取的越好，下游任务中的表现就越好。</p><blockquote><ul><li><p>效率太低，速度太慢，抽取图像特征花费大量时间，比多模态融合都多。</p></li><li><p>用一个预训练好的模型去抽取特征，表达能力受限。</p><blockquote><p>目标检测数据集不够大，规模不够大。如果模型不是端到端学习，只是从预训练模型抽取特征，大概率来说不是最优解。</p></blockquote></li></ul></blockquote><h2 id=relate-work class=heading-element><span>Relate work</span>
<a href=#relate-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/MultiModal learning/VILT.assets/vilt_model.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">Four categories of vision-and-language models</div></center><blockquote><p>第一类，代表作VSE，文本端较为简单，图像比较贵，融合端也是简单的神经网络。</p><p>第二类，代表作CLIP，图像和文本的计算力度等价，融合的时候将两种特征直接点乘，非常轻量。</p><p>第三类，代表作ViLBERT、UNITER占据了大部分工作，文本端非常轻量。图像端使用目标检测的系统，非常贵。融合端也使用了Transformer，相当于两个大模型。</p><p>第四类，代表作ViLT，基于ViT对图像使用patch embedding，模态融合部分做得比较大。</p></blockquote><table border=0><tr><td align=center><img src="/images/MultiModal learning/VILT.assets/vilt_compare.png"></td><td align=center><img src="/images/MultiModal learning/VILT.assets/vilt_runtime.png"></td></tr><tr><td colspan=2 align=center style="color:#000;border-bottrm:1px solid #d9d9d9;padding:2px">Visual comparison of conventional VLP architectures and ViLT</td></tr></table><p><strong>模态融合</strong>方法</p><blockquote><p>signal-stream approach：将两种特征拼接起来，用一个模型处理两个输入。</p><p>dual-stream approach：两个模型分别对两种模态信息进行处理，充分挖掘每种模态包含的信息，然后再融合。</p></blockquote><p>两种模型表现差不多，但是dual-stream approach参数多一些，VILT 采用signal-stream approaches。</p><p><strong>文本编码端</strong>都是用预训练的BERT里的tokenizer</p><p><strong>视觉编码端</strong></p><blockquote><ul><li><p><strong>Region Feature</strong>：经过一个Backbone抽取特征，然后经过RPN网络生成proposal，经过非极大值抑制 NMS 筛选边界框，最后经过ROI head得到图像序列。把一张图像变成了<strong>离散</strong>的bound-box，每个边界框内都含有明确的类别语义信息。(目标检测)</p></li><li><p><strong>Grid Feature</strong>：仅基于Backbone</p></li><li><p><strong>Patch Projection</strong>：基于 ViT 直接将图像打成patch，，得到一个<strong>有语义信息的离散的序列</strong>。</p></li></ul></blockquote><p>VILT 把模态的特征抽取做到了极小化，主要计算量在模态融合部分，提高模型推理速度。移除了Region feature</p><h2 id=methods class=heading-element><span>Methods</span>
<a href=#methods class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src="/images/MultiModal learning/VILT.assets/vilt.png"><br><div style="color:#000;border-bottrm:1px solid #d9d9d9;display:inline-block;padding:2px">Four categories of vision-and-language models</div></center><p>文本端有$L$个长为$H$ 的序列，$L$为一个句子中单词数量，$H$为序列长度。</p><p>图像端图像被打成 $N $个patch，每个patch也对应长为$H$的序列。</p><p>Modal-type embedding 模态信息（文本为0，图像为1），Token position embedding 文本位置信息，Patch position embedding 图像位置信息。</p><p>Modal-type embedding + position embedding + word embedding 不是拼接，是加在一起</p><p>Transformer Encoder的输入为$（N+L+2)\times H$的矩阵。* 代表 [CLS] token，$（N+L+2)\times H$中2代表两种模态的[CLS]。</p><p>使用了两个loss，分别是<strong>Image Text Matching</strong>和<strong>Mask Laguage Modeling</strong>。加个小loss ：<strong>Word Patch Alignment</strong></p><blockquote><p>Image Text Matching：文字，图片配对 （文本与图像是否匹配）</p><p>Mask Laguage Modeling：NLP的完形填空</p><p>Word Patch Alignment ：利用最优运输理论计算相似度（分布距离）</p></blockquote><p>Transformer 的输出为$1\times H$的矩阵，经过$H\times H$的pooler(权重矩阵)得到仍是$1\times H$的矩阵，最后经过一个FC层进行二分类任务。</p><h3 id=whole-word-masking class=heading-element><span>Whole word masking</span>
<a href=#whole-word-masking class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>例如giraffe长颈鹿这个单词，由三个词根组成，分别是gi，raf，fe，如果mask 的时候mask “raf”这个token。由于开头为gi结尾为fe的单词不多，模型就记住了中间一定是raf，就相当于模型学到了shortcut，这样泛化性就不好。</p><p>直接mask “giraffe” 整个单词。这样就需要借助图像信息，因此就加强了图像文本的联系。</p><h3 id=image-augmentation class=heading-element><span>Image Augmentation</span>
<a href=#image-augmentation class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>为什么前边的研究没有使用数据增强？</p><blockquote><p>多模态学习要考虑图像文本匹配的问题，数据增强可能会改变图像语义
使用预训练模型，无法进行数据增强</p></blockquote></blockquote><p>不适用color inversion和cutout避免与文本信息不匹配。</p><h2 id=experiments class=heading-element><span>Experiments</span>
<a href=#experiments class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>预训练所用的数据集叫4million(4个数据集图片加起来这个数)</p><blockquote><p>MSCOCO：113K图片 567K 长标题
VG： 108K图片 5.41M 短标题
GCC：3.01M图片对
SBU：867K图片对</p></blockquote><h2 id=future-work class=heading-element><span>Future work</span>
<a href=#future-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>scalability</strong>：transformer都是越大越好，数据集越大越好（做的更大）</p><blockquote><p><a href=https://arxiv.org/abs/2107.07651 target=_blank rel="external nofollow noopener noreferrer">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a> 用14million</p></blockquote><p><strong>Masked Modeling for Visual Inputs</strong>：图像重建 (NLP里进行Mask重建，图像肯定也有用)</p><blockquote></blockquote><p><strong>Augmentation Strategies</strong>：数据增强</p><blockquote><p><a href=https://arxiv.org/abs/2206.08358 target=_blank rel="external nofollow noopener noreferrer">MixGen: A New Multi-Modal Data Augmentation</a></p></blockquote><h2 id=推荐阅读 class=heading-element><span>推荐阅读</span>
<a href=#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href="https://www.bilibili.com/video/BV14r4y1j74y/?spm_id_from=333.788&amp;vd_source=d28e92983881d85b633a5acf8e46efaa" target=_blank rel="external nofollow noopener noreferrer">ViLT 论文精读</a></p><p>后续改进，时间提升，更少时间训练</p><blockquote><p><a href=https://arxiv.org/abs/2107.07651 target=_blank rel="external nofollow noopener noreferrer">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a> 单机8卡训练2-3天
<a href=https://arxiv.org/abs/2201.12086 target=_blank rel="external nofollow noopener noreferrer">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>
<a href=https://arxiv.org/abs/2206.02967 target=_blank rel="external nofollow noopener noreferrer">Masked Unsupervised Self-training for Zero-shot Image Classification</a></p></blockquote></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-08 18:22:27">更新于 2023-06-08&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/ data-title=VILT data-hashtags="Deep Learning,多模态学习"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/ data-title=VILT><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0/ class=post-tag title="标签 - 多模态学习">多模态学习</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/contrastive-learning/byol/ class=post-nav-item rel=prev title=BYOL><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>BYOL</a><a href=/posts/deeplearning/multimodal-learning/clip/ class=post-nav-item rel=next title=CLIP>CLIP<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024 - 2025</span><span class=author itemprop=copyrightHolder>
<a href=/>fengchen</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>