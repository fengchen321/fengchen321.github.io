<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>VIT - fengchen</title><meta name=author content><meta name=description content="VIT"><meta name=keywords content='Deep Learning,Transformer,CV'><meta itemprop=name content="VIT"><meta itemprop=description content="VIT"><meta itemprop=datePublished content="2023-06-09T18:22:27+08:00"><meta itemprop=dateModified content="2023-06-09T18:22:27+08:00"><meta itemprop=wordCount content="4393"><meta itemprop=keywords content="Deep Learning,Transformer,CV"><meta property="og:url" content="http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/"><meta property="og:site_name" content="fengchen"><meta property="og:title" content="VIT"><meta property="og:description" content="VIT"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-09T18:22:27+08:00"><meta property="article:modified_time" content="2023-06-09T18:22:27+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="CV"><meta name=twitter:card content="summary"><meta name=twitter:title content="VIT"><meta name=twitter:description content="VIT"><meta name=application-name content="FixIt"><meta name=apple-mobile-web-app-title content="FixIt"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/ title="VIT - fengchen"><link rel=prev type=text/html href=http://fengchen321.github.io/posts/deeplearning/multimodal-learning/albef/ title=ALBEF><link rel=next type=text/html href=http://fengchen321.github.io/posts/deeplearning/transformer_cv/mae/ title=MAE><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"VIT","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/transformer_cv\/vit\/"},"genre":"posts","keywords":"Deep Learning, Transformer, CV","wordcount":4393,"url":"http:\/\/fengchen321.github.io\/posts\/deeplearning\/transformer_cv\/vit\/","datePublished":"2023-06-09T18:22:27+08:00","dateModified":"2023-06-09T18:22:27+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"作者"},"description":"VIT"}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=fengchen><span class=header-title-text>fengchen</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/about/ title=关于本站>关于本站</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>VIT</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
Anonymous</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/deep-learning/ class=post-category title="分类 - Deep Learning"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> Deep Learning</a></span></div><div class=post-meta-line><span title="发布于 2023-06-09 18:22:27"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2023-06-09>2023-06-09</time></span>&nbsp;<span title="4393 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 4400 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 9 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#introdouction>Introdouction</a></li><li><a href=#related-work>Related work</a></li><li><a href=#vit-model>VIT Model</a><ul><li><a href=#vit-architecture>Vit Architecture</a></li><li><a href=#hybrid-architecture>Hybrid Architecture</a></li></ul></li><li><a href=#拓展阅读>拓展阅读</a></li></ul><ul><li><a href=#overall-architecture>Overall Architecture</a><ul><li><a href=#path-merging>Path Merging</a></li><li><a href=#swin-transformer-block>Swin Transformer Block</a></li></ul></li><li><a href=#shifted-window-attention><strong>Shifted Window Attention</strong></a></li><li><a href=#relative-position-bias>Relative position bias</a></li><li><a href=#拓展阅读-1>拓展阅读</a></li></ul></nav></div></div><div class=content id=content><p>[toc]</p><h2 id=vision-transformer-vit class=heading-element><span>Vision Transformer (VIT)</span>
<a href=#vision-transformer-vit class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2010.11929 target=_blank rel="external nofollow noopener noreferrer">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>
作者：Alexey Dosovitskiy; Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,Xiaohua Zhai
发表时间：(ICLR 2021)</p><p>==Transformer杀入CV界==</p><p><a href=https://github.com/google-research/vision_transformer target=_blank rel="external nofollow noopener noreferrer">官方代码</a></p></blockquote><p>每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches &ndash;> an image is worth 16 * 16 words</p><blockquote><p>一个 224 * 224 图片 变成一个 196 个的 16 * 16 图片块（words in NLP）。</p></blockquote><center><img src=/images/Transformer_CV/VIT.assets/vit.gif><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">vit</div></center><h2 id=introdouction class=heading-element><span>Introdouction</span>
<a href=#introdouction class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>Transformer 应用在 CV 的难点</p><blockquote><p>计算像素的 self-attention，序列长，维度爆炸</p><blockquote><p>Trnasformer 的计算复杂度是序列长度 n 的 平方即 $O（n^2）$</p><p>224 分辨率的图片，有 50176 个像素点，（2d 图片 flatten）序列长度是 BERT(512) 的近 100 倍。</p></blockquote></blockquote><p>CV 如何用attention( 降低序列长度)</p><blockquote><p>CNN 结构 + self-attention</p><blockquote><p><a href=https://arxiv.org/abs/1711.07971 target=_blank rel="external nofollow noopener noreferrer">Non-Local Network</a>, 网络中的特征图当作输入 Transformer</p><p><a href=https://arxiv.org/abs/2005.12872 target=_blank rel="external nofollow noopener noreferrer">DETR</a></p></blockquote><p>attention 替代卷积</p><blockquote><p><a href=https://arxiv.org/abs/1906.05909 target=_blank rel="external nofollow noopener noreferrer">stand-alone attention</a> 孤立自注意力</p><blockquote><p>用 local window 局部小窗口控制 transformer 的计算复杂度</p></blockquote><p><a href=https://arxiv.org/abs/2003.07853 target=_blank rel="external nofollow noopener noreferrer">axial attention</a> 轴注意力</p><blockquote><p>2d变成2个1d 顺序操作，降低计算复杂度</p></blockquote></blockquote></blockquote><p>Transformer 比 CNN 少 inductive biases 归纳偏置(先验知识 or 提前的假设)</p><blockquote><p>CNN 的 inductive biases 是 locality 和 平移等变性 translation equaivariance（平移不变性 spatial
invariance）</p><blockquote><p>locality: CNN用滑动窗口在图片上做卷积。假设是图片相邻的区域有相似的特征。</p><p>translation equaivariance：f (g(x)) = g( f(x) );f 和 g 函数的顺序不影响结果。</p><blockquote><p>CNN 的卷积核 像一个 template 模板，同样的物体无论移动到哪里，遇到了相同的卷积核，它的输出一致</p></blockquote></blockquote><p>Transformer 没有这些先验信息，只能从图片数据里，自己学习对视觉世界的感知。</p><p>ViT 用了图片 2d 结构 的 inductive bias 地方：resolution adjustment 尺寸改变 和 patch extraction 抽patches</p></blockquote><h2 id=related-work class=heading-element><span>Related work</span>
<a href=#related-work class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=https://arxiv.org/abs/1911.03584 target=_blank rel="external nofollow noopener noreferrer">ICLR 2020</a> 从输入图片里抽取 2 * 2 patches。</p><blockquote><p>CIFAR-10 32 * 32 图片，2 * 2足够，16 * 16 会过大。 抽好 patch 之后，在 patches 上 做 self-attention。</p></blockquote><h2 id=vit-model class=heading-element><span>VIT Model</span>
<a href=#vit-model class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>ViT-B/16为例</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>Patch_size</th><th style=text-align:center>Layers</th><th style=text-align:center>Hidden_size D</th><th style=text-align:center>MLP_size 4D</th><th style=text-align:center>Heads</th><th style=text-align:center>Params</th></tr></thead><tbody><tr><td style=text-align:center>$Vit_{base}$</td><td style=text-align:center>$16\times16$</td><td style=text-align:center>12</td><td style=text-align:center>768</td><td style=text-align:center>3071</td><td style=text-align:center>12</td><td style=text-align:center>86M</td></tr><tr><td style=text-align:center>$Vit_{large}$</td><td style=text-align:center>$16\times16$</td><td style=text-align:center>24</td><td style=text-align:center>1024</td><td style=text-align:center>4096</td><td style=text-align:center>16</td><td style=text-align:center>307M</td></tr><tr><td style=text-align:center>$Vit_{huge}$</td><td style=text-align:center>$14\times14$</td><td style=text-align:center>32</td><td style=text-align:center>1280</td><td style=text-align:center>5120</td><td style=text-align:center>16</td><td style=text-align:center>632M</td></tr></tbody></table></blockquote><p>划分 patches，flatten patches 的线性投影 + patches 的位置信息，得到输入transformer 的 tokens</p><blockquote><p>将图像$224×224×3$<strong>划分</strong>成大小$16×16$的patch(小方块)，每个patch块可以看做是一个token(词向量)，共有$(224/16)^2=196$个token，每个token的长度为$16×16×3=768$。<code>[16, 16, 3] -> [768]</code></p><blockquote><p>在代码实现中，直接使用一个卷积核大小为16x16，步距为16，卷积核个数为768的<strong>卷积</strong>来实现。通过卷积<code>[224, 224, 3] -> [14, 14, 768]</code>，然后把H以及W两个维度展平即可<code>[14, 14, 768] -> [196, 768]</code></p><p>如果改变图像的输入大小，ViT不会改变patchs的大小，那么patchs的数量会发生变化，那么之前学习的pos_embed就维度对不上了，ViT采用的方案是通过插值来解决这个问题</p></blockquote><p><strong>[class]</strong> token：可训练的参数，长度为768的向量，<code>Concat([1, 768], [196, 768]) -> [197, 768]</code></p><blockquote><p>所有的 tokens 在做两两的交互信息。因此，<strong>[CLS]</strong> 也会和所有的图片patches 的token 交互，从而 <strong>[CLS]</strong> 从图片 patches + position 的 embedding 学到有用信息，最后用**[CLS]** 做分类判断。</p><p>CV 通常的全局特征：feature map (14 * 14) &ndash;> GAP globally average-pooling 全局平均池化 &ndash;> a flatten vector 全局的图片特征向量 &ndash;> MLP 分类</p><p>同样的，Transformer 的输出元素 + GAP也可以用做全局信息 + 分类，效果差异不大；</p><p>ViT 对齐 标准的 transformer，选用 NLP 里常用的 CLS 和 1d position embedding</p></blockquote><p><strong>Position Embedding</strong>：采用的是一个可训练的参数（<strong>1D Pos. Emb.</strong>） <code>Add([197, 768], [197, 768]) -> [197, 768]</code></p><blockquote><p><code>选择不同位置编码</code>几乎没有差异，原因是Transformer是直接在patch上操作而不是基于像素级，较少数量的 patches 之间的相对位置信息，容易学到，因此，空间信息编码方式差异没那么重要</p><center><img src=/images/Transformer_CV/VIT.assets/vit_Position_Embedding.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">vit_Position_Embedding</div></center></blockquote></blockquote><h3 id=vit-architecture class=heading-element><span>Vit Architecture</span>
<a href=#vit-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><center><img src=/images/Transformer_CV/VIT.assets/vit_Architecture.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">vit_Architecture</div></center><p><strong>MLP Head</strong></p><blockquote><p>整个Encoder的输出为<code>[197, 768]</code>我们仅仅保留最前面的CLS token作为全连接的输入<code>[1, 768]</code>，然后接上全连接层及<code>分类数n_class</code>，使用交叉熵损失函数计算损失，反向传播更新网络的权重和参数。</p><blockquote><p>在训练ImageNet21K时是由<code>Linear</code>+<code>tanh激活函数</code>+<code>Linear</code>组成。但是迁移到ImageNet1K上或者你自己的数据上时，只用一个<code>Linear</code>即可</p></blockquote></blockquote><h3 id=hybrid-architecture class=heading-element><span>Hybrid Architecture</span>
<a href=#hybrid-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>前 CNN + 后 Transformer</p><p>R50不同之处</p><blockquote><p>R50的卷积层采用的StdConv2d不是传统的Conv2d</p><p>所有的BatchNorm层替换成GroupNorm层。</p><p>在原Resnet50网络中，stage堆叠次数 [3,4,6,3]。R50中，把stage4中的3个Block移至stage3中，变成 [3,4,9]。</p></blockquote><p>通过R50 Backbone进行特征提取后，得到的特征矩阵shape是<code>[14, 14, 1024]</code>，接着再输入Patch Embedding层，注意Patch Embedding中卷积层Conv2d的kernel_size和stride都变成了1，只是用来调整channel。</p><center><img src=/images/Transformer_CV/VIT.assets/vit_Hybrid_Architecture.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Hybrid_Architecture</div></center><h2 id=拓展阅读 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>代码</p><blockquote><p><a href=https://github.com/huggingface/transformers target=_blank rel="external nofollow noopener noreferrer">State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</a></p><p><a href=https://github.com/rwightman/pytorch-image-models target=_blank rel="external nofollow noopener noreferrer">timm版vit</a></p><p><a href=https://github.com/lucidrains/vit-pytorch target=_blank rel="external nofollow noopener noreferrer">lucidrains/vit-pytorch含动图</a></p></blockquote><p><a href=https://www.bilibili.com/video/BV15P4y137jb target=_blank rel="external nofollow noopener noreferrer">B站：ViT论文逐段精读【论文精读】</a></p><p><a href=https://www.bilibili.com/video/BV1Jh411Y7WQ target=_blank rel="external nofollow noopener noreferrer">B站：Vision Transformer详解</a></p><p><a href=https://mp.weixin.qq.com/s/YiejUQBaKX3eyVgwaV03Dg target=_blank rel="external nofollow noopener noreferrer">视觉Transformer(ViT)模型创新思路总结</a></p><h2 id=swin-transformer class=heading-element><span>Swin Transformer</span>
<a href=#swin-transformer class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>文章标题：<a href=https://arxiv.org/abs/2103.14030 target=_blank rel="external nofollow noopener noreferrer">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a>
作者：Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo</p><p>发表时间：(ICCV 2021)</p><p><a href=https://github.com/microsoft/Swin-Transformer target=_blank rel="external nofollow noopener noreferrer">官方代码</a></p><p>==多层次的Vision Transformer==</p></blockquote><p>Swin Transformer是一个用了移动窗口的层级式的Vision Transformer</p><blockquote><p>Swin：来自于 Shifted Windows</p><blockquote><p>更大的效率</p><p>通过 shifting 移动的这个操作，能够让相邻的两个窗口之间有了交互，所以上下层之间就可以有 cross-window connection，从而变相的达到了一种全局建模的能力</p></blockquote><p>层级式 Hierarchical</p></blockquote><p>减少序列长度方式</p><blockquote><p>用后续的特征图来当做Transformer的输入，</p><p>把图片打成 patch</p><p>把图片画成一个一个的小窗口，然后在窗口里面去做自注意力</p></blockquote><p>借鉴了很多卷积神经网络的设计理念以及先验知识</p><blockquote><p>采取了在小窗口之内算自注意力</p><blockquote><p>利用了卷积神经网络里的 Locality 的 Inductive bias，就是利用了局部性的先验知识，同一个物体的不同部位或者语义相近的不同物体还是大概率会出现在相连的地方</p></blockquote><p>提出来了一个类似于池化的操作叫做 patch merging</p><blockquote><p>把相邻的小 patch 合成一个大 patch，这样合并出来的这一个大patch其实就能看到之前四个小patch看到的内容，它的感受野就增大了，同时也能抓住多尺寸的特征</p></blockquote></blockquote><h2 id=overall-architecture class=heading-element><span>Overall Architecture</span>
<a href=#overall-architecture class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><center><img src=/images/Transformer_CV/VIT.assets/swin_vit.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin_vit</div></center><center><img src=/images/Transformer_CV/VIT.assets/swin_all.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin</div></center><ul><li><code>win. sz. 7x7</code>表示使用的窗口（Windows）的大小</li><li><code>dim</code>表示feature map的channel深度（或者说token的向量长度）</li><li><code>head</code>表示多头注意力模块中head的个数</li></ul><center><img src=/images/Transformer_CV/VIT.assets/swin_Architecture.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin_Architecture</div></center><p><strong>patch partition</strong>：将图像$224×224×3$<strong>划分</strong>成大小$4×4$的patch(小方块)，得到$56\times56\times48$大小。</p><blockquote><p>（$224/4=56,\ 4\times4\times3=48$） <code>[224, 224, 3] -> [56, 56, 48]</code></p></blockquote><p><strong>Linear Embedding</strong>：要把向量的维度变成一个预先设置好的值<strong>C</strong>，对于 Swin tiny来说$C=96$</p><blockquote><p><code>[56, 56, 48] ->[56, 56, 96]</code></p><p>Patch Partition 和 Linear Embedding 就相当于是 ViT 里的Patch Projection 操作，而在代码里也是用一次卷积操作就完成</p></blockquote><p>$56\times56=3136$太长，引入了<strong>基于窗口的自注意力计算</strong>，每个窗口按照默认来说，都只有$M^2=7^2=49$个 patch，所以说序列长度就只有49就相当小了</p><blockquote><p>共有$ (56/7)\times(56/7)=8\times8=64 $个窗口。</p></blockquote><p>Stage 1 经过 2 个 Swin Transformer Block，做了窗口滑动后输出的尺寸依然为 $56\times56\times96$。</p><p>Stage 2 经过 Patch Merging后，尺寸减半，通道数翻倍，变成了$ 28\times28\times192$，再经过 2 个 Swin Transformer Block，输出$ 28\times28\times192$。</p><p>Stage 3 经过Patch Merging后，尺寸减半，通道数翻倍，变成了$ 14\times14\times384$，再经过 6 个 Swin Transformer Block，也就是窗口滑动了 3 次，输出 $ 14\times14\times384$。</p><p>Stage 4 经过Patch Merging后，尺寸减半，通道数翻倍，变成了$ 7\times7\times768$，再经过 2 个 Swin Transformer Block，输出$ 7\times7\times768$。</p><h3 id=path-merging class=heading-element><span>Path Merging</span>
<a href=#path-merging class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>$H\times W \times C -> \ \frac{H}{2}\times \frac{W}{2} \times 4C-> \ \frac{H}{2}\times \frac{W}{2} \times 2C$</p></blockquote><center><img src=/images/Transformer_CV/VIT.assets/swin_Path_Merging.png width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin_Path_Merging</div></center><h3 id=swin-transformer-block class=heading-element><span>Swin Transformer Block</span>
<a href=#swin-transformer-block class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p><strong>W-MSA</strong></p><blockquote><table border=0><tr><td align=center><img src=/images/Transformer_CV/VIT.assets/swin_MSA.gif></td><td align=center><img src=/images/Transformer_CV/VIT.assets/swin_WMSA.gif></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">MSA</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">W_MSA</td></tr></table><p>拿stage1举例：尺寸为$56\times56\times96$；每个窗口按照默认来说，都只有$M^2=7^2=49$个 patch；共有$ (56/7)\times(56/7)=8\times8=64 $个窗口。这64个窗口里分别去算它们的自注意力。</p></blockquote><p><strong>SW-MSA</strong></p><blockquote><table border=0><tr><td><img src=/images/Transformer_CV/VIT.assets/swin_SWMSA.gif></td><td><img src=/images/Transformer_CV/VIT.assets/swin_SWMSA.png></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">SW_MSA</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">SW_MSA</td></tr></table>移动窗口就是把原来的窗口往右下角移动一半窗口(M/2)的距离<blockquote><p>如果Transformer是上下两层连着做这种操作，先是 window再是 shifted window 的话，就能起到窗口和窗口之间互相通信的目的。</p><p>两个结构是成对使用的，先使用一个W-MSA结构再使用一个SW-MSA结构。所以堆叠Swin Transformer Block的次数都是偶数。</p></blockquote></blockquote></blockquote>$$
Attention(Q,K,V)=softmax(\frac{Q\dot K^T}{\sqrt d}V)
$$<p><strong>SA模块</strong></p><p>$X^{hw\times C} \cdot W^{C\times C}_q = Q^{hw\times C}$ 矩阵运算量计算：$hw\times C \times C$</p><blockquote><p>$X^{hw\times C} $：将所有像素（token）拼接在一起得到的矩阵（一共有hw个像素，每个像素的深度为C）</p><p>$W^{C\times C}_q$：生成query的变换矩阵</p></blockquote><p>同理K，V的生成也是$hw\times C \times C$，共$3hwC^2$</p><p>$Q\cdot K^T$：$(hw \times C )\cdot(C \times hw)->(hw)^2C$</p><p>$\frac{Q\dot K^T}{\sqrt d}V$：$(hw \times hw )\cdot(hw \times C)->(hw)^2C$</p><p>一共$3hwC^2+2(hw)^2C$</p><p><strong>MSA模块</strong></p><p>多头注意力模块相比单头注意力模块的计算量多最后一个线性投影层$(hw \times C )\cdot(C \times C)->hwC^2$</p><p>一共$4hwC^2+2(hw)^2C$</p><p><strong>W_MSA模块</strong></p><p>对每个窗口内使用多头注意力模块,一共有$\frac{h}{M}\times \frac{w}{M}$个窗口，窗口高宽M</p><p>计算量：$\frac{h}{M}\times \frac{w}{M} \times(4hwC^2+2(hw)^2C)=4hwC^2+2M^2hwC$</p><h2 id=shifted-window-attention class=heading-element><span><strong>Shifted Window Attention</strong></span>
<a href=#shifted-window-attention class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><strong>通过对特征图移位，并给Attention设置mask来间接实现的</strong></p><center><img src=/images/Transformer_CV/VIT.assets/swin_shift_mask.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin_shift_mask</div></center><p><strong>特征图移位</strong>：使用<code>torch.roll (x, shifts=-1, dims=0)</code>将第一排数值移动到最下面，再使用<code>torch.roll (x, shifts=-1, dims=1)</code>将变换后的第二张图中的第一列移动到最右边</p><center><img src=/images/Transformer_CV/VIT.assets/swin_shift.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin_shift</div></center><center><img src=/images/Transformer_CV/VIT.assets/swin_mask0.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">swin_mask</div></center><table border=0><tr><td align=center><img src=/images/Transformer_CV/VIT.assets/swin_mask1.png></td><td align=center><img src=/images/Transformer_CV/VIT.assets/swin_mask2.png></td><td align=center><img src=/images/Transformer_CV/VIT.assets/swin_mask3.png></td></tr><tr><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Mask_1</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Mask_2</td><td align=center style="color:orange;border-bottom:1px solid #d9d9d9;color:#999;padding:2px">Mask_3</td></tr></table><p>上图黑色区域是需要的，白色需要Mask，加上较大负数如-100即可。</p><p>最后还要恢复位置reverse cyclic shift</p><h2 id=relative-position-bias class=heading-element><span>Relative position bias</span>
<a href=#relative-position-bias class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2>$$
Attention(Q,K,V)=softmax((\frac{Q\dot K^T}{\sqrt d}+B)V)
$$<center><img src=/images/Transformer_CV/VIT.assets/swin_position_0.png width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Relative position bias</div></center><p>上图中的窗口中有 2*2 个 patch，分别给这四个位置标上绝对位置索引，分别为 (0,0)、(0,1)、(1,0)、(1,1)，第一个序号代表行，第二个序号代表列。以蓝色像素为参考点。用蓝色像素的绝对位置索引与其他位置索引进行相减，就得到其他位置相对蓝色像素的<strong>相对位置索引</strong>。我们将各个相对位置索引展开成一个行向量，再进行拼接得到了下面的矩阵。</p><center><img src=/images/Transformer_CV/VIT.assets/swin_position_1.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Relative position bias</div></center><p>将该矩阵加上一个 <strong>M-1</strong>，M 为窗口的大小，在 Swin Transformer 中为 7，这里为 2。再将每一个<strong>行标</strong>都乘以 <strong>2M-1</strong>，最后将行标和列标求和，就得到最后一个矩阵的值，这个矩阵中的值就是相对位置索引</p><center><img src=/images/Transformer_CV/VIT.assets/swin_position_table.png width=600><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Relative position bias</div></center><p>这个相对位置索引需要去索引的值会有一个相对位置偏置表 (relative position bias table)；这个表的元素的个数为 <strong>(2M-1)*(2M-1)</strong>。</p><center><img src=/images/Transformer_CV/VIT.assets/swin_bias.png><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px">Relative position bias</div></center><h2 id=拓展阅读-1 class=heading-element><span>拓展阅读</span>
<a href=#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p><a href=pytorch_classification/swin_transformer>Pytorch实现代码</a></p><p><a href="https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.788&amp;vd_source=d28e92983881d85b633a5acf8e46efaa" target=_blank rel="external nofollow noopener noreferrer">B站：Swin Transformer论文精读【论文精读】</a></p><p><a href="https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.788.recommend_more_video.1&amp;vd_source=d28e92983881d85b633a5acf8e46efaa" target=_blank rel="external nofollow noopener noreferrer">B站：Swin-Transformer网络结构详解</a></p><p><a href=https://aistudio.baidu.com/aistudio/education/preview/2011960 target=_blank rel="external nofollow noopener noreferrer">从零开始学视觉Transformer</a></p><p><a href=https://my.oschina.net/u/3768341/blog/5529722 target=_blank rel="external nofollow noopener noreferrer">Swin Transformer 介绍</a></p><p><a href=https://avoid.overfit.cn/post/50b62c574f364a62b53c4db363486f74 target=_blank rel="external nofollow noopener noreferrer">使用动图深入解释微软的Swin Transformer</a></p><p><a href=https://zhuanlan.zhihu.com/p/367111046 target=_blank rel="external nofollow noopener noreferrer">知乎：图解Swin Transformer</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-06-09 18:22:27">更新于 2023-06-09&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/ data-title=VIT data-hashtags="Deep Learning,Transformer,CV"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/ data-hashtag="Deep Learning"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/ data-title=VIT><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/deep-learning/ class=post-tag title="标签 - Deep Learning">Deep Learning</a><a href=/tags/transformer/ class=post-tag title="标签 - Transformer">Transformer</a><a href=/tags/cv/ class=post-tag title="标签 - CV">CV</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/deeplearning/multimodal-learning/albef/ class=post-nav-item rel=prev title=ALBEF><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>ALBEF</a><a href=/posts/deeplearning/transformer_cv/mae/ class=post-nav-item rel=next title=MAE>MAE<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.139.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.15"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>
<a href=/></a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:100},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50},version:"v0.3.15"}</script><script src=/js/theme.min.js defer></script></body></html>