<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>所有文章 - fengchen</title><link>http://fengchen321.github.io/posts/</link><description>fengchen</description><generator>Hugo 0.139.0 &amp; FixIt v0.3.15</generator><language>zh-CN</language><lastBuildDate>Sun, 01 Dec 2024 13:33:12 +0800</lastBuildDate><atom:link href="http://fengchen321.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Linx系统编程</title><link>http://fengchen321.github.io/posts/computer/linx%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/</link><pubDate>Sun, 01 Dec 2024 13:33:12 +0800</pubDate><guid>http://fengchen321.github.io/posts/computer/linx%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/</guid><category domain="http://fengchen321.github.io/categories/computer/">Computer</category><description>&lt;h2 id="linx系统编程" class="heading-element">&lt;span>Linx系统编程&lt;/span>
 &lt;a href="#linx%e7%b3%bb%e7%bb%9f%e7%bc%96%e7%a8%8b" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>在 Linux 中，手册节号通常被分为以下 8 个部分：&lt;/p></description></item><item><title>Record</title><link>http://fengchen321.github.io/posts/other/record/</link><pubDate>Sun, 01 Dec 2024 13:33:12 +0800</pubDate><guid>http://fengchen321.github.io/posts/other/record/</guid><category domain="http://fengchen321.github.io/categories/other/">Other</category><description>&lt;h2 id="latex" class="heading-element">&lt;span>Latex&lt;/span>
 &lt;a href="#latex" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>KaTeX 默认不支持 numcases 环境，还是使用cases吧。&lt;/p></description></item><item><title>Blog配置</title><link>http://fengchen321.github.io/posts/other/blog/</link><pubDate>Sun, 24 Nov 2024 20:14:59 +0800</pubDate><guid>http://fengchen321.github.io/posts/other/blog/</guid><category domain="http://fengchen321.github.io/categories/other/">Other</category><description>&lt;h2 id="安装hugo" class="heading-element">&lt;span>安装Hugo&lt;/span>
 &lt;a href="#%e5%ae%89%e8%a3%85hugo" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">winget install Hugo.Hugo.Extended&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>安装完成后查看hugo版本验证安装是否成功&lt;/p></description></item><item><title>BERT</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/</link><pubDate>Sat, 10 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="bert" class="heading-element">&lt;span>BERT&lt;/span>
 &lt;a href="#bert" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1810.04805"target="_blank" rel="external nofollow noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a>
作者：Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
发表时间：(NAACL-HLT 2019)&lt;/p></description></item><item><title>Transformer</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/</link><pubDate>Sat, 10 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="transformer" class="heading-element">&lt;span>Transformer&lt;/span>
 &lt;a href="#transformer" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1706.03762#"target="_blank" rel="external nofollow noopener noreferrer">Attention Is All You Need&lt;/a>
作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
发表时间：(NIPS 2017)&lt;/p></description></item><item><title>MAE</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_cv/mae/</link><pubDate>Fri, 09 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_cv/mae/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="mae" class="heading-element">&lt;span>MAE&lt;/span>
 &lt;a href="#mae" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2111.06377"target="_blank" rel="external nofollow noopener noreferrer">Masked Autoencoders Are Scalable Vision Learners&lt;/a>
作者：Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick&lt;/p></description></item><item><title>VIT</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/</link><pubDate>Fri, 09 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;p>[toc]&lt;/p>
&lt;h2 id="vision-transformer-vit" class="heading-element">&lt;span>Vision Transformer (VIT)&lt;/span>
 &lt;a href="#vision-transformer-vit" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2010.11929"target="_blank" rel="external nofollow noopener noreferrer">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a>
作者：Alexey Dosovitskiy; Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,Xiaohua Zhai
发表时间：(ICLR 2021)&lt;/p></description></item><item><title>ALBEF</title><link>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/albef/</link><pubDate>Thu, 08 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/albef/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="albef" class="heading-element">&lt;span>ALBEF&lt;/span>
 &lt;a href="#albef" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2107.07651"target="_blank" rel="external nofollow noopener noreferrer">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Align-before-Fuse%3A-Vision-and-Language-Learning-Li-Selvaraju/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>CLIP</title><link>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/</link><pubDate>Thu, 08 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="clip" class="heading-element">&lt;span>CLIP&lt;/span>
 &lt;a href="#clip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2103.00020"target="_blank" rel="external nofollow noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>VILT</title><link>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/</link><pubDate>Thu, 08 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="vilt" class="heading-element">&lt;span>VILT&lt;/span>
 &lt;a href="#vilt" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2102.03334"target="_blank" rel="external nofollow noopener noreferrer">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>BYOL</title><link>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/</link><pubDate>Wed, 07 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="swav" class="heading-element">&lt;span>SwAV&lt;/span>
 &lt;a href="#swav" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2006.09882"target="_blank" rel="external nofollow noopener noreferrer">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>InstDisc</title><link>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/instdisc/</link><pubDate>Wed, 07 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/instdisc/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="instdisc" class="heading-element">&lt;span>InstDisc&lt;/span>
 &lt;a href="#instdisc" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1805.01978"target="_blank" rel="external nofollow noopener noreferrer">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Unsupervised-Feature-Learning-via-Non-Parametric-Wu-Xiong/41b03c500922893906d04403cff16a5d08f26ea7"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>Moco</title><link>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/</link><pubDate>Wed, 07 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/moco/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="moco" class="heading-element">&lt;span>MoCo&lt;/span>
 &lt;a href="#moco" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1911.05722"target="_blank" rel="external nofollow noopener noreferrer">Momentum Contrast for Unsupervised Visual Representation Learning&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Momentum-Contrast-for-Unsupervised-Visual-Learning-He-Fan/ec46830a4b275fd01d4de82bffcabe6da086128f"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>SimCLR</title><link>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/simclr/</link><pubDate>Wed, 07 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/simclr/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="simclr-v1" class="heading-element">&lt;span>SimCLR-V1&lt;/span>
 &lt;a href="#simclr-v1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2002.05709"target="_blank" rel="external nofollow noopener noreferrer">A Simple Framework for Contrastive Learning of Visual Representations&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/A-Simple-Framework-for-Contrastive-Learning-of-Chen-Kornblith/34733eaf66007516347a40ad5d9bbe1cc9dacb6b"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>Branch Channel Attention</title><link>http://fengchen321.github.io/posts/deeplearning/attention/branch_channel-attention/</link><pubDate>Tue, 06 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/attention/branch_channel-attention/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="different-branches" class="heading-element">&lt;span>different branches&lt;/span>
 &lt;a href="#different-branches" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;h2 id="sknet" class="heading-element">&lt;span>SKNet&lt;/span>
 &lt;a href="#sknet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1903.06586"target="_blank" rel="external nofollow noopener noreferrer">Selective Kernel Networks&lt;/a>
作者：Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang
发表时间：(CVPR 2019)&lt;/p></description></item><item><title>Channel &amp;&amp; Spatial Attention</title><link>http://fengchen321.github.io/posts/deeplearning/attention/channlespatial-attention/</link><pubDate>Tue, 06 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/attention/channlespatial-attention/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="directly-estimate-3d-attention-map" class="heading-element">&lt;span>Directly estimate 3D attention map&lt;/span>
 &lt;a href="#directly-estimate-3d-attention-map" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;h2 id="residual-attention" class="heading-element">&lt;span>Residual Attention&lt;/span>
 &lt;a href="#residual-attention" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1704.06904"target="_blank" rel="external nofollow noopener noreferrer">Residual Attention Network for Image Classification&lt;/a>
作者：Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang
发表时间：(CVPR 2017)&lt;/p></description></item><item><title>Channel Attention</title><link>http://fengchen321.github.io/posts/deeplearning/attention/channel-attention/</link><pubDate>Tue, 06 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/attention/channel-attention/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="senet" class="heading-element">&lt;span>SENet&lt;/span>
 &lt;a href="#senet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1709.01507"target="_blank" rel="external nofollow noopener noreferrer">Squeeze-and-Excitation Networks&lt;/a>
作者：Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu
发表时间：(CVPR 2018)&lt;/p></description></item><item><title>Spatial Attention</title><link>http://fengchen321.github.io/posts/deeplearning/attention/spatial-attention/</link><pubDate>Tue, 06 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/attention/spatial-attention/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="基于-rnn-的方法" class="heading-element">&lt;span>基于 RNN 的方法&lt;/span>
 &lt;a href="#%e5%9f%ba%e4%ba%8e-rnn-%e7%9a%84%e6%96%b9%e6%b3%95" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;h2 id="ram" class="heading-element">&lt;span>RAM&lt;/span>
 &lt;a href="#ram" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：
作者：
发表时间：()&lt;/p></description></item><item><title>CycleGAN</title><link>http://fengchen321.github.io/posts/deeplearning/generative-model/cyclegan/</link><pubDate>Mon, 05 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/generative-model/cyclegan/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="cyclegan" class="heading-element">&lt;span>CycleGAN&lt;/span>
 &lt;a href="#cyclegan" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1703.10593"target="_blank" rel="external nofollow noopener noreferrer">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a>
作者：&lt;a href="https://www.cs.cmu.edu/~junyanz/"target="_blank" rel="external nofollow noopener noreferrer">Jun-Yan Zhu&lt;/a>, &lt;a href="https://taesung.me/"target="_blank" rel="external nofollow noopener noreferrer">Taesung Park&lt;/a>, &lt;a href="http://web.mit.edu/phillipi/"target="_blank" rel="external nofollow noopener noreferrer">Phillip Isola&lt;/a> &lt;a href="http://www.eecs.berkeley.edu/~efros/"target="_blank" rel="external nofollow noopener noreferrer">Alexei A. Efros&lt;/a>&lt;/p></description></item><item><title>DALL·E·2</title><link>http://fengchen321.github.io/posts/deeplearning/generative-model/dall_e2/</link><pubDate>Mon, 05 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/generative-model/dall_e2/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="dalle2" class="heading-element">&lt;span>DALL·E·2&lt;/span>
 &lt;a href="#dalle2" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2204.06125"target="_blank" rel="external nofollow noopener noreferrer">Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a> |&lt;a href="https://www.semanticscholar.org/paper/Hierarchical-Text-Conditional-Image-Generation-with-Ramesh-Dhariwal/c57293882b2561e1ba03017902df9fc2f289dea2"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;br>
作者：Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/p></description></item><item><title>GAN</title><link>http://fengchen321.github.io/posts/deeplearning/generative-model/gan/</link><pubDate>Mon, 05 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/generative-model/gan/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="gan" class="heading-element">&lt;span>GAN&lt;/span>
 &lt;a href="#gan" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1406.2661"target="_blank" rel="external nofollow noopener noreferrer">Generative Adversarial Networks&lt;/a>
作者：Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,&lt;/p></description></item><item><title>SSD</title><link>http://fengchen321.github.io/posts/deeplearning/object-detection/ssd/</link><pubDate>Sun, 04 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/object-detection/ssd/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="ssd" class="heading-element">&lt;span>SSD&lt;/span>
 &lt;a href="#ssd" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1512.02325"target="_blank" rel="external nofollow noopener noreferrer">SSD: Single Shot MultiBox Detector&lt;/a>
作者：Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy
发表时间：(ECCV 2016)&lt;/p></description></item><item><title>YOLO</title><link>http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/</link><pubDate>Sun, 04 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/object-detection/yolo/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;p>[toc]&lt;/p>
&lt;h2 id="yolo-v1" class="heading-element">&lt;span>YOLO V1&lt;/span>
 &lt;a href="#yolo-v1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1506.02640"target="_blank" rel="external nofollow noopener noreferrer">You Only Look Once:Unified, Real-Time Object Detection&lt;/a>
作者：&lt;a href="https://pjreddie.com/"target="_blank" rel="external nofollow noopener noreferrer">Joseph Redmon&lt;/a>, Santosh Divvalay, &lt;a href="http://www.rossgirshick.info/"target="_blank" rel="external nofollow noopener noreferrer">Ross Girshick&lt;/a>, &lt;a href="https://homes.cs.washington.edu/~ali/index.html"target="_blank" rel="external nofollow noopener noreferrer">Ali Farhadi&lt;/a>
发表时间：(CVPR 2016)&lt;/p></description></item><item><title>Distilling knowledge</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/distilling-knowledge-/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/distilling-knowledge-/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="distilling-knowledge" class="heading-element">&lt;span>Distilling knowledge&lt;/span>
 &lt;a href="#distilling-knowledge" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1503.02531"target="_blank" rel="external nofollow noopener noreferrer">Distilling the knowledge in a neural network&lt;/a>&lt;/p></description></item><item><title>GhostNet</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/ghostnet/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/ghostnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="ghostnet" class="heading-element">&lt;span>GhostNet&lt;/span>
 &lt;a href="#ghostnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1911.11907"target="_blank" rel="external nofollow noopener noreferrer">GhostNet: More Features from Cheap Operations&lt;/a>&lt;/p></description></item><item><title>LCNet</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/lcnet/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/lcnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="pp-lcnet" class="heading-element">&lt;span>PP-LCNet&lt;/span>
 &lt;a href="#pp-lcnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2109.15099"target="_blank" rel="external nofollow noopener noreferrer">PP-LCNet: A Lightweight CPU Convolutional Neural Network&lt;/a>&lt;/p></description></item><item><title>MobileNet</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/mobilenet/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/mobilenet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="mobilenetv1" class="heading-element">&lt;span>MobileNetV1&lt;/span>
 &lt;a href="#mobilenetv1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1704.04861"target="_blank" rel="external nofollow noopener noreferrer">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications&lt;/a>&lt;/p></description></item><item><title>NAS</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/nas/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/nas/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="mnasnet" class="heading-element">&lt;span>MnasNet&lt;/span>
 &lt;a href="#mnasnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Tan_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile_CVPR_2019_paper"target="_blank" rel="external nofollow noopener noreferrer">MnasNet: Platform-Aware Neural Architecture Search for Mobile&lt;/a>
作者：Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le
发表时间：(CVPR 2019)&lt;/p></description></item><item><title>Re-parameterization</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/re-parameterization/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/re-parameterization/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="acnet" class="heading-element">&lt;span>ACNet&lt;/span>
 &lt;a href="#acnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1908.03930"target="_blank" rel="external nofollow noopener noreferrer">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks&lt;/a>&lt;/p></description></item><item><title>ShuffleNet</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/shufflenet/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/shufflenet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="shufflenetv1" class="heading-element">&lt;span>ShuffleNetV1&lt;/span>
 &lt;a href="#shufflenetv1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html"target="_blank" rel="external nofollow noopener noreferrer">Shufflenet: An extremely efficient convolutional neural network for mobile devices&lt;/a>
作者：&lt;a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=sra"target="_blank" rel="external nofollow noopener noreferrer">Xiangyu Zhang&lt;/a>，&lt;a href="https://scholar.google.com/citations?user=Jv4LCj8AAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=sra"target="_blank" rel="external nofollow noopener noreferrer">Xinyu Zhou&lt;/a>，&lt;a href="https://scholar.google.com/citations?user=SCwGvlUAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=sra"target="_blank" rel="external nofollow noopener noreferrer">Mengxiao Lin&lt;/a> ，&lt;a href="https://scholar.google.com/citations?user=ALVSZAYAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=sra"target="_blank" rel="external nofollow noopener noreferrer">Jian Sun&lt;/a> ，Megvii Inc (Face++)
发表时间：(CVPR 2018)&lt;/p></description></item><item><title>SqueezeNet</title><link>http://fengchen321.github.io/posts/deeplearning/light-weight/squeezenet/</link><pubDate>Sat, 03 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/light-weight/squeezenet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="squeezenet" class="heading-element">&lt;span>SqueezeNet&lt;/span>
 &lt;a href="#squeezenet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1602.07360"target="_blank" rel="external nofollow noopener noreferrer">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size&lt;/a>
作者：
发表时间：(ICLR 2016)&lt;/p></description></item><item><title>AlexNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/alexnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="alexnet" class="heading-element">&lt;span>AlexNet&lt;/span>
 &lt;a href="#alexnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"target="_blank" rel="external nofollow noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p></description></item><item><title>AutoML</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/automl/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/automl/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="nir" class="heading-element">&lt;span>NIR&lt;/span>
 &lt;a href="#nir" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2002.12580"target="_blank" rel="external nofollow noopener noreferrer">Neural Inheritance Relation Guided One-Shot Layer Assignment Search&lt;/a>
作者：&lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Meng%2C&amp;#43;R"target="_blank" rel="external nofollow noopener noreferrer">Rang Meng&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chen%2C&amp;#43;W"target="_blank" rel="external nofollow noopener noreferrer">Weijie Chen&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xie%2C&amp;#43;D"target="_blank" rel="external nofollow noopener noreferrer">Di Xie&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang%2C&amp;#43;Y"target="_blank" rel="external nofollow noopener noreferrer">Yuan Zhang&lt;/a>, &lt;a href="https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Pu%2C&amp;#43;S"target="_blank" rel="external nofollow noopener noreferrer">Shiliang Pu&lt;/a>
发表时间：(AAAI 2020)&lt;/p></description></item><item><title>ConvNeXt</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/convnext-/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/convnext-/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="convnext" class="heading-element">&lt;span>ConvNeXt&lt;/span>
 &lt;a href="#convnext" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2201.03545v1"target="_blank" rel="external nofollow noopener noreferrer">A ConvNet for the 2020s&lt;/a>
作者：&lt;a href="https://liuzhuang13.github.io/"target="_blank" rel="external nofollow noopener noreferrer">Zhuang Liu&lt;/a>, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie&lt;/p></description></item><item><title>EfficientNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/efficientnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/efficientnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="efficientnet" class="heading-element">&lt;span>EfficientNet&lt;/span>
 &lt;a href="#efficientnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1905.11946"target="_blank" rel="external nofollow noopener noreferrer">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks&lt;/a>
作者：Mingxing Tan, Quoc V. Le
发表时间：(ICML 2019)&lt;/p></description></item><item><title>Inception</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/inception/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/inception/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;p>策略：&lt;font color=#f12c60>&lt;strong>split-transform-merge&lt;/strong>&lt;/font>&lt;/p>
&lt;h2 id="inceptionv1googlenet" class="heading-element">&lt;span>InceptionV1（GoogLeNet）&lt;/span>
 &lt;a href="#inceptionv1googlenet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1409.4842"target="_blank" rel="external nofollow noopener noreferrer">Going Deeper with Convolutions&lt;/a>&lt;/p></description></item><item><title>ResNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/resnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="resnet" class="heading-element">&lt;span>ResNet&lt;/span>
 &lt;a href="#resnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1512.03385"target="_blank" rel="external nofollow noopener noreferrer">Deep Residual Learning for Image Recognition&lt;/a>&lt;/p></description></item><item><title>SENet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/senet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/senet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="senet" class="heading-element">&lt;span>SENet&lt;/span>
 &lt;a href="#senet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1709.01507"target="_blank" rel="external nofollow noopener noreferrer">Squeeze-and-Excitation Networks&lt;/a>
作者：Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu
发表时间：(CVPR 2018)&lt;/p></description></item><item><title>VGGNet</title><link>http://fengchen321.github.io/posts/deeplearning/image-classification/vggnet/</link><pubDate>Fri, 02 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/image-classification/vggnet/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="vggnet" class="heading-element">&lt;span>VGGNet&lt;/span>
 &lt;a href="#vggnet" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1409.1556"target="_blank" rel="external nofollow noopener noreferrer">Very Deep Convolutional Networks for Large-Scale Visual Recognition&lt;/a>
作者：&lt;a href="https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=L7lMQkQAAAAJ"target="_blank" rel="external nofollow noopener noreferrer">Simonyan K&lt;/a>, &lt;a href="https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=UZ5wscMAAAAJ"target="_blank" rel="external nofollow noopener noreferrer">Zisserman A. V&lt;/a>
发表时间：(ICLR 2015)
&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"target="_blank" rel="external nofollow noopener noreferrer">论文主页&lt;/a>&lt;/p></description></item><item><title>Deep Learning Paper</title><link>http://fengchen321.github.io/posts/deeplearning/paper/</link><pubDate>Thu, 01 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/paper/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="paper" class="heading-element">&lt;span>Paper&lt;/span>
 &lt;a href="#paper" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;h2 id="image-classification" class="heading-element">&lt;span>Image Classification&lt;/span>
 &lt;a href="#image-classification" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;font face="Noto Serif SC" color=#ff0000>&lt;strong>ALexNet&lt;/strong>&lt;/font>：&lt;a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"target="_blank" rel="external nofollow noopener noreferrer">ImageNet Classification with Deep Convolutional Neural Networks&lt;/a> (NIPS 2012)&lt;/p></description></item></channel></rss>