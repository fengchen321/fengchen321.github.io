<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>fengchen</title><link>http://fengchen321.github.io/</link><description>fengchen</description><generator>Hugo 0.139.0 &amp; FixIt v0.3.15</generator><language>zh-CN</language><lastBuildDate>Sun, 24 Nov 2024 20:14:59 +0800</lastBuildDate><atom:link href="http://fengchen321.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog配置</title><link>http://fengchen321.github.io/posts/other/blog/</link><pubDate>Sun, 24 Nov 2024 20:14:59 +0800</pubDate><guid>http://fengchen321.github.io/posts/other/blog/</guid><category domain="http://fengchen321.github.io/categories/other/">Other</category><description>&lt;h2 id="安装hugo" class="heading-element">&lt;span>安装Hugo&lt;/span>
 &lt;a href="#%e5%ae%89%e8%a3%85hugo" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">winget install Hugo.Hugo.Extended&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>安装完成后查看hugo版本验证安装是否成功&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">hugo version&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="配置博客源" class="heading-element">&lt;span>配置博客源&lt;/span>
 &lt;a href="#%e9%85%8d%e7%bd%ae%e5%8d%9a%e5%ae%a2%e6%ba%90" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;h2 id="使用hugo创建网站" class="heading-element">&lt;span>使用hugo创建网站&lt;/span>
 &lt;a href="#%e4%bd%bf%e7%94%a8hugo%e5%88%9b%e5%bb%ba%e7%bd%91%e7%ab%99" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">hugo new site blog &lt;span class="c1"># 可以替换成任意你想要的名字&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># github里创建同名仓库，到时候git push进去就行&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="安装主题" class="heading-element">&lt;span>安装主题&lt;/span>
 &lt;a href="#%e5%ae%89%e8%a3%85%e4%b8%bb%e9%a2%98" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://themes.gohugo.io/"target="_blank" rel="external nofollow noopener noreferrer">hugo 主题&lt;/a>&lt;/p>
&lt;p>根据主题文档安装，用的&lt;a href="https://fixit.lruihao.cn/zh-cn/documentation/installation/"target="_blank" rel="external nofollow noopener noreferrer">fixit主题&lt;/a>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> blog
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git submodule add https://github.com/hugo-fixit/FixIt.git themes/FixIt
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git submodule update --remote --merge themes/FixIt&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>测试只需要把主blog/themes/FixIt/demo放在blog下相对于的文件夹里就行，比如&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">cp themes/FixIt/demo/hugo.toml hugo.toml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># blog/themes/FixIt/demo/content/posts 替换blog/content&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中修改hugo.toml里的baseurl修改成你的网站&lt;user>.github.io&lt;/p>
&lt;h2 id="本地调试和预览" class="heading-element">&lt;span>本地调试和预览&lt;/span>
 &lt;a href="#%e6%9c%ac%e5%9c%b0%e8%b0%83%e8%af%95%e5%92%8c%e9%a2%84%e8%a7%88" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">hugo new posts/test/a.md
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hugo server&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="配置action" class="heading-element">&lt;span>配置Action&lt;/span>
 &lt;a href="#%e9%85%8d%e7%bd%aeaction" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;code>settings -&amp;gt; Developer Settings -&amp;gt; Personal access tokens(Token classic) -&amp;gt; generate new token里创建一个tokens，注意勾选repo和workflow权限&lt;/code>&lt;/p>
&lt;p>在博客源仓库的&lt;code>Settings -&amp;gt; Secrets and variables -&amp;gt; Actions -&amp;gt; Repository secrets&lt;/code>中添加一个NAME为&lt;code>ACTION_TOKEN&lt;/code>（随便什么名字，后面要使用）内容为刚刚创建的tokens&lt;/p>
&lt;p>创建 blog/.github/workflows/gh-pages.yml&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">name: GitHub Pages
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">on:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> push:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> branches:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - main 
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> pull_request:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">jobs:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> deploy:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> runs-on: ubuntu-22.04
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> concurrency:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> group: &lt;span class="si">${&lt;/span>&lt;span class="p">{ github.workflow &lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="o">}&lt;/span>-&lt;span class="si">${&lt;/span>&lt;span class="p">{ github.ref &lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> steps:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - uses: actions/checkout@v4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> with:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> submodules: &lt;span class="nb">true&lt;/span> &lt;span class="c1"># Fetch Hugo themes (true OR recursive)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> fetch-depth: &lt;span class="m">0&lt;/span> &lt;span class="c1"># Fetch all history for .GitInfo and .Lastmod&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: Setup Hugo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> uses: peaceiris/actions-hugo@v3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> with:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> hugo-version: &lt;span class="s1">&amp;#39;0.139.0&amp;#39;&lt;/span> 
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> extended: &lt;span class="nb">true&lt;/span> &lt;span class="c1"># 是否启用hugo extended&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: Build
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> run: hugo --minify
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: Deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> uses: peaceiris/actions-gh-pages@v3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> with:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> EXTERNAL_REPOSITORY: fengchen321/fengchen321.github.io &lt;span class="c1"># 你的Github Pages远程仓库名&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> PERSONAL_TOKEN: &lt;span class="si">${&lt;/span>&lt;span class="p">{ secrets.ACTION_TOKEN &lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="o">}&lt;/span> &lt;span class="c1"># setting 存放的名字而不是原始key&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> PUBLISH_DIR: ./public
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> PUBLISH_BRANCH: main&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>push该博客源即可&lt;/p>
&lt;h2 id="参考阅读" class="heading-element">&lt;span>参考阅读&lt;/span>
 &lt;a href="#%e5%8f%82%e8%80%83%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://ratmomo.github.io/p/2024/06/%E4%BD%BF%E7%94%A8-hugo--github-pages-%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"target="_blank" rel="external nofollow noopener noreferrer">使用 Hugo + Github Pages 部署个人博客&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://matrix-a.github.io/"target="_blank" rel="external nofollow noopener noreferrer">matrix-a&lt;/a>&lt;/p></description></item><item><title>BERT</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/</link><pubDate>Sat, 10 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/bert/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="bert" class="heading-element">&lt;span>BERT&lt;/span>
 &lt;a href="#bert" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1810.04805"target="_blank" rel="external nofollow noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a>
作者：Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
发表时间：(NAACL-HLT 2019)&lt;/p>
&lt;p>&lt;a href="https://github.com/google-research/bert"target="_blank" rel="external nofollow noopener noreferrer">官方代码&lt;/a>&lt;/p>
&lt;p>==Transformer一统NLP的开始==&lt;/p>
&lt;/blockquote>
&lt;p>BERT: 用深的、双向的、transformer 来做预训练，用来做语言理解的任务。&lt;/p>
&lt;blockquote>
&lt;p>pre-training: 在一个大的数据集上训练好一个模型 pre-training，模型的主要任务是用在其它任务 training 上&lt;/p>
&lt;p>deep bidirectional transformers: 深的双向 transformers&lt;/p>
&lt;p>language understanding: 更广义，transformer 主要用在机器翻译 MT&lt;/p>
&lt;/blockquote>
&lt;h2 id="abstract" class="heading-element">&lt;span>Abstract&lt;/span>
 &lt;a href="#abstract" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>新的语言表征模型 BERT: &lt;strong>B&lt;/strong>idirectional &lt;strong>E&lt;/strong>ncoder &lt;strong>R&lt;/strong>epresentations from &lt;strong>T&lt;/strong>ransformers&lt;/p>
&lt;blockquote>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/BERT.assets/Elmo_GPT_Bert.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Elmo_GPT_Bert&lt;/div>
&lt;/center>
&lt;p>&lt;a href="https://arxiv.org/abs/1802.05365v2"target="_blank" rel="external nofollow noopener noreferrer">ELMo&lt;/a>：使用左右侧的上下文信息 ；基于RNN，应用下游任务需要一点点调整架构&lt;/p>
&lt;p>GPT：使用左边的上下文信息，预测未来&lt;/p>
&lt;p>BERT：使用左右侧的上下文信息 ；基于Transformer，应用下游任务只需要调整最上层&lt;/p>
&lt;blockquote>
&lt;p>从无标注的文本中（jointly conditioning 联合左右的上下文信息）预训练得到无标注文本的 deep bidirectional representations&lt;/p>
&lt;p>BERT = ELMo 的 bidirectional 信息 + GPT 的新架构 transformer&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="introduction" class="heading-element">&lt;span>Introduction&lt;/span>
 &lt;a href="#introduction" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>NLP任务分两类&lt;/p>
&lt;blockquote>
&lt;p>sentence-level tasks ：句子情绪识别、两个句子的关系；&lt;/p>
&lt;p>token-level tasks ：NER (人名、街道名) 需要 fine-grained output&lt;/p>
&lt;/blockquote>
&lt;p>BERT训练方法&lt;/p>
&lt;blockquote>
&lt;p>通过 MLM 带掩码的语言模型作为预训练的目标，来减轻语言模型的单向约束。inspired by the Close task 1953&lt;/p>
&lt;p>**MLM ** (masked language model)：每次随机选输入的词源 tokens, 然后 mask 它们，目标函数是预测被 masked 的词；15%的词汇mask&lt;/p>
&lt;blockquote>
&lt;blockquote>
&lt;p>假设输入里面的第二个词汇是被盖住的，把其对应的embedding输入到一个多分类模型中，来预测被盖住的单词。类似挖空填词、完形填空&lt;/p>
&lt;/blockquote>
&lt;p>standard language model：只看左边的信息&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>NSP&lt;/strong>: (next sentence prediction )：预测下一个句子；判断两个句子是随机采样的 or 原文相邻，学习sentence-level 的信息。&lt;/p>
&lt;blockquote>
&lt;p>把两句话连起来，中间加一个[SEP]作为两个句子的分隔符。而在两个句子的开头，放一个[CLS]标志符，将其得到的embedding输入到二分类的模型，输出两个句子是不是接在一起的。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>在训练BERT的时候，这两个任务是同时训练的。所以，BERT的损失函数是把这两个任务的损失函数加起来的，是一个**「多任务」**训练&lt;/p>
&lt;p>﻿贡献&lt;/p>
&lt;blockquote>
&lt;p>bidirectional 双向信息的重要性&lt;/p>
&lt;p>BERT 首个微调模型，在 sentence-level and token-level task效果好&lt;/p>
&lt;p>好的预训练模型，不用对特定任务做一些模型架构的改动&lt;/p>
&lt;/blockquote>
&lt;h2 id="related-work" class="heading-element">&lt;span>Related Work&lt;/span>
 &lt;a href="#related-work" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>Unsupervised Feature-based approaches&lt;/p>
&lt;blockquote>
&lt;p>非监督的基于特征表示的工作：词嵌入、ELMo等&lt;/p>
&lt;/blockquote>
&lt;p>Unsupervised Fine-tuning approaches&lt;/p>
&lt;blockquote>
&lt;p>非监督的基于微调的工作：GPT等&lt;/p>
&lt;/blockquote>
&lt;p>Transfer Learning from Supervised Data&lt;/p>
&lt;blockquote>
&lt;p>在有标签的数据上做迁移学习。&lt;/p>
&lt;/blockquote>
&lt;h2 id="bert-1" class="heading-element">&lt;span>Bert&lt;/span>
 &lt;a href="#bert-1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>预训练 + 微调&lt;/strong>&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/BERT.assets/bert_stage.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">bert_stage&lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>pre-training：使用 unlabeled data 训练&lt;/p>
&lt;p>fine-tuning：微调的 BERT 使用预训练的参数 初始化，所有的权重参数通过下游任务的 labeled data 进行微调。&lt;/p>
&lt;p>每一个下游任务会创建一个 新的 BERT 模型，（由预训练参数初始化），但每一个下游任务会根据自己任务的labeled data 来微调自己的 BERT 模型。&lt;/p>
&lt;/blockquote>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">model name&lt;/th>
 &lt;th style="text-align: center">L&lt;/th>
 &lt;th style="text-align: center">H&lt;/th>
 &lt;th style="text-align: center">A&lt;/th>
 &lt;th style="text-align: center">Total Parameters&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">$BERT_{base}$&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">768&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">110M&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">$BERT_{base}$&lt;/td>
 &lt;td style="text-align: center">24&lt;/td>
 &lt;td style="text-align: center">1024&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">340M&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>L：transform blocks的个数
H：hidden size 隐藏层大小
A：自注意力机制 multi-head 中 head 头的个数&lt;/p>
&lt;/blockquote>
&lt;p>BERT 模型复杂度和层数 L 是 linear, 和宽度 H 是 平方关系。
深度变成了以前的两倍，在宽度上面也选择一个值，使得这个增加的平方大概是之前的两倍。&lt;/p>
&lt;blockquote>
&lt;p>$H_{large}=\sqrt {2} H_{base}=\sqrt 2 \times 768=1086$&lt;/p>
&lt;/blockquote>
&lt;p>H = 16，因为每个 head 的维度都固定在了64。所以宽度增加了， head 数也增加了&lt;/p>
&lt;blockquote>
&lt;p>$H = 64 \times A:\ \ 768=64\times 12;\ \ 1024=64\times 16$&lt;/p>
&lt;/blockquote>
&lt;p>嵌入层：输入字典大小30k，输出H&lt;/p>
&lt;p>transformer blocks($H^2\times 12$)：self-attention($H^2\times 4$) + MLP ($H^2\times 8$)&lt;/p>
&lt;blockquote>
&lt;p>Transformer block:&lt;/p>
&lt;blockquote>
&lt;p>多头Q,K,V投影矩阵合并$H(64\times A)$+输出后再H*H投影&lt;/p>
&lt;/blockquote>
&lt;p>MLP 的 2个全连接层：&lt;/p>
&lt;blockquote>
&lt;p>第一个全连接层输入是 H，输出是 4 * H；
第二个全 连接层输入是 4 * H，输出是 H。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>$Total \ Parameters = 30K\times H + 12 \times H^2 \times L$&lt;/p>
&lt;blockquote>
&lt;p>$BERT_{base} = 30000\times 768 + 12 \times 768^2 \times 12 = 107.97M$&lt;/p>
&lt;p>$BERT_{large} = 30000\times 1024+ 12 \times 1024^2\times 24= 332.71M$&lt;/p>
&lt;/blockquote>
&lt;h3 id="inputoutput-representation预训练微调共通部分" class="heading-element">&lt;span>Input/Output Representation(预训练&amp;amp;微调共通部分）&lt;/span>
 &lt;a href="#inputoutput-representation%e9%a2%84%e8%ae%ad%e7%bb%83%e5%be%ae%e8%b0%83%e5%85%b1%e9%80%9a%e9%83%a8%e5%88%86" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>BERT 的输入和 transformer 区别&lt;/p>
&lt;blockquote>
&lt;p>transformer 预训练时候的输入是一个序列对。编码器和解码器分别会输入一个序列。
BERT 只有一个编码器，为了使 BERT 能处理两个句子的情况，需要把两个句子并成一个序列。&lt;/p>
&lt;/blockquote>
&lt;p>BERT切词&lt;/p>
&lt;blockquote>
&lt;p>WordPiece, 把一个出现概率低的词切开，只保留一个词出现频率高的子序列，30k token 经常出现的词（子
序列）的字典。
否则，空格切词 &amp;ndash;&amp;gt; 一个词是一个 token。数据量打的时候，词典会特别大，到百万级别。可学习的参数基
本都在嵌入层了。&lt;/p>
&lt;/blockquote>
&lt;p>BERT 的输入序列构成 [ CLS ] + [ SEP ]&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/BERT.assets/Input_Representation.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">输入序列&lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>Token embeddings: 词源的embedding层，整成的embedding层， 每一个 token 有对应的词向量。
Segement embeddings: 这个 token 属于第一句话 A还是第二句话 B。
Position embedding 的输入是 token 词源在这个序列 sequence 中的位置信息。（和Transformer不一样，这是学习出来的）&lt;/p>
&lt;/blockquote>
&lt;p>BERT 的 segment embedding （属于哪个句子）和 position embedding （位置在哪里）是学习得来的，
transformer 的 position embedding 是给定的。&lt;/p>
&lt;blockquote>
&lt;p>序列开始:&lt;strong>[CLS]&lt;/strong> 输出的是句子层面的信息 sequence representation&lt;/p>
&lt;blockquote>
&lt;p>BERT 使用的是 transformer 的 encoder，self-attention layer 会看输入的每个词和其它所有词的关系。
就算 &lt;strong>[ CLS ]&lt;/strong> 这个词放在我的第一个的位置，他也是有办法能看到之后所有的词。所以他放在第一个是没关
系的，不一定要放在最后。&lt;/p>
&lt;/blockquote>
&lt;p>区分两个合在一起的句子的方法：&lt;/p>
&lt;blockquote>
&lt;p>每个句子后 + &lt;strong>[ SEP ]&lt;/strong> 表示 seperate
学一个嵌入层 来表示整个句子是第一句还是第二句&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h3 id="pre-training-bert" class="heading-element">&lt;span>Pre-training BERT&lt;/span>
 &lt;a href="#pre-training-bert" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>预训练的 key factors: 目标函数，预训练的数据&lt;/p>
&lt;h4 id="mlm" class="heading-element">&lt;span>MLM&lt;/span>
 &lt;a href="#mlm" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h4>&lt;p>由 WordPiece 生成的词源序列中的词源，它有 15% 的概率会随机替换成一个掩码。但是对于特殊的词源不
做替换&lt;/p>
&lt;blockquote>
&lt;p>15% 计划被 masked 的词：80% 的概率被替换为 [MASK], 10% 换成 random token,10% 不改变原 token。&lt;/p>
&lt;p>特殊的词源：第一个词源 [ CLS ] 和中间的分割词源 [SEP]。&lt;/p>
&lt;p>问题：预训练和微调看到的数据不一样&lt;/p>
&lt;blockquote>
&lt;p>预训练的输入序列有 15% [MASK]，微调时的数据没有 [MASK].&lt;/p>
&lt;/blockquote>
&lt;p>为什么要Mask&lt;/p>
&lt;blockquote>
&lt;p>语言模型会根据前面单词来预测下一个单词，但是self-attention的注意力只会放在自己身上，那么这样100%预测到自己，毫无意义，所以用Mask，把需要预测的词给挡住。&lt;/p>
&lt;/blockquote>
&lt;p>Mask方式优缺点：&lt;/p>
&lt;blockquote>
&lt;p>1）被随机选择15%的词当中以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力；&lt;/p>
&lt;p>2）被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）。&lt;/p>
&lt;p>3）针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息。主要针对这一短板，因此google此后发表了BERT-WWM，国内的哈工大联合讯飞发表了中文版的BERT-WWM。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h4 id="nsp" class="heading-element">&lt;span>NSP&lt;/span>
 &lt;a href="#nsp" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h4>&lt;p>输入序列有 2 个句子 A 和 B，50% 正例，50%反例&lt;/p>
&lt;blockquote>
&lt;p>50% B 在 A 之后,是一对连续句子，标记为 IsNext；50% 是语料库中 a random sentence 随机采样的，标记为 NotNext。&lt;/p>
&lt;p>&lt;img loading="lazy" src="BERT.assets/image-20220613225404379.png" alt="image-20220613225404379" srcset="BERT.assets/image-20220613225404379.png?size=small, BERT.assets/image-20220613225404379.png?size=medium 1.5x, BERT.assets/image-20220613225404379.png?size=large 2x" data-title="image-20220613225404379" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/p>
&lt;p>flight ## less：flightless 出现概率不高，WordPiece 分成了 2 个出现频率高的子序列，## 表示 less 是flightless 的一部分。&lt;/p>
&lt;/blockquote>
&lt;h3 id="fine-tuning-bert" class="heading-element">&lt;span>Fine-tuning BERT&lt;/span>
 &lt;a href="#fine-tuning-bert" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>BERT 经过微小的改造（增加一个小小的层），就可以用于各种各样的语言任务。&lt;/p>
&lt;p>（a,b）与 Next Sentence Prediction类似，通过在 &lt;strong>「[CLS]」&lt;/strong> 标记的 Transformer 输出顶部添加分类层，完成诸如情感分析之类的**「分类」**任务&lt;/p>
&lt;p>（c）在问答任务（例如 SQuAD v1.1）中，会收到一个关于文本序列的问题，并需要在序列中标记答案。使用 BERT，可以通过学习标记答案开始和结束的两个额外向量来训练问答模型。&lt;/p>
&lt;p>（d）在命名实体识别 (NER) 中，接收文本序列，并需要标记文本中出现的各种类型的实体（人、组织、日期等）。使用 BERT，可以通过将每个标记的输出向量输入到预测 NER 标签的分类层来训练 NER 模型&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/BERT.assets/Fine-tuning BERT.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">differernt tasks&lt;/div>
&lt;/center>
&lt;h2 id="拓展阅读" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://www.bilibili.com/video/BV1PL411M7eQ?spm_id_from=333.999.0.0"target="_blank" rel="external nofollow noopener noreferrer">BERT 论文逐段精读【论文精读】&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=UYPa347-DdE"target="_blank" rel="external nofollow noopener noreferrer">李宏毅：ELMO, BERT, GPT&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://jalammar.github.io/illustrated-bert/"target="_blank" rel="external nofollow noopener noreferrer">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://wmathor.com/index.php/archives/1456/"target="_blank" rel="external nofollow noopener noreferrer">BERT 详解（附带 ELMo、GPT 介绍）&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"target="_blank" rel="external nofollow noopener noreferrer">BERT 科普文&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/"target="_blank" rel="external nofollow noopener noreferrer">作者对双向的回应&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://hal.inria.fr/hal-02131630/document"target="_blank" rel="external nofollow noopener noreferrer">ACL 2019：What does BERT learn about the structure of language?&lt;/a>：BERT的低层网络就学习到了短语级别的信息表征，BERT的中层网络就学习到了丰富的语言学特征，而BERT的高层网络则学习到了丰富的语义信息特征&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/1905.05950"target="_blank" rel="external nofollow noopener noreferrer">BERT Rediscovers the Classical NLP Pipeline&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/cDG7DwHFL1kHErwyYGT4UA"target="_blank" rel="external nofollow noopener noreferrer">关于BERT：你不知道的事&lt;/a>&lt;/p></description></item><item><title>Transformer</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/</link><pubDate>Sat, 10 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_nlp/transformer/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="transformer" class="heading-element">&lt;span>Transformer&lt;/span>
 &lt;a href="#transformer" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1706.03762#"target="_blank" rel="external nofollow noopener noreferrer">Attention Is All You Need&lt;/a>
作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
发表时间：(NIPS 2017)&lt;/p>
&lt;p>==继MLP、CNN、RNN后的第四大类架构==&lt;/p>
&lt;/blockquote>
&lt;h2 id="introduction" class="heading-element">&lt;span>Introduction&lt;/span>
 &lt;a href="#introduction" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>sequence transduction:&lt;/strong> 序列转录，序列到序列的生成。input一个序列，output一个序列。&lt;/p>
&lt;blockquote>
&lt;p>机器翻译：输入一句中文，输出一句英文。&lt;/p>
&lt;/blockquote>
&lt;p>RNN ：从左往右一步一步计算，对第 t 个状态 $h_t$，由 $h_{t-1}$（历史信息）和 当前词 t 计算。&lt;/p>
&lt;blockquote>
&lt;p>难以并行。&lt;/p>
&lt;blockquote>
&lt;p>通过 factorization 分解 tricks 和 conditional computation 并行化来提升计算效率&lt;/p>
&lt;/blockquote>
&lt;p>过早的历史信息可能被丢掉。时序信息是一步一步往后传递的&lt;/p>
&lt;blockquote>
&lt;p>时序长的时候一个大的 $h_t$存历史信息。每一个 计算步都需要存储，内存开销大&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="background" class="heading-element">&lt;span>Background&lt;/span>
 &lt;a href="#background" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>CNN（局部像素&amp;ndash;&amp;gt;全部像素；多通道 &amp;ndash;&amp;gt; multi-head）&lt;/p>
&lt;blockquote>
&lt;p>Transformer 的 attention mechanism 每一次看到所有的像素，一层能够看到整个序列。&lt;/p>
&lt;p>Transformer 的 multi-head self-attention 模拟 CNNs 多通道输出的效果。&lt;/p>
&lt;/blockquote>
&lt;p>自注意力，是一种将单个序列的不同位置关联起来以计算序列表示的注意力机制&lt;/p>
&lt;h2 id="model-architecture" class="heading-element">&lt;span>Model Architecture&lt;/span>
 &lt;a href="#model-architecture" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/The Transformer - model architecture.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">The Transformer - model architecture&lt;/div>
&lt;/center>
&lt;p>先将输入&lt;strong>Input&lt;/strong>使用&lt;a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca"target="_blank" rel="external nofollow noopener noreferrer">&lt;strong>embedding algorithm&lt;/strong>&lt;/a>转成向量。&lt;/p>
&lt;blockquote>
&lt;p>编码器的都会接收到一个list（每个元素都是512维的词向量）。list的尺寸是可以设置的超参，通常是训练集的最长句子的长度。&lt;/p>
&lt;/blockquote>
&lt;p>加入位置编码&lt;strong>Positional Encoding&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>RNN ：把上一时刻的输出 作为下一个时刻的输入，来传递时序信息&lt;/p>
&lt;p>Attention： 在输入里面加入时序信息 &amp;ndash;&amp;gt; positional encoding&lt;/p>
&lt;blockquote>
&lt;p>output 是 value 的加权和（权重是 query 和 key 之间的距离，和序列信息无关）&lt;/p>
&lt;p>一个词在嵌入层表示成一个 512 维的向量，用另一个 512 维的向量来表示一个数字代表位置信息&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/Transformer_positional_Encoding.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Transformer_positional_Encoding&lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>positional encodding 是 cos 和 sin 的一个函数，在 [-1, +1] 之间抖动的。&lt;/p>
&lt;blockquote>
&lt;p>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$&lt;/p>
&lt;p>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$&lt;/p>
&lt;p>矩阵第pos行第2i列；行代表词元在序列中的位置，列代表位置编码的不同维度&lt;/p>
&lt;p>为啥设计这样的函数，参考&lt;a href="https://zh.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html"target="_blank" rel="external nofollow noopener noreferrer">位置编码&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>$input\ embedding * \sqrt{d_{model}}$&lt;/p>
&lt;blockquote>
&lt;p>学 embedding 的时候，会把每一个向量的 L2 Norm 学的比较小。&lt;/p>
&lt;p>乘上$\sqrt{d_{model}}$使得 embedding 和 positional encoding 的 scale 也是在差不多的 [-1, +1] 数值区间，可以做加法&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>加入位置编码后再进行dropout=0.1。&lt;/p>
&lt;h3 id="encoder" class="heading-element">&lt;span>Encoder&lt;/span>
 &lt;a href="#encoder" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>Transformer的编码器是由多(N=6)个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。&lt;/p>
&lt;blockquote>
&lt;p>第一个子层是**[多头自注意力](#####Multi-Head Attention)（multi-head self-attention）**；&lt;/p>
&lt;blockquote>
&lt;p>输入key、value 和 query 其实就是一个东西，就是自己本身&lt;/p>
&lt;/blockquote>
&lt;p>第二个子层是基于位置的前馈网络（position-wise feed-forward network）。&lt;/p>
&lt;blockquote>
&lt;p>作用在最后一个维度的 &lt;strong>MLP&lt;/strong>&lt;/p>
&lt;p>Point-wise: 把一个 MLP 对每一个词 （position）作用一次，对每个词作用的是是同一个多层感知机（MLP）&lt;/p>
&lt;p>$FFN(x)=max(0,xW_1+b_1)W_2+b_2$：512&amp;ndash;&amp;gt;2048&amp;ndash;&amp;gt;512&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>每个子层都采用了残差连接（residual connection）和层规范化（layer normalization）&lt;/p>
&lt;blockquote>
&lt;p>$LayerNorm(x+Sublayer(x))$&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/Transformer_LayerNor.png" height="400" />
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Transformer_LayerNor&lt;/div>
&lt;/center>
&lt;p>residual connections 需要输入输出维度一致，不一致需要做投影。简单起见，固定每一层的输出维度$d_{model }$= 512&lt;/p>
&lt;blockquote>
&lt;p>简单设计：只需调 2 个参数: $d_{model }$ 每层维度有多大 和 N 多少层，影响后续一系列网络的设计，BERT、GPT。&lt;/p>
&lt;/blockquote>
&lt;p>层规范化（layer normalization）&lt;/p>
&lt;blockquote>
&lt;img src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/transformer_LN.png" >
&lt;p>H：句长，W：词向量长 N：Batch&lt;/p>
&lt;p>Layer Normalization：是在一个句上的进行归一化。&lt;/p>
&lt;p>Batch Normalization：是把每个Batch中每句话的第一个字的同一维度看成一组做归一化。&lt;/p>
&lt;p>LayerNorm 每个样本自己算均值和方差，不需要存全局的均值和方差。&lt;/p>
&lt;p>LayerNorm 更稳定，不管样本长还是短，均值和方差是在每个样本内计算。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h4 id="attention" class="heading-element">&lt;span>Attention&lt;/span>
 &lt;a href="#attention" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h4>&lt;p>注意力函数是 一个将一个 query 和一些 key - value 对 映射成一个输出的函数，其中所有的 query、key、value 和 output 都是一些向量。&lt;/p>
&lt;blockquote>
&lt;p>output 是 value 的一个加权和 &amp;ndash;&amp;gt; 输出的维度 == value 的维度。&lt;/p>
&lt;p>query改变，权值分配不一样，输出不一样&lt;/p>
&lt;/blockquote>
&lt;p>query 和 key 的长度是等长的，都等于 dk。value 的维度是 dv，输出也是 dv。&lt;/p>
&lt;p>query 和 key 可以不等长，可用加性的注意力机制处理。&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/Transformer_attention.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Transformer_attention&lt;/div>
&lt;/center>
&lt;h5 id="scaled-dot-product-attention" class="heading-element">&lt;span>Scaled Dot-product Attention&lt;/span>
 &lt;a href="#scaled-dot-product-attention" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h5>&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/Transformer_attention_1.png" width="1000" />
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Transformer_attention&lt;/div>
&lt;/center>
&lt;p>注意力的具体计算是：对每一个 query 和 key 做内积，然后把它作为相似度。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>两个向量做内积：用来衡量两向量的相似度。内积的值越大，它的余弦值越大，这两个向量的相似度就越高。如果你的内积的值为 0 ，这两个向量正交了，没有相似度。&lt;/strong>&lt;/p>
&lt;/blockquote>
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt {d_k}})V
$$&lt;p>一个 query 对所有 key 的内积值，然后再除以$\sqrt{d_k}$， 再做 softmax。 softmax 是对每一行的值做 softmax，然后每一行之间是独立的，会得到权重。&lt;/p>
&lt;blockquote>
&lt;p>除以$\sqrt{d_k}$：防止softmax函数的梯度消失。&lt;/p>
&lt;blockquote>
&lt;p>2 个向量的长度比较长的时候，点积的值可能会比较大，相对的差距会变大，导致最大值 softmax会更加靠近于1，剩下那些值就会更加靠近于0。值就会更加向两端靠拢，算梯度的时候，梯度比较小。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>&lt;strong>Mask机制&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>padding mask&lt;/strong>：对输入序列进行对齐。&lt;/p>
&lt;blockquote>
&lt;p>具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。&lt;/p>
&lt;p>操作和Sequence mask一致。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;font color=#e16f00>&lt;strong>Sequence mask&lt;/strong>&lt;/font>：避免在 t 时刻，看到 t 时刻以后的东西。(选择使用，在decoder时使用)&lt;/p>
&lt;blockquote>
&lt;p>操作实现：把$ Q_t $和 $K_t $和他们之后的值换成一个很大的负数，进入 softmax 后，权重为0。&lt;/p>
&lt;p>和 V 矩阵做矩阵乘法时，没看到 t 时刻以后的内容，只看 t 时刻之前的 key - value pair。&lt;/p>
&lt;p>mask是个 0 1矩阵，和attention（scale QK）size一样，t 时刻以后 mask 为 0。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h5 id="multi-head-attention" class="heading-element">&lt;span>Multi-Head Attention&lt;/span>
 &lt;a href="#multi-head-attention" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h5>&lt;ol>
&lt;li>
&lt;p>多头机制扩大了模型对不同位置的关注能力&lt;/p>
&lt;/li>
&lt;li>
&lt;p>多头机制赋予attention多种子表达方式&lt;/p>
&lt;blockquote>
&lt;p>先投影到低维，投影的 w 是可以学习的；multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ol>
&lt;p>输入：原始的 value、key、query&lt;/p>
&lt;p>进入一个Linear层，把 value、key、query 投影到比较低的维度。然后再做一个 scaled dot product 。执行 h 次会得到 h 个输出，再把 h 个 输出向量全部合并 concat 在一起，最后做一次线性的投影 Linear。&lt;/p>
&lt;blockquote>
&lt;p>投影维度 $d_v = d_{model} / h = 512 / 8 = 64$，每个 head 得到 64 维度，concat，再投影回 $d_{model}$。&lt;/p>
&lt;/blockquote>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/Transformer_multi-headed_self-attention.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">concat过程&lt;/div>
&lt;/center>
&lt;h3 id="decoder" class="heading-element">&lt;span>Decoder&lt;/span>
 &lt;a href="#decoder" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>Decoder 是 auto-regressive 自回归。当前时刻的输入是之前一些时刻的输出。做预测时，decoder 不能看到之后时刻的输出。&lt;/p>
&lt;p>Transformer解码器也是由多(N=6)个相同的层叠加而成的，每个层都有三个子层（子层表示为sublayer）。&lt;/p>
&lt;p>attention mechanism 每一次能看完完整的输入，要避免这个情况的发生。&lt;/p>
&lt;blockquote>
&lt;p>第一个子层是&lt;strong>带掩码的多头自注意力（Masked multi-head self-attention）&lt;/strong>；&lt;/p>
&lt;blockquote>
&lt;p>输入qkv复制 3 份&lt;/p>
&lt;p>masked 体现在，在预测第 t 个时刻的输出的时候，看不到 t 时刻以后的输入,具体操作看&lt;a href="####Attention">Mask机制&lt;/a>，两个Mask相加。&lt;/p>
&lt;p>保留了自回归（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。&lt;/p>
&lt;/blockquote>
&lt;p>第二个子层是**[多头自注意力](#####Multi-Head Attention)（multi-head self-attention）**；&lt;/p>
&lt;blockquote>
&lt;p>不再是 self-attention。&lt;/p>
&lt;p>&lt;strong>key - value&lt;/strong> 来自 encoder 的输出。 &lt;strong>query&lt;/strong> 是来自 decoder 里 masked multi-head attention 的输出。&lt;/p>
&lt;p>attention：query 注意到当前的 query 感兴趣的东西，对当前的 query的不感兴趣的内容，可以忽略掉。&lt;/p>
&lt;blockquote>
&lt;p>在 encoder 和 decoder 之间传递信息&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>第三个子层是基于位置的前馈网络（position-wise feed-forward network）。&lt;/p>
&lt;/blockquote>
&lt;p>每个子层都采用了残差连接（residual connection）和层规范化（layer normalization）&lt;/p>
&lt;p>关于序列到序列模型（sequence-to-sequence model），在训练阶段，其输出序列的所有位置的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，只有生成的词元才能用于解码器的自注意力计算中。流程如下（包含解码器Decoder的shifted right 输入状况）：&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/transformer_decoding_1.gif" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">decoder_step1&lt;/div>
&lt;/center>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/transformer_decoding_2.gif" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Decoder_step_end&lt;/div>
&lt;/center>
&lt;h3 id="the-final-linear-and-softmax-layer" class="heading-element">&lt;span>&lt;strong>The Final Linear and Softmax Layer&lt;/strong>&lt;/span>
 &lt;a href="#the-final-linear-and-softmax-layer" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>线性层是个简单的全连接层，将解码器的最后输出映射到一个非常大的logits向量上。&lt;/p>
&lt;blockquote>
&lt;p>假设模型已知有1万个单词（输出的词表）从训练集中学习得到。那么，logits向量就有1万维，每个值表示是某个词的可能倾向值。&lt;/p>
&lt;/blockquote>
&lt;p>softmax层将这些分数转换成概率值（都是正值，且加和为1），最高值对应的维上的词就是这一步的输出单词。&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_NLP/Transformer.assets/transformer_decoder_output_softmax.png" >
&lt;/center>
&lt;h2 id="拓展阅读" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"target="_blank" rel="external nofollow noopener noreferrer">哈佛注释版：The Annotated Transformer&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2108.07258"target="_blank" rel="external nofollow noopener noreferrer">斯坦福100+作者的200+页综述&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/pdf/1911.07013.pdf"target="_blank" rel="external nofollow noopener noreferrer">对LayerNorm的新研究&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2103.03404"target="_blank" rel="external nofollow noopener noreferrer">对Attention在Transformer里面作用的研究&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.788"target="_blank" rel="external nofollow noopener noreferrer">B站：Transformer论文逐段精读【论文精读】&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV15v411W78M?spm_id_from=333.999.0.0&amp;amp;vd_source=d28e92983881d85b633a5acf8e46efaa"target="_blank" rel="external nofollow noopener noreferrer">B站：Transformer中Self-Attention以及Multi-Head Attention详解&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV1SK4y1d7Qh?spm_id_from=333.999.0.0"target="_blank" rel="external nofollow noopener noreferrer">B站：Transformer模型(1/2): 剥离RNN，保留Attention&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://jalammar.github.io/illustrated-transformer/"target="_blank" rel="external nofollow noopener noreferrer">The Illustrated Transformer&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/366014410"target="_blank" rel="external nofollow noopener noreferrer">Transformer 论文详细解读:多配图&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://blog.csdn.net/qq_37541097/article/details/117691873"target="_blank" rel="external nofollow noopener noreferrer">详解Transformer中Self-Attention以及Multi-Head Attention&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/403433120"target="_blank" rel="external nofollow noopener noreferrer">知乎：【Transformer】10分钟学会Transformer | Pytorch代码讲解 | 代码可运行&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.zhihu.com/question/325839123"target="_blank" rel="external nofollow noopener noreferrer">知乎：深度学习attention机制中的Q,K,V分别是从哪来的？&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#can-kao-zi-liao"target="_blank" rel="external nofollow noopener noreferrer">芦苇的机器学习笔记：Self-Attention和Transformer&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html"target="_blank" rel="external nofollow noopener noreferrer">李沐：动手学深度学习——10.7. Transformer&lt;/a>&lt;/p></description></item><item><title>MAE</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_cv/mae/</link><pubDate>Fri, 09 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_cv/mae/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="mae" class="heading-element">&lt;span>MAE&lt;/span>
 &lt;a href="#mae" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2111.06377"target="_blank" rel="external nofollow noopener noreferrer">Masked Autoencoders Are Scalable Vision Learners&lt;/a>
作者：Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick&lt;/p>
&lt;p>发表时间：2021&lt;/p>
&lt;p>&lt;a href="https://github.com/facebookresearch/mae"target="_blank" rel="external nofollow noopener noreferrer">官方代码&lt;/a>&lt;/p>
&lt;p>==BERT的CV版==&lt;/p>
&lt;/blockquote>
&lt;p>Masked Autoencoders are scalable vision learners 带掩码的自编码器 是可扩展的视觉学习器&lt;/p>
&lt;blockquote>
&lt;p>两个词的用法&lt;/p>
&lt;blockquote>
&lt;p>scalable：可扩展的，模型比较大
efficient：算法特别快&lt;/p>
&lt;/blockquote>
&lt;p>vision learners：一个 backbone 的模型
masked：来源于 BERT： 每次挖掉一些东西，然后去预测挖掉的东西
Auto-encoder：
auto “自”，ML模型 auto 自模型；
样本 x 和 标号 y 来自于同样的句子里面的词 &amp;ndash;&amp;gt; auto
加 auto 在 encoder之前，MAE 的图片标号是图片本身，区分于其它工作&lt;/p>
&lt;/blockquote>
&lt;h2 id="what-makes-masked-autoencoding-different-between-vision-and-language" class="heading-element">&lt;span>What makes masked autoencoding different between vision and language？&lt;/span>
 &lt;a href="#what-makes-masked-autoencoding-different-between-vision-and-language" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>什么使得 带掩码的自编码器模型在 CV 和 NLP 处理上的不一样呢？&lt;/p>
&lt;p>CV 使用 CNN，卷积窗口不好将 mask 放进去&lt;/p>
&lt;blockquote>
&lt;p>CNN 在一张图片上，使用一个卷积窗口、不断地平滑，来汇聚一些像素上面的信息 + 模式识别
Transformer 的一个 mask 对应的是一个特定的词，会一直保留，和别的词区分开来
卷积上不好做掩码：图片的一块盖住 by 像素替换成一个特定的值，卷积窗口扫过来、扫过去时，无法区分边界，无法保持 mask 的特殊性，无法拎出来 mask；最后从掩码信息很难还原出来&lt;/p>
&lt;p>卷积不好加入位置编码？ 不那么充分&lt;/p>
&lt;blockquote>
&lt;p>Transformer 需要位置编码：attention 机制没有位置信息
卷积自带位置信息，不断平移时，不需要加入位置信息&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>语言和图片的信息密度不同&lt;/p>
&lt;blockquote>
&lt;p>NLP 的一个词是一个语义的实体，一个词在字典里有很长的解释；一句话去掉几个词，任务很难，i.e., 完形填空 &amp;ndash;&amp;gt; BERT 的 mask 比例不能过高
CV 的图片的Mask&lt;/p>
&lt;blockquote>
&lt;p>Mask块太少，直接通过对邻居的像素值进行插值还原，太简单&lt;/p>
&lt;p>随机去掉很高比例的块，极大降低图片的冗余性，迫使模型学习更好的表征：nontrivial 任务，使模型去看 一张图片的 holistic 全局信息，而不仅关注局部&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>The autoencoder‘s decoder&lt;/p>
&lt;blockquote>
&lt;p>CV 还原图片的原始像素：低层次的表示
NLP 还原句子里的词：语义层次更高，i.e., BERT 的一个全连接层还原词
图片分类、目标检测的 decoder：一个全连接层
语义分割（像素级别的输出）：一个全连接层不够，很有可能使用一个转置的卷积神经网络、来做一个比较大解码器。&lt;/p>
&lt;/blockquote>
&lt;h2 id="approach" class="heading-element">&lt;span>Approach&lt;/span>
 &lt;a href="#approach" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>随机盖住图片里的一些块(patch, image 的一个块)，再重构缺失的像素。&lt;/strong>&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/MAE.assets/MAE.png" width="600" />
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">MAE&lt;/div>
&lt;/center>
&lt;p>&lt;strong>预训练流程&lt;/strong>：input &amp;ndash;&amp;gt; patches &amp;ndash;&amp;gt; masked &amp;ndash;&amp;gt; unmasked patches in encoder &amp;ndash;&amp;gt; unmasked + masked 按位置排列进 decoder &amp;ndash;&amp;gt; decoder 重构 masked patches 的像素&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>patches + masked&lt;/strong>：一张红色鸟图片进来，切成 patches，masked 块 (3/4) 是 灰色的。
&lt;strong>unmasked patches，encoder&lt;/strong>：没有 masked (1 / 4) 的块 进入 encoder (ViT)，得到每一块的特征（蓝色）。
encoder 的输出 和 masked tokens 按照在图片中的原始位置排列成一长条向量 （包含位置信息）。
长条向量 进入 decoder，解码器尝试重构缺失的像素信息，还原原始图片&lt;/p>
&lt;blockquote>
&lt;p>解码器的最后一层： a linear projection&lt;/p>
&lt;blockquote>
&lt;p>一个 patch 是 16 * 16 像素的话，线性层会投影到长为 256 的维度，再 reshape(16, 16), 还原原始像素信息
损失函数： MSE，像素值相减，再平方和（只作用于非可见块的损失，和 BERT 一样）&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>encoder 比 decoder 高：计算量主要来自于 encoder，对图片的像素进行编码&lt;/p>
&lt;p>用 MAE 做一个 CV 的任务，只需要用编码器。一张图片进来，不需要做掩码，直接切成 patches 格子块，然后得到所有 patches 的特征表示，当成是这张图片的特征表达，用来做 CV 的任务&lt;/p>
&lt;h2 id="simple-implementation" class="heading-element">&lt;span>Simple implementation&lt;/span>
 &lt;a href="#simple-implementation" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>对每一个输入 patch 生成 a token：一个一个 patch 的线性投影 + 位置信息
随机采样：randomly shuffle 随机打断序列，把最后一块拿掉。&lt;/p>
&lt;blockquote>
&lt;p>从头部均匀的、没有重置的样本采样
25% 意味着 随机 shuffle， 只保留前 25%&lt;/p>
&lt;/blockquote>
&lt;p>after encoding 解码时：append 跟以前长度一样的这些掩码的一些词源 mask tokens （一个可以学习的向量 + 位置信息），重新 unshuffle 还原到原来的顺序&lt;/p>
&lt;blockquote>
&lt;p>MSE 算误差时，跟原始图的 patches 对应&lt;/p>
&lt;/blockquote>
&lt;h2 id="拓展阅读" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://www.bilibili.com/video/BV1sq4y1q77t?spm_id_from=333.1007.top_right_bar_window_history.content.click"target="_blank" rel="external nofollow noopener noreferrer">MAE 论文逐段精读【论文精读】&lt;/a>&lt;/p></description></item><item><title>VIT</title><link>http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/</link><pubDate>Fri, 09 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/transformer_cv/vit/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;p>[toc]&lt;/p>
&lt;h2 id="vision-transformer-vit" class="heading-element">&lt;span>Vision Transformer (VIT)&lt;/span>
 &lt;a href="#vision-transformer-vit" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2010.11929"target="_blank" rel="external nofollow noopener noreferrer">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a>
作者：Alexey Dosovitskiy; Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,Xiaohua Zhai
发表时间：(ICLR 2021)&lt;/p>
&lt;p>==Transformer杀入CV界==&lt;/p>
&lt;p>&lt;a href="https://github.com/google-research/vision_transformer"target="_blank" rel="external nofollow noopener noreferrer">官方代码&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches &amp;ndash;&amp;gt; an image is worth 16 * 16 words&lt;/p>
&lt;blockquote>
&lt;p>一个 224 * 224 图片 变成一个 196 个的 16 * 16 图片块（words in NLP）。&lt;/p>
&lt;/blockquote>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/vit.gif" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">vit&lt;/div>
&lt;/center>
&lt;h2 id="introdouction" class="heading-element">&lt;span>Introdouction&lt;/span>
 &lt;a href="#introdouction" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>Transformer 应用在 CV 的难点&lt;/p>
&lt;blockquote>
&lt;p>计算像素的 self-attention，序列长，维度爆炸&lt;/p>
&lt;blockquote>
&lt;p>Trnasformer 的计算复杂度是序列长度 n 的 平方即 $O（n^2）$&lt;/p>
&lt;p>224 分辨率的图片，有 50176 个像素点，（2d 图片 flatten）序列长度是 BERT(512) 的近 100 倍。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>CV 如何用attention( 降低序列长度)&lt;/p>
&lt;blockquote>
&lt;p>CNN 结构 + self-attention&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/1711.07971"target="_blank" rel="external nofollow noopener noreferrer">Non-Local Network&lt;/a>, 网络中的特征图当作输入 Transformer&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2005.12872"target="_blank" rel="external nofollow noopener noreferrer">DETR&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>attention 替代卷积&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/1906.05909"target="_blank" rel="external nofollow noopener noreferrer">stand-alone attention&lt;/a> 孤立自注意力&lt;/p>
&lt;blockquote>
&lt;p>用 local window 局部小窗口控制 transformer 的计算复杂度&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/2003.07853"target="_blank" rel="external nofollow noopener noreferrer">axial attention&lt;/a> 轴注意力&lt;/p>
&lt;blockquote>
&lt;p>2d变成2个1d 顺序操作，降低计算复杂度&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>Transformer 比 CNN 少 inductive biases 归纳偏置(先验知识 or 提前的假设)&lt;/p>
&lt;blockquote>
&lt;p>CNN 的 inductive biases 是 locality 和 平移等变性 translation equaivariance（平移不变性 spatial
invariance）&lt;/p>
&lt;blockquote>
&lt;p>locality: CNN用滑动窗口在图片上做卷积。假设是图片相邻的区域有相似的特征。&lt;/p>
&lt;p>translation equaivariance：f (g(x)) = g( f(x) );f 和 g 函数的顺序不影响结果。&lt;/p>
&lt;blockquote>
&lt;p>CNN 的卷积核 像一个 template 模板，同样的物体无论移动到哪里，遇到了相同的卷积核，它的输出一致&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>Transformer 没有这些先验信息，只能从图片数据里，自己学习对视觉世界的感知。&lt;/p>
&lt;p>ViT 用了图片 2d 结构 的 inductive bias 地方：resolution adjustment 尺寸改变 和 patch extraction 抽patches&lt;/p>
&lt;/blockquote>
&lt;h2 id="related-work" class="heading-element">&lt;span>Related work&lt;/span>
 &lt;a href="#related-work" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://arxiv.org/abs/1911.03584"target="_blank" rel="external nofollow noopener noreferrer">ICLR 2020&lt;/a> 从输入图片里抽取 2 * 2 patches。&lt;/p>
&lt;blockquote>
&lt;p>CIFAR-10 32 * 32 图片，2 * 2足够，16 * 16 会过大。 抽好 patch 之后，在 patches 上 做 self-attention。&lt;/p>
&lt;/blockquote>
&lt;h2 id="vit-model" class="heading-element">&lt;span>VIT Model&lt;/span>
 &lt;a href="#vit-model" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>ViT-B/16为例&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">Model&lt;/th>
 &lt;th style="text-align: center">Patch_size&lt;/th>
 &lt;th style="text-align: center">Layers&lt;/th>
 &lt;th style="text-align: center">Hidden_size D&lt;/th>
 &lt;th style="text-align: center">MLP_size 4D&lt;/th>
 &lt;th style="text-align: center">Heads&lt;/th>
 &lt;th style="text-align: center">Params&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">$Vit_{base}$&lt;/td>
 &lt;td style="text-align: center">$16\times16$&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">768&lt;/td>
 &lt;td style="text-align: center">3071&lt;/td>
 &lt;td style="text-align: center">12&lt;/td>
 &lt;td style="text-align: center">86M&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">$Vit_{large}$&lt;/td>
 &lt;td style="text-align: center">$16\times16$&lt;/td>
 &lt;td style="text-align: center">24&lt;/td>
 &lt;td style="text-align: center">1024&lt;/td>
 &lt;td style="text-align: center">4096&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">307M&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">$Vit_{huge}$&lt;/td>
 &lt;td style="text-align: center">$14\times14$&lt;/td>
 &lt;td style="text-align: center">32&lt;/td>
 &lt;td style="text-align: center">1280&lt;/td>
 &lt;td style="text-align: center">5120&lt;/td>
 &lt;td style="text-align: center">16&lt;/td>
 &lt;td style="text-align: center">632M&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;/blockquote>
&lt;p>划分 patches，flatten patches 的线性投影 + patches 的位置信息，得到输入transformer 的 tokens&lt;/p>
&lt;blockquote>
&lt;p>将图像$224×224×3$&lt;strong>划分&lt;/strong>成大小$16×16$的patch(小方块)，每个patch块可以看做是一个token(词向量)，共有$(224/16)^2=196$个token，每个token的长度为$16×16×3=768$。&lt;code>[16, 16, 3] -&amp;gt; [768]&lt;/code>&lt;/p>
&lt;blockquote>
&lt;p>在代码实现中，直接使用一个卷积核大小为16x16，步距为16，卷积核个数为768的&lt;strong>卷积&lt;/strong>来实现。通过卷积&lt;code>[224, 224, 3] -&amp;gt; [14, 14, 768]&lt;/code>，然后把H以及W两个维度展平即可&lt;code>[14, 14, 768] -&amp;gt; [196, 768]&lt;/code>&lt;/p>
&lt;p>如果改变图像的输入大小，ViT不会改变patchs的大小，那么patchs的数量会发生变化，那么之前学习的pos_embed就维度对不上了，ViT采用的方案是通过插值来解决这个问题&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>[class]&lt;/strong> token：可训练的参数，长度为768的向量，&lt;code>Concat([1, 768], [196, 768]) -&amp;gt; [197, 768]&lt;/code>&lt;/p>
&lt;blockquote>
&lt;p>所有的 tokens 在做两两的交互信息。因此，&lt;strong>[CLS]&lt;/strong> 也会和所有的图片patches 的token 交互，从而 &lt;strong>[CLS]&lt;/strong> 从图片 patches + position 的 embedding 学到有用信息，最后用**[CLS]** 做分类判断。&lt;/p>
&lt;p>CV 通常的全局特征：feature map (14 * 14) &amp;ndash;&amp;gt; GAP globally average-pooling 全局平均池化 &amp;ndash;&amp;gt; a flatten vector 全局的图片特征向量 &amp;ndash;&amp;gt; MLP 分类&lt;/p>
&lt;p>同样的，Transformer 的输出元素 + GAP也可以用做全局信息 + 分类，效果差异不大；&lt;/p>
&lt;p>ViT 对齐 标准的 transformer，选用 NLP 里常用的 CLS 和 1d position embedding&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Position Embedding&lt;/strong>：采用的是一个可训练的参数（&lt;strong>1D Pos. Emb.&lt;/strong>） &lt;code>Add([197, 768], [197, 768]) -&amp;gt; [197, 768]&lt;/code>&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>选择不同位置编码&lt;/code>几乎没有差异，原因是Transformer是直接在patch上操作而不是基于像素级，较少数量的 patches 之间的相对位置信息，容易学到，因此，空间信息编码方式差异没那么重要&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/vit_Position_Embedding.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">vit_Position_Embedding&lt;/div>
&lt;/center>
&lt;/blockquote>
&lt;/blockquote>
&lt;h3 id="vit-architecture" class="heading-element">&lt;span>Vit Architecture&lt;/span>
 &lt;a href="#vit-architecture" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/vit_Architecture.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">vit_Architecture&lt;/div>
&lt;/center>
&lt;p>&lt;strong>MLP Head&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>整个Encoder的输出为&lt;code>[197, 768]&lt;/code>我们仅仅保留最前面的CLS token作为全连接的输入&lt;code>[1, 768]&lt;/code>，然后接上全连接层及&lt;code>分类数n_class&lt;/code>，使用交叉熵损失函数计算损失，反向传播更新网络的权重和参数。&lt;/p>
&lt;blockquote>
&lt;p>在训练ImageNet21K时是由&lt;code>Linear&lt;/code>+&lt;code>tanh激活函数&lt;/code>+&lt;code>Linear&lt;/code>组成。但是迁移到ImageNet1K上或者你自己的数据上时，只用一个&lt;code>Linear&lt;/code>即可&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h3 id="hybrid-architecture" class="heading-element">&lt;span>Hybrid Architecture&lt;/span>
 &lt;a href="#hybrid-architecture" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>前 CNN + 后 Transformer&lt;/p>
&lt;p>R50不同之处&lt;/p>
&lt;blockquote>
&lt;p>R50的卷积层采用的StdConv2d不是传统的Conv2d&lt;/p>
&lt;p>所有的BatchNorm层替换成GroupNorm层。&lt;/p>
&lt;p>在原Resnet50网络中，stage堆叠次数 [3,4,6,3]。R50中，把stage4中的3个Block移至stage3中，变成 [3,4,9]。&lt;/p>
&lt;/blockquote>
&lt;p>通过R50 Backbone进行特征提取后，得到的特征矩阵shape是&lt;code>[14, 14, 1024]&lt;/code>，接着再输入Patch Embedding层，注意Patch Embedding中卷积层Conv2d的kernel_size和stride都变成了1，只是用来调整channel。&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/vit_Hybrid_Architecture.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Hybrid_Architecture&lt;/div>
&lt;/center>
&lt;h2 id="拓展阅读" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>代码&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/huggingface/transformers"target="_blank" rel="external nofollow noopener noreferrer">State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/rwightman/pytorch-image-models"target="_blank" rel="external nofollow noopener noreferrer">timm版vit&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/lucidrains/vit-pytorch"target="_blank" rel="external nofollow noopener noreferrer">lucidrains/vit-pytorch含动图&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV15P4y137jb"target="_blank" rel="external nofollow noopener noreferrer">B站：ViT论文逐段精读【论文精读】&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV1Jh411Y7WQ"target="_blank" rel="external nofollow noopener noreferrer">B站：Vision Transformer详解&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://mp.weixin.qq.com/s/YiejUQBaKX3eyVgwaV03Dg"target="_blank" rel="external nofollow noopener noreferrer">视觉Transformer(ViT)模型创新思路总结&lt;/a>&lt;/p>
&lt;h2 id="swin-transformer" class="heading-element">&lt;span>Swin Transformer&lt;/span>
 &lt;a href="#swin-transformer" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2103.14030"target="_blank" rel="external nofollow noopener noreferrer">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a>
作者：Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo&lt;/p>
&lt;p>发表时间：(ICCV 2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/microsoft/Swin-Transformer"target="_blank" rel="external nofollow noopener noreferrer">官方代码&lt;/a>&lt;/p>
&lt;p>==多层次的Vision Transformer==&lt;/p>
&lt;/blockquote>
&lt;p>Swin Transformer是一个用了移动窗口的层级式的Vision Transformer&lt;/p>
&lt;blockquote>
&lt;p>Swin：来自于 Shifted Windows&lt;/p>
&lt;blockquote>
&lt;p>更大的效率&lt;/p>
&lt;p>通过 shifting 移动的这个操作，能够让相邻的两个窗口之间有了交互，所以上下层之间就可以有 cross-window connection，从而变相的达到了一种全局建模的能力&lt;/p>
&lt;/blockquote>
&lt;p>层级式 Hierarchical&lt;/p>
&lt;/blockquote>
&lt;p>减少序列长度方式&lt;/p>
&lt;blockquote>
&lt;p>用后续的特征图来当做Transformer的输入，&lt;/p>
&lt;p>把图片打成 patch&lt;/p>
&lt;p>把图片画成一个一个的小窗口，然后在窗口里面去做自注意力&lt;/p>
&lt;/blockquote>
&lt;p>借鉴了很多卷积神经网络的设计理念以及先验知识&lt;/p>
&lt;blockquote>
&lt;p>采取了在小窗口之内算自注意力&lt;/p>
&lt;blockquote>
&lt;p>利用了卷积神经网络里的 Locality 的 Inductive bias，就是利用了局部性的先验知识，同一个物体的不同部位或者语义相近的不同物体还是大概率会出现在相连的地方&lt;/p>
&lt;/blockquote>
&lt;p>提出来了一个类似于池化的操作叫做 patch merging&lt;/p>
&lt;blockquote>
&lt;p>把相邻的小 patch 合成一个大 patch，这样合并出来的这一个大patch其实就能看到之前四个小patch看到的内容，它的感受野就增大了，同时也能抓住多尺寸的特征&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="overall-architecture" class="heading-element">&lt;span>Overall Architecture&lt;/span>
 &lt;a href="#overall-architecture" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_vit.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin_vit&lt;/div>
&lt;/center>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_all.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin&lt;/div>
&lt;/center>
&lt;ul>
&lt;li>&lt;code>win. sz. 7x7&lt;/code>表示使用的窗口（Windows）的大小&lt;/li>
&lt;li>&lt;code>dim&lt;/code>表示feature map的channel深度（或者说token的向量长度）&lt;/li>
&lt;li>&lt;code>head&lt;/code>表示多头注意力模块中head的个数&lt;/li>
&lt;/ul>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_Architecture.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin_Architecture&lt;/div>
&lt;/center>
&lt;p>&lt;strong>patch partition&lt;/strong>：将图像$224×224×3$&lt;strong>划分&lt;/strong>成大小$4×4$的patch(小方块)，得到$56\times56\times48$大小。&lt;/p>
&lt;blockquote>
&lt;p>（$224/4=56,\ 4\times4\times3=48$） &lt;code> [224, 224, 3] -&amp;gt; [56, 56, 48]&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Linear Embedding&lt;/strong>：要把向量的维度变成一个预先设置好的值&lt;strong>C&lt;/strong>，对于 Swin tiny来说$C=96$&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>[56, 56, 48] -&amp;gt;[56, 56, 96] &lt;/code>&lt;/p>
&lt;p>Patch Partition 和 Linear Embedding 就相当于是 ViT 里的Patch Projection 操作，而在代码里也是用一次卷积操作就完成&lt;/p>
&lt;/blockquote>
&lt;p>$56\times56=3136$太长，引入了&lt;strong>基于窗口的自注意力计算&lt;/strong>，每个窗口按照默认来说，都只有$M^2=7^2=49$个 patch，所以说序列长度就只有49就相当小了&lt;/p>
&lt;blockquote>
&lt;p>共有$ (56/7)\times(56/7)=8\times8=64 $个窗口。&lt;/p>
&lt;/blockquote>
&lt;p>Stage 1 经过 2 个 Swin Transformer Block，做了窗口滑动后输出的尺寸依然为 $56\times56\times96$。&lt;/p>
&lt;p>Stage 2 经过 Patch Merging后，尺寸减半，通道数翻倍，变成了$ 28\times28\times192$，再经过 2 个 Swin Transformer Block，输出$ 28\times28\times192$。&lt;/p>
&lt;p>Stage 3 经过Patch Merging后，尺寸减半，通道数翻倍，变成了$ 14\times14\times384$，再经过 6 个 Swin Transformer Block，也就是窗口滑动了 3 次，输出 $ 14\times14\times384$。&lt;/p>
&lt;p>Stage 4 经过Patch Merging后，尺寸减半，通道数翻倍，变成了$ 7\times7\times768$，再经过 2 个 Swin Transformer Block，输出$ 7\times7\times768$。&lt;/p>
&lt;h3 id="path-merging" class="heading-element">&lt;span>Path Merging&lt;/span>
 &lt;a href="#path-merging" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;blockquote>
&lt;p>$H\times W \times C -&amp;gt; \ \frac{H}{2}\times \frac{W}{2} \times 4C-&amp;gt; \ \frac{H}{2}\times \frac{W}{2} \times 2C$&lt;/p>
&lt;/blockquote>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_Path_Merging.png" width="600"/>
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin_Path_Merging&lt;/div>
&lt;/center>
&lt;h3 id="swin-transformer-block" class="heading-element">&lt;span>Swin Transformer Block&lt;/span>
 &lt;a href="#swin-transformer-block" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;blockquote>
&lt;p>&lt;strong>W-MSA&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;table border="0">
 &lt;tr>
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_MSA.gif">&lt;/td> 
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_WMSA.gif" >&lt;/td>
 &lt;/tr>
 &lt;tr >
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">MSA&lt;/td>
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">W_MSA&lt;/td>
 &lt;/tr>
&lt;/table>
&lt;p>拿stage1举例：尺寸为$56\times56\times96$；每个窗口按照默认来说，都只有$M^2=7^2=49$个 patch；共有$ (56/7)\times(56/7)=8\times8=64 $个窗口。这64个窗口里分别去算它们的自注意力。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>SW-MSA&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;table border="0">
 &lt;tr>
 &lt;td>&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_SWMSA.gif">&lt;/td> 
 &lt;td>&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_SWMSA.png" >&lt;/td>
 &lt;/tr>
 &lt;tr >
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">SW_MSA&lt;/td>
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">SW_MSA&lt;/td>
 &lt;/tr>
&lt;/table>
移动窗口就是把原来的窗口往右下角移动一半窗口(M/2)的距离
&lt;blockquote>
&lt;p>如果Transformer是上下两层连着做这种操作，先是 window再是 shifted window 的话，就能起到窗口和窗口之间互相通信的目的。&lt;/p>
&lt;p>两个结构是成对使用的，先使用一个W-MSA结构再使用一个SW-MSA结构。所以堆叠Swin Transformer Block的次数都是偶数。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
$$
Attention(Q,K,V)=softmax(\frac{Q\dot K^T}{\sqrt d}V)
$$&lt;p>&lt;strong>SA模块&lt;/strong>&lt;/p>
&lt;p>$X^{hw\times C} \cdot W^{C\times C}_q = Q^{hw\times C}$ 矩阵运算量计算：$hw\times C \times C$&lt;/p>
&lt;blockquote>
&lt;p>$X^{hw\times C} $：将所有像素（token）拼接在一起得到的矩阵（一共有hw个像素，每个像素的深度为C）&lt;/p>
&lt;p>$W^{C\times C}_q$：生成query的变换矩阵&lt;/p>
&lt;/blockquote>
&lt;p>同理K，V的生成也是$hw\times C \times C$，共$3hwC^2$&lt;/p>
&lt;p>$Q\cdot K^T$：$(hw \times C )\cdot(C \times hw)-&amp;gt;(hw)^2C$&lt;/p>
&lt;p>$\frac{Q\dot K^T}{\sqrt d}V$：$(hw \times hw )\cdot(hw \times C)-&amp;gt;(hw)^2C$&lt;/p>
&lt;p>一共$3hwC^2+2(hw)^2C$&lt;/p>
&lt;p>&lt;strong>MSA模块&lt;/strong>&lt;/p>
&lt;p>多头注意力模块相比单头注意力模块的计算量多最后一个线性投影层$(hw \times C )\cdot(C \times C)-&amp;gt;hwC^2$&lt;/p>
&lt;p>一共$4hwC^2+2(hw)^2C$&lt;/p>
&lt;p>&lt;strong>W_MSA模块&lt;/strong>&lt;/p>
&lt;p>对每个窗口内使用多头注意力模块,一共有$\frac{h}{M}\times \frac{w}{M}$个窗口，窗口高宽M&lt;/p>
&lt;p>计算量：$\frac{h}{M}\times \frac{w}{M} \times(4hwC^2+2(hw)^2C)=4hwC^2+2M^2hwC$&lt;/p>
&lt;h2 id="shifted-window-attention" class="heading-element">&lt;span>&lt;strong>Shifted Window Attention&lt;/strong>&lt;/span>
 &lt;a href="#shifted-window-attention" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>通过对特征图移位，并给Attention设置mask来间接实现的&lt;/strong>&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_shift_mask.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin_shift_mask&lt;/div>
&lt;/center>
&lt;p>&lt;strong>特征图移位&lt;/strong>：使用&lt;code>torch.roll (x, shifts=-1, dims=0)&lt;/code>将第一排数值移动到最下面，再使用&lt;code>torch.roll (x, shifts=-1, dims=1)&lt;/code>将变换后的第二张图中的第一列移动到最右边&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_shift.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin_shift&lt;/div>
&lt;/center>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_mask0.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">swin_mask&lt;/div>
&lt;/center>
&lt;table border="0">
 &lt;tr>
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_mask1.png">&lt;/td> 
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_mask2.png" >&lt;/td>
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_mask3.png">&lt;/td> 
 &lt;/tr>
 &lt;tr >
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">Mask_1&lt;/td>
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">Mask_2&lt;/td>
 &lt;td align="center" style="color:orange; border-bottom: 1px solid #d9d9d9;color: #999;
padding: 2px;">Mask_3&lt;/td>
 &lt;/tr>
&lt;/table>
&lt;p>上图黑色区域是需要的，白色需要Mask，加上较大负数如-100即可。&lt;/p>
&lt;p>最后还要恢复位置reverse cyclic shift&lt;/p>
&lt;h2 id="relative-position-bias" class="heading-element">&lt;span>Relative position bias&lt;/span>
 &lt;a href="#relative-position-bias" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>$$
Attention(Q,K,V)=softmax((\frac{Q\dot K^T}{\sqrt d}+B)V)
$$&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_position_0.png" width="600" />
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Relative position bias&lt;/div>
&lt;/center>
&lt;p>上图中的窗口中有 2*2 个 patch，分别给这四个位置标上绝对位置索引，分别为 (0,0)、(0,1)、(1,0)、(1,1)，第一个序号代表行，第二个序号代表列。以蓝色像素为参考点。用蓝色像素的绝对位置索引与其他位置索引进行相减，就得到其他位置相对蓝色像素的&lt;strong>相对位置索引&lt;/strong>。我们将各个相对位置索引展开成一个行向量，再进行拼接得到了下面的矩阵。&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_position_1.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Relative position bias&lt;/div>
&lt;/center>
&lt;p>将该矩阵加上一个 &lt;strong>M-1&lt;/strong>，M 为窗口的大小，在 Swin Transformer 中为 7，这里为 2。再将每一个&lt;strong>行标&lt;/strong>都乘以 &lt;strong>2M-1&lt;/strong>，最后将行标和列标求和，就得到最后一个矩阵的值，这个矩阵中的值就是相对位置索引&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_position_table.png" width="600" />
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Relative position bias&lt;/div>
&lt;/center>
&lt;p>这个相对位置索引需要去索引的值会有一个相对位置偏置表 (relative position bias table)；这个表的元素的个数为 &lt;strong>(2M-1)*(2M-1)&lt;/strong>。&lt;/p>
&lt;center>
&lt;img 
src="http://fengchen321.github.io/images/Transformer_CV/VIT.assets/swin_bias.png" >
&lt;br>
&lt;div style="color:orange; border-bottom: 1px solid #d9d9d9;
display: inline-block;
color: #999;
padding: 2px;">Relative position bias&lt;/div>
&lt;/center>
&lt;h2 id="拓展阅读-1" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="pytorch_classification/swin_transformer">Pytorch实现代码&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.788&amp;amp;vd_source=d28e92983881d85b633a5acf8e46efaa"target="_blank" rel="external nofollow noopener noreferrer">B站：Swin Transformer论文精读【论文精读】&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.788.recommend_more_video.1&amp;amp;vd_source=d28e92983881d85b633a5acf8e46efaa"target="_blank" rel="external nofollow noopener noreferrer">B站：Swin-Transformer网络结构详解&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://aistudio.baidu.com/aistudio/education/preview/2011960"target="_blank" rel="external nofollow noopener noreferrer">从零开始学视觉Transformer&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://my.oschina.net/u/3768341/blog/5529722"target="_blank" rel="external nofollow noopener noreferrer">Swin Transformer 介绍&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://avoid.overfit.cn/post/50b62c574f364a62b53c4db363486f74"target="_blank" rel="external nofollow noopener noreferrer">使用动图深入解释微软的Swin Transformer&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/367111046"target="_blank" rel="external nofollow noopener noreferrer">知乎：图解Swin Transformer&lt;/a>&lt;/p></description></item><item><title>ALBEF</title><link>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/albef/</link><pubDate>Thu, 08 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/albef/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="albef" class="heading-element">&lt;span>ALBEF&lt;/span>
 &lt;a href="#albef" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2107.07651"target="_blank" rel="external nofollow noopener noreferrer">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Align-before-Fuse%3A-Vision-and-Language-Learning-Li-Selvaraju/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb82c5f9efdb2ae56baa084ca41aeddd8a665c1d1%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, Steven Hoi&lt;/p>
&lt;p>发表时间：(NIPS 2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/salesforce/ALBEF"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/ALBEF.assets/ALBEF.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">ALBEF
 &lt;/div>
&lt;/center>
&lt;p>ALBEF 包含一个图像编码器 (ViT-B/16)、一个文本编码器（前 6 层 BERT）和一个多模态编码器（后 6 层 BERT，带有额外的交叉注意层）。&lt;/p>
&lt;p>image input打成patch，通过patch embedding layer，在通过12层 Vision Transformer&lt;/p>
&lt;blockquote>
&lt;p>$224\times224-$-&amp;gt; $(196+1)\times 768=197\times768$&lt;/p>
&lt;/blockquote>
&lt;p>BERT前六层去做文本编码，剩下的六层transformer encoder直接当成multi-model fusion的过程&lt;/p>
&lt;p>&lt;strong>Loss&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>
&lt;p>Image-Text Contrastive Learning (ITC)。类似于CLIP，增大同（正）样本对的similarity，减小负样本对的similarity。&lt;/p>
&lt;blockquote>
&lt;p>CLS Token当做全局特征，图像和文本各一个$768\times1$的一个向量;通过downsample和normalization变成$256\times 1$ （MoCo实现）&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>Masked Language Modeling (MLM，generative)。类似于BERT，遮盖住一些单词，然后预测出来。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Image-Text Matching (ITM，contrastive)。二分类任务，判断图-文对是否匹配。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>动量蒸馏 momentum distillation&lt;/p>
&lt;h2 id="拓展阅读" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://blog.salesforceairesearch.com/align-before-fuse/"target="_blank" rel="external nofollow noopener noreferrer">ALBEF offical blog&lt;/a>&lt;/p>
&lt;h2 id="vlmo" class="heading-element">&lt;span>VLMo&lt;/span>
 &lt;a href="#vlmo" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2111.02358"target="_blank" rel="external nofollow noopener noreferrer">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/VLMo%3A-Unified-Vision-Language-Pre-Training-with-Wang-Bao/cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Furu Wei&lt;/p>
&lt;p>发表时间：(NIPS 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/microsoft/unilm/tree/master/vlmo"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/ALBEF.assets/VLMo.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">VLMo
 &lt;/div>
&lt;/center></description></item><item><title>CLIP</title><link>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/</link><pubDate>Thu, 08 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/clip/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="clip" class="heading-element">&lt;span>CLIP&lt;/span>
 &lt;a href="#clip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2103.00020"target="_blank" rel="external nofollow noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever&lt;/p>
&lt;p>发表时间：(ICML 2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/openai/CLIP"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a> 代码只是可以用来做推理并没有开源&lt;/p>
&lt;p>图片和文本之间的对比学习&lt;/p>
&lt;p>CLIP：Con-trastive Language-Image Pre-training&lt;/p>
&lt;/blockquote>
&lt;p>利用自然语言的这种监督信号去学习一个迁移性能好的视觉网络&lt;/p>
&lt;blockquote>
&lt;p>优点&lt;/p>
&lt;ul>
&lt;li>
&lt;p>不需要再去标注数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>图片-文本对这种多模态特征适合zero-shot迁移学习&lt;/p>
&lt;blockquote>
&lt;p>单模态的对比学习：MoCo；单模态的掩码学习：MAE；只能学到视觉特征，很难zero-shot迁移学习&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;p>局限性：&lt;/p>
&lt;ul>
&lt;li>ResNet50打平手但是离SOTA还很远，扩大模型和数据集能提高预计资源$\times 1000$，代价太大&lt;/li>
&lt;li>在有些数据集上的zero-shot效果也不好：细分类数据集，抽象概念&lt;/li>
&lt;li>推理时，目标数据集out-of-distribution,CLIP泛化照样差&lt;/li>
&lt;li>不能做成生成式模型（GPT）（对比学习的目标函数和生成式的目标函数结合）&lt;/li>
&lt;li>数据利用不高效（数据大）减少数据用量：数据增强；自监督；伪标签&lt;/li>
&lt;li>下游任务数据集测试调参带入偏见：创建一个用来测试各种各样的zero-shot的迁移能力的数据集&lt;/li>
&lt;li>网上爬的未清洗，可能带有社会偏见&lt;/li>
&lt;li>提供一些训练样本反而效果变差（Few Shot效果不好）&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>不使用ImageNet的训练集的情况下直接Zero-shot 做推理就获得和之前监督训练好ResNet50同样的效果&lt;/p>
&lt;p>使用超大规模 web Image Text 数据集&lt;/p>
&lt;h2 id="related-work" class="heading-element">&lt;span>Related work&lt;/span>
 &lt;a href="#related-work" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://arxiv.org/abs/1612.09161"target="_blank" rel="external nofollow noopener noreferrer">Learning visual n-grams from web data&lt;/a>：和CLIP相似，没有transformer和大规模数据集，效果很差&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2006.06666"target="_blank" rel="external nofollow noopener noreferrer">VirTex (CVPR 2021)&lt;/a> 自回归的预测方式去做模型的预训练&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2008.01392"target="_blank" rel="external nofollow noopener noreferrer">ICMLM (ECCV 2020)&lt;/a> 用这种完形填空的方式去做预训练&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2010.00747"target="_blank" rel="external nofollow noopener noreferrer">ConVIRT (MLHC 2022)&lt;/a> 和CLIP类似，只在医疗图像上做了实验&lt;/p>
&lt;h2 id="methods" class="heading-element">&lt;span>Methods&lt;/span>
 &lt;a href="#methods" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/CLIP.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">CLIP 模型总览图
 &lt;/div>
&lt;/center>
&lt;p>(1) 模型的输入是一个图片和文字的配对；图片通过了一个图片编码器 &lt;strong>Image Encoder&lt;/strong> 得到了一些特征 $I_1,I_2,&amp;hellip;,I_N$；句子通过一个文本编码器 &lt;strong>Text Encoder&lt;/strong> 得到一些文本的特征 $T_1,T_2,&amp;hellip;,T_N$。&lt;/p>
&lt;blockquote>
&lt;p>正样本：对角线上文本和图片配对的元素 $N$&lt;/p>
&lt;p>负样本：其他 $N^2-N$&lt;/p>
&lt;/blockquote>
&lt;p>(2) prompt template 提示模板&lt;/p>
&lt;p>把Image Net 里的1,000个类变成1000个句子；句子通过预训练好的文本编码器得到1,000个文本的特征&lt;/p>
&lt;blockquote>
&lt;p>如何变成句子？用物体类别去替代图里的 object 变成 &lt;strong>A photo of a (object).&lt;/strong>&lt;/p>
&lt;p>为什么要prompt template ？只用一个单词去做 prompt 经常出现歧异性（不同语境下意思不同）。由于模型预训练时，图片和句子成对使用，推理时直接用类别单词得到的文本特征(distribution gap)，效果就会稍有下降。&lt;/p>
&lt;p>prompt engineering ：为每个任务定制提示文本可以显着提高零样本性能（缩小解空间）&lt;/p>
&lt;p>prompt ensemble：80个模板结果综合&lt;/p>
&lt;/blockquote>
&lt;p>(3) Zero-shot&lt;/p>
&lt;p>推理时，输入一张图片通过预训练好的图片编码器得到图片的特征 $I_1$，$I_1 $ 和所有的文本特征做cosine similarity (相似性比较)，得到文本特征最相似的句子$I_1T_3$。&lt;/p>
&lt;p>摆脱了categorical label 的限制&lt;/p>
&lt;blockquote>
&lt;p>不论是训练还是推理，都不需要提前定好一个标签列表。&lt;/p>
&lt;p>任意一张照片可以通过给模型输入不同的文本句子从而知道这张图片里到底有没有感兴趣的物体&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/CLIP_implementation.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">Numpy-like pseudocode for the core of an implementation of CLIP.
 &lt;/div>
&lt;/center>
&lt;ul>
&lt;li>
&lt;p>两个输入：一个是图片的输入；一个是文本的输入。通过编码器输出图像特征和文本特征&lt;/p>
&lt;/li>
&lt;li>
&lt;p>线性投射层 W 学习一下如何从单模态转变为多模态，再做一次 L2 归一化&lt;/p>
&lt;blockquote>
&lt;p>投射层 线性还是非线性 没太大关系（数据集大，多模态）&lt;/p>
&lt;p>数据增强只使用随机裁剪&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>计算consine similarity&lt;/p>
&lt;/li>
&lt;li>
&lt;p>交叉熵目标函数 一个是 Image loss；一个是 text loss； 把两个 loss 加起来取平均&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="推荐阅读" class="heading-element">&lt;span>推荐阅读&lt;/span>
 &lt;a href="#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://openai.com/blog/clip/"target="_blank" rel="external nofollow noopener noreferrer">官方博客&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.880.my_history.page.click&amp;amp;vd_source=d28e92983881d85b633a5acf8e46efaa"target="_blank" rel="external nofollow noopener noreferrer">CLIP 论文精读&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/orpatashnik/StyleCLIP"target="_blank" rel="external nofollow noopener noreferrer">style CLIP&lt;/a> (ICCV 2021)： CLIP + style GAN 想通过文字上的改变从而去引导图像生成&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2106.14843"target="_blank" rel="external nofollow noopener noreferrer">CLIP draw&lt;/a> 不需要任何训练，CLIPDraw在矢量笔画上操作，而不是在像素图像上操作，使绘画偏向于更简单的人类可识别的形状。&lt;/p>
&lt;p>&lt;a href="https://github.com/johanmodin/clifs"target="_blank" rel="external nofollow noopener noreferrer">视频检索&lt;/a>：CLIP模型把检索对象(一句话表示)变成文本特征，把视频里的每一帧都变成视觉上的特征，然后一帧一帧的去跟文本特征做对比然后挑出相似性最高的那一帧展现出来&lt;/p>
&lt;p>&lt;a href="https://lilianweng.github.io/posts/2021-09-25-train-large/"target="_blank" rel="external nofollow noopener noreferrer">How to Train Really Large Models on Many GPUs?&lt;/a>&lt;/p>
&lt;h2 id="lseg" class="heading-element">&lt;span>LSeg&lt;/span>
 &lt;a href="#lseg" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2201.03546"target="_blank" rel="external nofollow noopener noreferrer">Language-driven Semantic Segmentation&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl&lt;/p>
&lt;p>发表时间：(ICLR 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/isl-org/lang-seg"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>CLIP做图像分割：像素级别的分类&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/Lseg.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">Lseg overview
 &lt;/div>
&lt;/center>
&lt;ul>
&lt;li>
&lt;p>模型的输入是一个图片和文字的配对；图片通过了一个图片编码器 &lt;strong>Image Encoder&lt;/strong> 得到了一些密集特征$C\times \tilde H \times \tilde W$矩阵 ，各元素为$I_{11},I_{12},&amp;hellip;,I_{\tilde H \tilde W}$；文本通过一个文本编码器 &lt;strong>Text Encoder&lt;/strong> 得到一些文本的特征$N\times C$矩阵，各元素为 $T_1,T_2,&amp;hellip;,T_N$。&lt;/p>
&lt;blockquote>
&lt;p>图片编码器：dpt的结构-vision Transformer + decoder&lt;/p>
&lt;blockquote>
&lt;p>decoder目的：把bottleneck feature慢慢upscale；特征维度$C$一般是512或者768&lt;/p>
&lt;p>使用原始的ViT或者dit的预训练参数&lt;/p>
&lt;/blockquote>
&lt;p>文本编码器：CLIP里的文本编码器&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>图片特征和文本特征做点积得到$N\times \tilde H \times \tilde W$矩阵，各元素为$F_{11},F_{12},&amp;hellip;,F_{\tilde H \tilde W}$；拿输出特征和最后的ground truth去做cross entropy loss&lt;/p>
&lt;/li>
&lt;li>
&lt;p>spetial regularization block 文本和视觉特征交互，加两个这种block效果最好&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>局限性&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>目标函数不是对比学习；也不是无监督学习的框架；依赖于手工标注的segametation mask&lt;/p>
&lt;/blockquote>
&lt;h2 id="groupvit" class="heading-element">&lt;span>GroupViT&lt;/span>
 &lt;a href="#groupvit" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2202.11094"target="_blank" rel="external nofollow noopener noreferrer">GroupViT: Semantic Segmentation Emerges from Text Supervision&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang&lt;/p>
&lt;p>发表时间：(CVPR 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/NVlabs/GroupViT"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>CLIP做图像分割：监督信号来自于文本&lt;/p>
&lt;/blockquote>
&lt;p>为什么叫group?&lt;/p>
&lt;blockquote>
&lt;p>视觉做无监督分割经常就是用一类方法叫做grouping（一种自下而上的方式）&lt;/p>
&lt;blockquote>
&lt;p>类似于有一些聚类中心点，从这个点开始发散，把附近周围相似的点逐渐扩充成一个group，那这个group相当是一个segametation mask。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="methods-1" class="heading-element">&lt;span>Methods&lt;/span>
 &lt;a href="#methods-1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>ViT + grouping block + 可学习的group tokens&lt;/strong>&lt;/p>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/GroupViT_Pipeline.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">The Architecture and Training Pipeline of GroupViT
 &lt;/div>
&lt;/center>
&lt;ul>
&lt;li>
&lt;p>图像编码器：&lt;strong>Vision Transformer&lt;/strong>(12层Transformer layers)&lt;/p>
&lt;blockquote>
&lt;p>两部分输入&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>
&lt;p>原始图像的patch embedding&lt;/p>
&lt;blockquote>
&lt;p>大小$224\times224$的图片，patch size选择$16\times16$；就有一个$14\times14=196$序列长度的一个序列
然后经过这个linear projection就得到了patch embedding，维度为$196\times384$(ViT small)&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>可学习的&lt;strong>group tokens&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>开始设的是$64\times384$：64个聚类中心；384为了保持维度和patch embedding进行拼接&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>grouping block&lt;/strong> (6层Transfor Layer之后加了一个grouping block)&lt;/p>
&lt;blockquote>
&lt;p>类似于自注意力的方式先算一个相似度矩阵，用这个相似的矩阵去帮助原来的这个image token
做聚类中心的分配，从而完成了输入$(196+64)\times384$降到这个$64\times 384$&lt;/p>
&lt;blockquote>
&lt;p>合并成为更大的group，做一次聚类的分配&lt;/p>
&lt;p>降低序列长度，模型的计算复杂度，训练时间相应的都减少了&lt;/p>
&lt;/blockquote>
&lt;p>第9层Transformer Layer 之后又加了一次grouping block：$64\times 384$降到这个$8\times 384$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>文本编码器得到文本特在$z^T$；图像编码器输出$8\times 384$进行average pooling得到$1\times384$，在通过MLP得到图片特征$z^I$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>后续和CLIP一样对比学习&lt;/p>
&lt;/li>
&lt;li>
&lt;p>zero shot推理&lt;/p>
&lt;blockquote>
&lt;p>给定一个图片首先经过group ViT 得到最后8个group Embedding&lt;/p>
&lt;p>再把有可能这些标签通过这个文本编码器得到一系列的这个文本特征&lt;/p>
&lt;p>计算这些图像的Group Embedding和这些文本的特征之间的相似度&lt;/p>
&lt;p>局限性：最多只能检测到8类；没有很好的利用dense prediction的特性；CLIP 这种训练方式
没有办法学到这些背景类（语义太模糊）&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h2 id="vild" class="heading-element">&lt;span>VILD&lt;/span>
 &lt;a href="#vild" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2104.13921"target="_blank" rel="external nofollow noopener noreferrer">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui&lt;/p>
&lt;p>发表时间：(ICLR 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="methods-2" class="heading-element">&lt;span>Methods&lt;/span>
 &lt;a href="#methods-2" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/VILD_1.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">VILD
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>(a) &lt;strong>baseline&lt;/strong> 就是一个maskRCNN(定位+分类)&lt;/p>
&lt;blockquote>
&lt;p>两阶段的分类器：第一阶段RPN抽取 $N$个region Proposal ；第二阶段就是根据着$N$个 Proposal 通过detection head得到一些region embedding ，最后再通过一些分类头判断类别&lt;/p>
&lt;/blockquote>
&lt;p>(b) &lt;strong>ViLD-text&lt;/strong>：和a类似得到N个region embedding之后，和base category基类+背景类的text embedding去做点乘计算相似度，得到一个81维的向量，将这个向量做softmax，再去和ground truth做交叉熵，得到的结果即为ViLD的损失函数&lt;/p>
&lt;blockquote>
&lt;p>text embedding：经过CLIP的文本编码器得到的，不参与训练的。（类别通过prompt生成一个句子进入编码器输出）&lt;/p>
&lt;p>在b中需要改动的参数有两处，一是图像处理模块，也即抽取图像特征的backbone需要训练；二是背景类的embedding。&lt;/p>
&lt;blockquote>
&lt;p>背景类：不在基础类里的所有别的类别&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>(c) &lt;strong>ViLD-image&lt;/strong>：利用CLIP的图像编码器对自己的视觉backbone进行知识蒸馏，让backbone输出的region embedding 尽可能地靠近CLIP的image embedding&lt;/p>
&lt;blockquote>
&lt;p>一些抽好的Proposal 做一些resize的操作&lt;/p>
&lt;p>c 中输入的是M个pre-computed proposal，和a、b不同（加快训练）&lt;/p>
&lt;blockquote>
&lt;p>预先把所有图像的proposal算出来，然后一次性扔到CLIP图像编码器中先抽好存到硬盘中，这样在训练的时候就直接把这些存好的embedding取出来就可以了。&lt;/p>
&lt;p>损失函数：常用的L1 Loss。需要注意的是，作者在把一个proposal送入CLIP的图像编码器时，是将其1x和1.5x分别送入进行编码，最后再把这两个embedding加起来。&lt;/p>
&lt;/blockquote>
&lt;p>损失函数：常用的L1 Loss&lt;/p>
&lt;/blockquote>
&lt;p>(d) ViLD：ViLD-image和ViLD-text两个的合体&lt;/p>
&lt;blockquote>
&lt;p>左侧将N+M个proposal同时输入进目标检测框架，然后分开，n个Embedding去算cross entropy loss
然后m 个 precomputer embedding去算这个蒸馏的L_1 loss。&lt;/p>
&lt;p>右侧为teacher网络，只有训练的时候用，测试的时候用不到。&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/VILD_ensemble.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">VILD_ensemble
 &lt;/div>
&lt;/center>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/VILD.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">VILD
 &lt;/div>
&lt;/center>
&lt;ul>
&lt;li>
&lt;p>训练阶段&lt;/p>
&lt;blockquote>
&lt;p>图片先通过一个RPN得到一些region Proposal 然后通过RoI Align 和一些Conv层得到一些region embedding $R_1,R_2$；&lt;/p>
&lt;p>绿色的基础类先通过一个prompt然后通过文本编码器得到绿色的文本编码和$R_1,R_2$做点乘，再和ground truth做cross entropy loss；
把已经抽取好的region Proposal 通过CLIP model得到一些CLIP的iamge embedding $ I_1, I_2$；使用蒸馏计算$L_1$ loss 希望$R_1, R_2$呢尽可能的跟$I_1, I_2 $去接近&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>推理阶段&lt;/p>
&lt;blockquote>
&lt;p>不论是基础类还是新类都通过prompt再通过这个文本编码器得到所有的这些text embedding；然后让Mask RCNN抽取的region embedding去和text embedding做相似度计算，计算结果最大的那个，就是模型输出的检测到的类型。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h2 id="拓展阅读" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/565836721"target="_blank" rel="external nofollow noopener noreferrer">利用图像文本的知识蒸馏来进行开放词表目标检测&lt;/a>&lt;/p>
&lt;h2 id="glip" class="heading-element">&lt;span>GLIP&lt;/span>
 &lt;a href="#glip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2112.03857"target="_blank" rel="external nofollow noopener noreferrer">Grounded Language-Image Pre-training&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong&lt;/p>
&lt;p>发表时间：(CVPR 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/microsoft/GLIP"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>object detection 目标检测：给定图片，把bounding box 给找出来&lt;/p>
&lt;p>phrase grounding：给定图片和文本，根据文本把物体找出来&lt;/p>
&lt;blockquote>
&lt;p>定位 loss 部分差不多&lt;/p>
&lt;p>分类 loss 部分&lt;/p>
&lt;blockquote>
&lt;p>detection：它的标签是一个或者两个单词是one-hot的这种标签&lt;/p>
&lt;blockquote>
&lt;p>给定图片通过backbone得到$N\times D$的region embedding (n个bounding box，每个bounding box Embedding的维度是d)；通过$C\times D$矩阵的分类头；MNS把bounding box筛选一下，然后再去跟ground Truth 去算cross entropy loss&lt;/p>
&lt;/blockquote>
&lt;p>Vision grounding：标签是一个句子。&lt;/p>
&lt;blockquote>
&lt;p>给定图片通过backbone得到了一些region feature；一个句子prompt通过文本编码器得到文本的embedding，进行相似度计算。（类似ViLD-text）&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>目标检测和Vision grounding 结合&lt;/p>
&lt;blockquote>
&lt;p>判断一下什么时候算是一个positive match；什么时候算是一个negative match&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/GLIP.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">GLIP
 &lt;/div>
&lt;/center>
&lt;ul>
&lt;li>图片通过图像编码器得到一些region embedding；文本通过文本编码器得到一些text embedding&lt;/li>
&lt;li>用Cross Attention啊把这个文本和图像的特征交互一下&lt;/li>
&lt;/ul>
&lt;h2 id="拓展阅读-1" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;h2 id="clipasso" class="heading-element">&lt;span>CLIPasso&lt;/span>
 &lt;a href="#clipasso" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2202.05822"target="_blank" rel="external nofollow noopener noreferrer">CLIPasso: Semantically-Aware Object Sketching&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：&lt;a href="https://yaelvi116.wixsite.com/mysite"target="_blank" rel="external nofollow noopener noreferrer">Yael Vinker&lt;/a>, &lt;a href="https://pajouheshgar.github.io/"target="_blank" rel="external nofollow noopener noreferrer">Ehsan Pajouheshgar&lt;/a>, &lt;a href="https://jessica-bo.github.io/"target="_blank" rel="external nofollow noopener noreferrer">Jessica Y. Bo&lt;/a>, &lt;a href="https://roman-bachmann.github.io/"target="_blank" rel="external nofollow noopener noreferrer">Roman Bachmann&lt;/a>, &lt;a href="https://www.cs.tau.ac.il/~amberman/"target="_blank" rel="external nofollow noopener noreferrer">Amit Haim Bermano&lt;/a>, &lt;a href="https://danielcohenor.com/"target="_blank" rel="external nofollow noopener noreferrer">Daniel Cohen-Or&lt;/a>, &lt;a href="https://vilab.epfl.ch/zamir/"target="_blank" rel="external nofollow noopener noreferrer">Amir Zamir&lt;/a>, &lt;a href="https://faculty.idc.ac.il/arik/site/index.asp"target="_blank" rel="external nofollow noopener noreferrer">Ariel Shamir&lt;/a>&lt;/p>
&lt;p>发表时间：(SIGGRAPH 2022) (Best Paper Award)&lt;/p>
&lt;p>&lt;a href="https://clipasso.github.io/clipasso/"target="_blank" rel="external nofollow noopener noreferrer">主页介绍 + code&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/CLIPasso.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">CLIPasso
 &lt;/div>
&lt;/center>
&lt;p>贝兹曲线&lt;/p>
&lt;blockquote>
&lt;p>通过一系列的2维的点控制的一个曲线&lt;/p>
&lt;/blockquote>
&lt;p>基于saliency的一个初始化的方式&lt;/p>
&lt;blockquote>
&lt;p>把图片扔给已经训练好的Vision Transformer，然后把最后一层的多头自注意力取加权平均做成了一个siliancy map；在这个siliancy map上去看哪些区域更显著，这些显著的区域上去采点。&lt;/p>
&lt;/blockquote>
&lt;p>定义了这几个曲线，也就这里说的$S_1$到$S_N$就是n个笔画，通过光栅化器Rasterizer得到简笔画。&lt;/p>
&lt;p>&lt;strong>Loss 选择&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>$L_s$ 基于语义性的目标函数：简笔画生成的特征和原始图像生成的特征尽可能的接近&lt;/p>
&lt;p>$L_g$ 基于geometric的目标函数：resnet的 2 3 4各阶段特征拿出来算loss，而不是用最后的那个2048维的特征。&lt;/p>
&lt;blockquote>
&lt;p>保证最后生成的简笔画无论是在几何形状上，位置上跟原有的图像尽可能的一致；而且在语义信息上也能尽可能的保持一致&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>&lt;strong>局限性&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>图像有背景，效果就会大打折扣。必须是一个物体然后处在一个纯白色的背景上&lt;/p>
&lt;blockquote>
&lt;p>先把一张带背景的图片，把这个物体抠出来，背景是一个白色幕布的图片，扔给CLIPasso去生成简笔画(两阶段)&lt;/p>
&lt;/blockquote>
&lt;p>初始化的笔画都是同时生成的而不是序列生成的（怎样才能一笔一画）&lt;/p>
&lt;p>通过控制笔画数去控制图片的抽象程度 （手动&amp;ndash;优化参数）&lt;/p>
&lt;/blockquote>
&lt;h2 id="拓展阅读-2" class="heading-element">&lt;span>拓展阅读&lt;/span>
 &lt;a href="#%e6%8b%93%e5%b1%95%e9%98%85%e8%af%bb-2" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://distill.pub/2021/multimodal-neurons/"target="_blank" rel="external nofollow noopener noreferrer">Multimodal Neurons in Artificial Neural Networks&lt;/a> 可视化分析 CLIP&lt;/p>
&lt;h2 id="clip4clip" class="heading-element">&lt;span>CLIP4Clip&lt;/span>
 &lt;a href="#clip4clip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2104.08860"target="_blank" rel="external nofollow noopener noreferrer">CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, Tianrui Li&lt;/p>
&lt;p>发表时间：( 2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/ArrowLuo/CLIP4Clip"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>视频领域&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/CLIP4Clip.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">CLIP4Clip
 &lt;/div>
&lt;/center>
&lt;p>对含有时序的视频特征处理，假设10帧&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>
&lt;p>10个图像的特征直接取平均 （没有考虑到这个时序的特性）&lt;/p>
&lt;blockquote>
&lt;p>一个是一个人逐渐的在坐下，另外一个是一个人逐渐的站起来；只是取一个这个平均的话，这两个动作无法区分&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>late fusion： 最原始的lstm把这10个特征扔给一个lstm，把最后的输出拿出来 （时序建模：Transformer替代）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>early fusion：把文本和这个图像帧的特征一起在学习&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="actionclip" class="heading-element">&lt;span>ActionCLIP&lt;/span>
 &lt;a href="#actionclip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2109.08472"target="_blank" rel="external nofollow noopener noreferrer">ActionCLIP: A New Paradigm for Video Action Recognition&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Mengmeng Wang, Jiazheng Xing, Yong Liu&lt;/p>
&lt;p>发表时间：( 2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/sallymmx/actionclip"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>动作识别&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/ActionCLIP.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">ActionCLIP
 &lt;/div>
&lt;/center>
&lt;p>视频的输入通过一个视频编码器得到一些特征，把标签当做文本给一个文本编码器得到一些文本的特征；去计算文本和图像之间的相似度；相似度矩阵和提前定义好的ground truth算一个loss。&lt;/p>
&lt;p>把cross entropy loss换成KL divergence&lt;/p>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/Overview of ActionCLIP.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">Overview of ActionCLIP
 &lt;/div>
&lt;/center>
&lt;h2 id="pointclip" class="heading-element">&lt;span>PointCLIP&lt;/span>
 &lt;a href="#pointclip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2112.02413"target="_blank" rel="external nofollow noopener noreferrer">PointCLIP: Point Cloud Understanding by CLIP&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/PointCLIP%3A-Point-Cloud-Understanding-by-CLIP-Zhang-Guo/f3ce9ba3fcec362b70263a7ed63d9404975496a0"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li&lt;/p>
&lt;p>发表时间：(CVPR 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/zrrskywalker/pointclip"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>3D点云&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/PointCLIP.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">PointCLIP
 &lt;/div>
&lt;/center>
&lt;p>把3D点云投射到2D平面上变成了2D的深度图，扔给clip的视觉编码器得到视觉表征。&lt;/p>
&lt;p>文本端通过prompt变成了句子point cloud depth Map of a 『CLASS』&lt;/p>
&lt;h2 id="depthclip" class="heading-element">&lt;span>DepthCLIP&lt;/span>
 &lt;a href="#depthclip" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2207.01077"target="_blank" rel="external nofollow noopener noreferrer">Can Language Understand Depth?&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Renrui Zhang, Ziyao Zeng, Ziyu Guo, Yafeng Li&lt;/p>
&lt;p>发表时间：(CVPR 2022)&lt;/p>
&lt;p>&lt;a href="https://github.com/adonis-galaxy/depthclip"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>用文本跨界估计深度&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/CLIP.assets/DepthCLIP.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">DepthCLIP
 &lt;/div>
&lt;/center>
&lt;p>把深度估计看成了一个分类问题，强制性的把深度距离分成了7大类&lt;/p></description></item><item><title>VILT</title><link>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/</link><pubDate>Thu, 08 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/multimodal-learning/vilt/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="vilt" class="heading-element">&lt;span>VILT&lt;/span>
 &lt;a href="#vilt" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2102.03334"target="_blank" rel="external nofollow noopener noreferrer">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Wonjae Kim, Bokyung Son, Ildoo Kim&lt;/p>
&lt;p>发表时间：(ICML 2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/dandelin/vilt"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>第一个摆脱了目标检测的视觉文本模型&lt;/p>
&lt;/blockquote>
&lt;h2 id="abstract" class="heading-element">&lt;span>Abstract&lt;/span>
 &lt;a href="#abstract" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>Vision and Language Pre-training(VLP) 当前的工作主要集中在图像特征抽取上，一般来讲，图像特征抽取的越好，下游任务中的表现就越好。&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>
&lt;p>效率太低，速度太慢，抽取图像特征花费大量时间，比多模态融合都多。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用一个预训练好的模型去抽取特征，表达能力受限。&lt;/p>
&lt;blockquote>
&lt;p>目标检测数据集不够大，规模不够大。如果模型不是端到端学习，只是从预训练模型抽取特征，大概率来说不是最优解。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="relate-work" class="heading-element">&lt;span>Relate work&lt;/span>
 &lt;a href="#relate-work" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
 &lt;img src="http://fengchen321.github.io/images/MultiModal learning/VILT.assets/vilt_model.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">Four categories of vision-and-language models
 &lt;/div>
&lt;/center>
> 第一类，代表作VSE，文本端较为简单，图像比较贵，融合端也是简单的神经网络。
>
> 第二类，代表作CLIP，图像和文本的计算力度等价，融合的时候将两种特征直接点乘，非常轻量。
>
> 第三类，代表作ViLBERT、UNITER占据了大部分工作，文本端非常轻量。图像端使用目标检测的系统，非常贵。融合端也使用了Transformer，相当于两个大模型。
>
> 第四类，代表作ViLT，基于ViT对图像使用patch embedding，模态融合部分做得比较大。
&lt;table border="0">
 &lt;tr>
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/MultiModal learning/VILT.assets/vilt_compare.png">&lt;/td>
 &lt;td align="center">&lt;img src="http://fengchen321.github.io/images/MultiModal learning/VILT.assets/vilt_runtime.png">&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td colspan="2" align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">Visual comparison of conventional VLP architectures and ViLT&lt;/td>
 &lt;/tr>
&lt;/table>
&lt;p>&lt;strong>模态融合&lt;/strong>方法&lt;/p>
&lt;blockquote>
&lt;p>signal-stream approach：将两种特征拼接起来，用一个模型处理两个输入。&lt;/p>
&lt;p>dual-stream approach：两个模型分别对两种模态信息进行处理，充分挖掘每种模态包含的信息，然后再融合。&lt;/p>
&lt;/blockquote>
&lt;p>两种模型表现差不多，但是dual-stream approach参数多一些，VILT 采用signal-stream approaches。&lt;/p>
&lt;p>&lt;strong>文本编码端&lt;/strong>都是用预训练的BERT里的tokenizer&lt;/p>
&lt;p>&lt;strong>视觉编码端&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Region Feature&lt;/strong>：经过一个Backbone抽取特征，然后经过RPN网络生成proposal，经过非极大值抑制 NMS 筛选边界框，最后经过ROI head得到图像序列。把一张图像变成了&lt;strong>离散&lt;/strong>的bound-box，每个边界框内都含有明确的类别语义信息。(目标检测)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Grid Feature&lt;/strong>：仅基于Backbone&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Patch Projection&lt;/strong>：基于 ViT 直接将图像打成patch，，得到一个&lt;strong>有语义信息的离散的序列&lt;/strong>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>VILT 把模态的特征抽取做到了极小化，主要计算量在模态融合部分，提高模型推理速度。移除了Region feature&lt;/p>
&lt;h2 id="methods" class="heading-element">&lt;span>Methods&lt;/span>
 &lt;a href="#methods" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;img loading="lazy" src="VILT.assets/vilt.png" alt="The main figure" srcset="VILT.assets/vilt.png?size=small, VILT.assets/vilt.png?size=medium 1.5x, VILT.assets/vilt.png?size=large 2x" data-title="The main figure" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/p>
&lt;p>文本端有$L$个长为$H$ 的序列，$L$为一个句子中单词数量，$H$为序列长度。&lt;/p>
&lt;p>图像端图像被打成 $N $个patch，每个patch也对应长为$H$的序列。&lt;/p>
&lt;p>Modal-type embedding 模态信息（文本为0，图像为1），Token position embedding 文本位置信息，Patch position embedding 图像位置信息。&lt;/p>
&lt;p>Modal-type embedding + position embedding + word embedding 不是拼接，是加在一起&lt;/p>
&lt;p>Transformer Encoder的输入为$（N+L+2)\times H$的矩阵。* 代表 [CLS] token，$（N+L+2)\times H$中2代表两种模态的[CLS]。&lt;/p>
&lt;p>使用了两个loss，分别是&lt;strong>Image Text Matching&lt;/strong>和&lt;strong>Mask Laguage Modeling&lt;/strong>。加个小loss ：&lt;strong>Word Patch Alignment&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Image Text Matching：文字，图片配对 （文本与图像是否匹配）&lt;/p>
&lt;p>Mask Laguage Modeling：NLP的完形填空&lt;/p>
&lt;p>Word Patch Alignment ：利用最优运输理论计算相似度（分布距离）&lt;/p>
&lt;/blockquote>
&lt;p>Transformer 的输出为$1\times H$的矩阵，经过$H\times H$的pooler(权重矩阵)得到仍是$1\times H$的矩阵，最后经过一个FC层进行二分类任务。&lt;/p>
&lt;h3 id="whole-word-masking" class="heading-element">&lt;span>Whole word masking&lt;/span>
 &lt;a href="#whole-word-masking" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;p>例如giraffe长颈鹿这个单词，由三个词根组成，分别是gi，raf，fe，如果mask 的时候mask “raf”这个token。由于开头为gi结尾为fe的单词不多，模型就记住了中间一定是raf，就相当于模型学到了shortcut，这样泛化性就不好。&lt;/p>
&lt;p>直接mask “giraffe” 整个单词。这样就需要借助图像信息，因此就加强了图像文本的联系。&lt;/p>
&lt;h3 id="image-augmentation" class="heading-element">&lt;span>Image Augmentation&lt;/span>
 &lt;a href="#image-augmentation" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h3>&lt;blockquote>
&lt;p>为什么前边的研究没有使用数据增强？&lt;/p>
&lt;blockquote>
&lt;p>多模态学习要考虑图像文本匹配的问题，数据增强可能会改变图像语义
使用预训练模型，无法进行数据增强&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>不适用color inversion和cutout避免与文本信息不匹配。&lt;/p>
&lt;h2 id="experiments" class="heading-element">&lt;span>Experiments&lt;/span>
 &lt;a href="#experiments" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>预训练所用的数据集叫4million(4个数据集图片加起来这个数)&lt;/p>
&lt;blockquote>
&lt;p>MSCOCO：113K图片 567K 长标题
VG： 108K图片 5.41M 短标题
GCC：3.01M图片对
SBU：867K图片对&lt;/p>
&lt;/blockquote>
&lt;h2 id="future-work" class="heading-element">&lt;span>Future work&lt;/span>
 &lt;a href="#future-work" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>scalability&lt;/strong>：transformer都是越大越好，数据集越大越好（做的更大）&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/2107.07651"target="_blank" rel="external nofollow noopener noreferrer">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation&lt;/a> 用14million&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Masked Modeling for Visual Inputs&lt;/strong>：图像重建 (NLP里进行Mask重建，图像肯定也有用)&lt;/p>
&lt;blockquote>
&lt;/blockquote>
&lt;p>&lt;strong>Augmentation Strategies&lt;/strong>：数据增强&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/2206.08358"target="_blank" rel="external nofollow noopener noreferrer">MixGen: A New Multi-Modal Data Augmentation&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="推荐阅读" class="heading-element">&lt;span>推荐阅读&lt;/span>
 &lt;a href="#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;a href="https://www.bilibili.com/video/BV14r4y1j74y/?spm_id_from=333.788&amp;amp;vd_source=d28e92983881d85b633a5acf8e46efaa"target="_blank" rel="external nofollow noopener noreferrer">ViLT 论文精读&lt;/a>&lt;/p>
&lt;p>后续改进，时间提升，更少时间训练&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/2107.07651"target="_blank" rel="external nofollow noopener noreferrer">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation&lt;/a> 单机8卡训练2-3天
&lt;a href="https://arxiv.org/abs/2201.12086"target="_blank" rel="external nofollow noopener noreferrer">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation&lt;/a>
&lt;a href="https://arxiv.org/abs/2206.02967"target="_blank" rel="external nofollow noopener noreferrer">Masked Unsupervised Self-training for Zero-shot Image Classification&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>BYOL</title><link>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/</link><pubDate>Wed, 07 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/byol/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="swav" class="heading-element">&lt;span>SwAV&lt;/span>
 &lt;a href="#swav" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2006.09882"target="_blank" rel="external nofollow noopener noreferrer">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin&lt;/p>
&lt;p>发表时间：(NIPS 2020)&lt;/p>
&lt;p>对比学习和聚类结合&lt;/p>
&lt;/blockquote>
&lt;h2 id="methods" class="heading-element">&lt;span>methods&lt;/span>
 &lt;a href="#methods" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>给定同样一张图片，如果生成不同的视角，不同的 views 的话，希望可以用一个视角得到的特征去预测另外一个视角得到的特征&lt;/p>
&lt;center>
 &lt;img src = "/images/Contrastive learning/BYOL.assets/SwAV.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">SwAV 网路
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>左边：一个图片 $ X$，做两次数据增强得到了$X_1、X_2$，然后所有的样本通过一个编码器 $f_{\theta}$，输出一个特征$Z_1、Z_2$，用这些特征做一个对比学习的 loss&lt;/p>
&lt;blockquote>
&lt;p>MoCo从memory bank取负样本6万个：这是一种近似做法&lt;/p>
&lt;p>直接拿所有图片的特征跟特征做对比有点原始而且有点费资源&lt;/p>
&lt;/blockquote>
&lt;p>SwAV：跟聚类的中心 $C$ (prototype) 比&lt;/p>
&lt;blockquote>
&lt;p>C 的维度是$d\times k$，d是特征的维度，k是聚类中心个数3,000&lt;/p>
&lt;/blockquote>
&lt;p>一个图片 $ X$，做两次数据增强得到了$X_1、X_2$，然后所有的样本通过一个编码器 $f_{\theta}$，输出一个特征$Z_1、Z_2$，先通过clustering让特征 $Z$ 和prototype $C$ 生成目标$Q_1、Q_2$；C点乘$Z_1$去预测$Q_2$，换位预测&lt;/p>
&lt;/blockquote>
&lt;h2 id="multi-crop" class="heading-element">&lt;span>&lt;strong>multi crop&lt;/strong>&lt;/span>
 &lt;a href="#multi-crop" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>思想：全局的和这个局部的特征都要关注&lt;/p>
&lt;/blockquote>
&lt;p>过去的方法：用的两个crop，一个正样本对$X_1、X_2$两个图片&lt;/p>
&lt;blockquote>
&lt;p>一个图片$X$，先把它resize 到$256\times 256$，然后随机crop两个$224\times 224$的图片当成 $X_1、X_2$&lt;/p>
&lt;/blockquote>
&lt;p>SwAV：大的crop抓住的是整个场景的特征，如果更想学习这些局部物体的特征，最好能多个 crop，去图片里crop一些区域，这样就能关注到一些局部的物体&lt;/p>
&lt;blockquote>
&lt;p>但是增加crop，会增加模型的计算复杂度，因为相当于使用了更多的正样本&lt;/p>
&lt;p>进行取舍：把这个crop变得小一点，变成160 ，取2个160的crop去学全局的特征；然后为了增加正样本的数量，为了学一些局部的特征，再去随机选4个小一点crop，大小为$96\times96$&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src = "/images/Contrastive learning/BYOL.assets/SwAV_multi_crop.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">SwAV_multi_crop 实验
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>基线模型 2 个$224\times224$，multi crop 2个$160\times160$+4个$96\times96$&lt;/p>
&lt;p>SimCLR+ multi crop 涨了2.4个点，如果把 multi crop这个技术用到 BYOL 上有可能BYOL会比SwAV的效果高&lt;/p>
&lt;p>如果没有这个multi crop的这个技术其实SwAV的性能也就跟MoCo v2是差不多的&lt;/p>
&lt;/blockquote>
&lt;h2 id="byol" class="heading-element">&lt;span>BYOL&lt;/span>
 &lt;a href="#byol" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2006.07733"target="_blank" rel="external nofollow noopener noreferrer">Bootstrap your own latent: A new approach to self-supervised Learning&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Bootstrap-Your-Own-Latent%3A-A-New-Approach-to-Grill-Strub/38f93092ece8eee9771e61c1edaf11b1293cae1b"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec&lt;/p>
&lt;p>发表时间：(2020)&lt;/p>
&lt;p>没有负样本&lt;/p>
&lt;p>&lt;a href="https://github.com/open-mmlab/mmselfsup/blob/master/mmselfsup/models/algorithms/byol.py"target="_blank" rel="external nofollow noopener noreferrer">openmmlab&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="标题" class="heading-element">&lt;span>标题&lt;/span>
 &lt;a href="#%e6%a0%87%e9%a2%98" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>&lt;strong>Bootstrap your own latent: A new approach to self-supervised Learning&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Bootstrap: If you &lt;strong>bootstrap&lt;/strong> an organization or an activity, you set it up or achieve it alone, using very few resources.&lt;/p>
&lt;p>latent: 特征 hidden、feature、embedding&lt;/p>
&lt;/blockquote>
&lt;p>只有正样本；目的：让所有相似的物体，特征也尽可能的相似&lt;/p>
&lt;blockquote>
&lt;p>缺陷：有一个躺平解&lt;/p>
&lt;blockquote>
&lt;p>如果一个模型不论什么输入，都返回同样的输出，那所有的特征都是一模一样的，loss就都是 0&lt;/p>
&lt;p>而只有加上&lt;strong>负样本的约束&lt;/strong>，不光相似的物体要有相似的特征；不相似的物体也要有不相似的特征；模型才有动力去继续学（防止模型学到这个躺平解）&lt;/p>
&lt;blockquote>
&lt;p>如果输出的所有特征都一样，那在负样本的 loss 无穷大；模型更新让正样本和负样本的 loss 都往下降，达到一个最优解&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="methods-1" class="heading-element">&lt;span>methods&lt;/span>
 &lt;a href="#methods-1" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
 &lt;img src = "/images/Contrastive learning/BYOL.assets/BYOL_1.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">BYOL 网络流程
 &lt;/div>
&lt;/center>
&lt;p>&lt;strong>前向过程&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>一个mini-batch 式的图片 $x$，做两次数据增强得到了$v、v&amp;rsquo;$;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$v$ 通过编码器 $f_\theta$ 得到特征$y_\theta$；$v&amp;rsquo;$ 通过编码器 $f_\xi$ 得到特征$y&amp;rsquo;_\xi$；输出2048维(ResNet50)&lt;/p>
&lt;blockquote>
&lt;p>$f_\theta$ 和 $f_\xi$ 使用同样的网络架构(ResNet50)；参数不同。$f_\theta$ 随着梯度更新而更新；$f_\xi$ 跟 MoCo 一样，使用动量编码器，以 moving average 形式更新&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>$y_\theta$通过 $g_\theta$ 得到特征$z_\theta$； $y&amp;rsquo;&lt;em>\xi$ 通过 $g&lt;/em>\xi$ 得到特征$z&amp;rsquo;_\xi$；输出256维&lt;/p>
&lt;blockquote>
&lt;p>$g_\theta$ 和 $g_\xi$ 使用同样的网络架构 (fc + BN+ ReLU + fc )；参数不同&lt;/p>
&lt;p>SimCLR 使用projection head 输出是128维
BYOL使用projector 输出是256维 （两者都是MLP层）&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>$z_\theta$ 通过 $q_\theta$ 得到新的特征 $q_\theta (z_\theta)$； $q_\theta (z_\theta)$ 和 $sg(z&amp;rsquo;_\xi)$ 尽可能一致&lt;/p>
&lt;blockquote>
&lt;p>sg：stop gradient&lt;/p>
&lt;p>$g_\theta$ 和 $q_\theta$ 使用同样的网络架构&lt;/p>
&lt;p>用自己一个视角的特征去预测另外一个视角的特征&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>2048维的 $y_\theta$​ 做下游任务；损失函数：mean square error loss&lt;/p>
&lt;/li>
&lt;/ul>
&lt;center>
 &lt;img src = "/images/Contrastive learning/BYOL.assets/BYOL_2.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">BYOL草图
 &lt;/div>
&lt;/center>
&lt;h2 id="推荐阅读" class="heading-element">&lt;span>推荐阅读&lt;/span>
 &lt;a href="#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>&lt;a href="https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/"target="_blank" rel="external nofollow noopener noreferrer">Understanding self-supervised and contrastive learning with &amp;ldquo;Bootstrap Your Own Latent&amp;rdquo;(BYOL)&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>跟BN后的平均图片mode 做对比&lt;/p>
&lt;p>使用 BN 会产生样本信息泄漏&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://arxiv.org/abs/2010.10241"target="_blank" rel="external nofollow noopener noreferrer">原作解释：BYOL works even without batch statistics&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>BYOL 不需要 batch norm 提供的那些 batch 的这个统计量照样能工作，回应之前博客里提出来假设&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="simsiam" class="heading-element">&lt;span>SimSiam&lt;/span>
 &lt;a href="#simsiam" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2011.10566"target="_blank" rel="external nofollow noopener noreferrer">Exploring Simple Siamese Representation Learning&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Exploring-Simple-Siamese-Representation-Learning-Chen-He/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者： &lt;a href="https://xinleic.xyz/"target="_blank" rel="external nofollow noopener noreferrer">Xinlei Chen&lt;/a>, &lt;a href="https://kaiminghe.github.io/"target="_blank" rel="external nofollow noopener noreferrer">Kaiming He&lt;/a>&lt;/p>
&lt;p>发表时间：(2020)&lt;/p>
&lt;p>&lt;a href="https://github.com/facebookresearch/simsiam"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>没有负样本，不需要大的batch size, 不需要动量编码器&lt;/p>
&lt;p>可以看成是一种 EM 算法，通过这种逐步更新的方式避免模型坍塌&lt;/p>
&lt;/blockquote>
&lt;h2 id="methods-2" class="heading-element">&lt;span>methods&lt;/span>
 &lt;a href="#methods-2" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;table border="0">
 &lt;tr>
 &lt;td align="center">&lt;img src = "/images/Contrastive learning/BYOL.assets/simsiam_net.png">&lt;/td>
 &lt;td align="center">&lt;img src = "/images/Contrastive learning/BYOL.assets/simsiam_Algorithm.png">&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">simsiam 网络&lt;/td>
 &lt;td align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">算法&lt;/td>
 &lt;/tr>
&lt;/table>
&lt;p>&lt;strong>前向过程&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>一个mini-batch 式的图片 $x$，做两次数据增强得到了$x_1、x_2&amp;rsquo;$;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$x_1, x_2$ 通过编码器 $f$ 得到特征 $z_1, z_2$ ;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$z_1,z_2$ 通过predictor $h$ 得到 $p_1,p_2$;&lt;/p>
&lt;/li>
&lt;/ul>
&lt;center>
 &lt;img src = "/images/Contrastive learning/BYOL.assets/simsiam_model_compare.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">不同的对比学习模型
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>&lt;strong>SimCLR&lt;/strong> ：两编码器都有梯度回传；对比任务
&lt;strong>SwAV&lt;/strong> ：没有跟负样本；跟聚类中心去比；对比任务
&lt;strong>BYOL&lt;/strong> ：用左边呢去预测右边；同时使用了动量编码器；预测任务
&lt;strong>SimSiam&lt;/strong> ：没有负样本，不需要大的batch size, 不需要动量编码器；预测任务&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src = "/images/Contrastive learning/BYOL.assets/simsiam_model_compare_1.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">不同的对比学习模型ImageNet实验
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>&lt;strong>batch size&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>只有 MoCo v2 和 SimSiam 是可以用256的；其它工作都是要用更大的 batch size&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>负样本&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>SimCLR 和 MoCo v2 要用负样本&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>动量编码器&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>SimCLR 没有用；SimCLR v2用了
SwAV 没有用&lt;/p>
&lt;/blockquote>
&lt;p>epoch越大，Simsiam就不行了。&lt;/p>
&lt;/blockquote>
&lt;h2 id="barlow-twins" class="heading-element">&lt;span>Barlow Twins&lt;/span>
 &lt;a href="#barlow-twins" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题： &lt;a href="https://arxiv.org/abs/2103.03230"target="_blank" rel="external nofollow noopener noreferrer">Barlow Twins: Self-Supervised Learning via Redundancy Reduction&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Barlow-Twins%3A-Self-Supervised-Learning-via-Zbontar-Jing/8a9d84d86ac0d76e63914802f9738325c3bece9c"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8a9d84d86ac0d76e63914802f9738325c3bece9c%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者: Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, Stéphane Deny&lt;/p>
&lt;p>发表时间: (ICML 2021)&lt;/p>
&lt;/blockquote>
&lt;h2 id="methods-3" class="heading-element">&lt;span>methods&lt;/span>
 &lt;a href="#methods-3" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;table border="0">
 &lt;tr>
 &lt;td align="center">&lt;img src = "/images/Contrastive learning/BYOL.assets/Barlow_Twins_net.png">&lt;/td>
 &lt;td align="center">&lt;img src = "/images/Contrastive learning/BYOL.assets/Barlow_Twins_Algorithm.png">&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">Barlow Twins 网络&lt;/td>
 &lt;td align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">算法&lt;/td>
 &lt;/tr>
&lt;/table>
&lt;p>损失函数&lt;/p>
&lt;blockquote>
&lt;p>生成了一个关联矩阵cross correlation matrix；希望这个矩阵能跟一个单位矩阵 identity matrix尽量的相似&lt;/p>
&lt;blockquote>
&lt;p>希望正样本的相似性尽量都逼近于1；跟别的样本相似性尽可能是0&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;h2 id="dino" class="heading-element">&lt;span>DINO&lt;/span>
 &lt;a href="#dino" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/2104.14294"target="_blank" rel="external nofollow noopener noreferrer">Emerging Properties in Self-Supervised Vision Transformers&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Emerging-Properties-in-Self-Supervised-Vision-Caron-Touvron/ad4a0938c48e61b7827869e4ac3baffd0aefab35"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者: Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin&lt;/p>
&lt;p>发表时间: (2021)&lt;/p>
&lt;p>&lt;a href="https://github.com/facebookresearch/dino"target="_blank" rel="external nofollow noopener noreferrer">offical code&lt;/a>&lt;/p>
&lt;p>transformer加自监督&lt;/p>
&lt;/blockquote>
&lt;p>一个完全不用任何标签信息训练出的 Vision Transformer ；如果把它的自注意力图进行可视化；发现它能非常准确的抓住每个物体的轮廓 (媲美图像分割)&lt;/p>
&lt;h2 id="methods-4" class="heading-element">&lt;span>methods&lt;/span>
 &lt;a href="#methods-4" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>MoCo：左边的网络叫做 query 编码器；右边叫做 key 编码器
BYOL ：左边的网络叫做 online network；右边叫做 target network
DINO ：左边的网络叫做 student network；右边叫做 teacher network&lt;/p>
&lt;table border="0">
 &lt;tr>
 &lt;td align="center">&lt;img src = "/images/Contrastive learning/BYOL.assets/DINO_net.png">&lt;/td>
 &lt;td align="center">&lt;img src = "/images/Contrastive learning/BYOL.assets/DINO_algorithm.png">&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">DINO 网络&lt;/td>
 &lt;td align="center" style="color:black; border-bottrm: 1px solid #d9d9d9;
 padding: 2px;">算法&lt;/td>
 &lt;/tr>
&lt;/table>
&lt;p>避免模型坍塌：centering 操作&lt;/p>
&lt;blockquote>
&lt;p>把整个 batch 里的样本都算一个均值然后减掉这个均值&lt;/p>
&lt;p>MoCoV3：随机初始化了一个 patch projection 层；然后冻结使得整个训练过程中都不变&lt;/p>
&lt;/blockquote></description></item><item><title>InstDisc</title><link>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/instdisc/</link><pubDate>Wed, 07 Jun 2023 18:22:27 +0800</pubDate><guid>http://fengchen321.github.io/posts/deeplearning/contrastive-learning/instdisc/</guid><category domain="http://fengchen321.github.io/categories/deep-learning/">Deep Learning</category><description>&lt;h2 id="instdisc" class="heading-element">&lt;span>InstDisc&lt;/span>
 &lt;a href="#instdisc" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1805.01978"target="_blank" rel="external nofollow noopener noreferrer">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Unsupervised-Feature-Learning-via-Non-Parametric-Wu-Xiong/41b03c500922893906d04403cff16a5d08f26ea7"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F41b03c500922893906d04403cff16a5d08f26ea7%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Zhirong Wu, Yuanjun Xiong, Stella Yu, Dahua Lin&lt;/p>
&lt;p>发表时间：(CVPR 2018)&lt;/p>
&lt;/blockquote>
&lt;p>这篇论文提出了个体判别任务以及memory bank&lt;/p>
&lt;blockquote>
&lt;p>把每一个 instance都看成是一个类别，也就是每一张图片都看作是一个类别，目标是能学一种特征能把每一个图片都区分开来&lt;/p>
&lt;/blockquote>
&lt;h2 id="approach" class="heading-element">&lt;span>Approach&lt;/span>
 &lt;a href="#approach" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
 &lt;img src = "/images/Contrastive learning/InstDisc.assets/InstDisc_net.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">InstDisc 网络
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>通过一个卷积神经网络把所有的图片都编码成一个特征，这些特征在最后的特征空间里能够尽可能的分开&lt;/p>
&lt;blockquote>
&lt;p>训练这个卷积神经网络使用的是对比学习&lt;/p>
&lt;blockquote>
&lt;p>需要有正样本和负样本，根据个体判别这个任务，正样本就是这个图片本身（可能经过一些数据增强），负样本就是数据集里所有其它的图片&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>把所有图片的特征全都存到memory bank 里，也就是一个字典（ImageNet数据集有128万的图片，memory bank里要存128万行，也就意味着每个特征的维度不能太高，否则存储代价太大了，本文用的是128维）&lt;/p>
&lt;p>&lt;strong>前向过程&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>假如batch size是256，有256个图片进入到编码器中，通过一个 ResNet50，最后的特征维度是2048维，然后把它降维降到128维，这就是每个图片的特征大小&lt;/li>
&lt;li>batch size 是 256 的话意味着有256个正样本，负样本从 memory bank 里随机地抽一些负样本出来。本文负样本个数4096&lt;/li>
&lt;li>用NCE loss 计算对比学习的目标函数&lt;/li>
&lt;li>更新网络后，把 mini batch里的数据样本所对应的那些特征，在 memory bank 里进行更新；不停更新，最后学到这个特征尽可能的有区分性&lt;/li>
&lt;/ul>
&lt;h2 id="cpc" class="heading-element">&lt;span>CPC&lt;/span>
 &lt;a href="#cpc" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1807.03748"target="_blank" rel="external nofollow noopener noreferrer">Representation Learning with Contrastive Predictive Coding)&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Aaron van den Oord, Yazhe Li, Oriol Vinyals&lt;/p>
&lt;p>发表时间：(2018)&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src = "/images/Contrastive learning/InstDisc.assets/CPC_net.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">CPC 网络
 &lt;/div>
&lt;/center>
&lt;p>CPC不仅可以处理音频，还可以处理图片、文字以及在强化学习里使用&lt;/p>
&lt;blockquote>
&lt;p>输入 x（一个持续的序列），t 表示当前时刻，t-i 表示过去的时刻，t+i 表示未来的时刻&lt;/p>
$$z_{t +1}、z_{t + 2}$$&lt;p>（未来时刻的特征输出）&lt;/p>
&lt;blockquote>
&lt;p>一般常见的自回归模型，就是 RNN 或者 LSTM的模型&lt;/p>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>对比学习的体现&lt;/p>
&lt;ul>
&lt;li>正样本：未来的输入通过编码器以后得到的未来时刻的特征输出，这相当于做的预测是 query，而真正未来时刻的输出是由输入决定的，相对于预测来说是正样本；&lt;/li>
&lt;li>负样本：比较广泛，比如可以任意选取输入通过这个编码器得到输出，它都应该跟预测是不相似的。&lt;/li>
&lt;/ul>
&lt;p>CPC V2用了更大的模型、用了更大的图像块、做了更多方向上的预测任务，把batch norm 换成了 layer norm，而使用了更多的数据增强。&lt;/p>
&lt;h2 id="invaspread" class="heading-element">&lt;span>InvaSpread&lt;/span>
 &lt;a href="#invaspread" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：&lt;a href="https://arxiv.org/abs/1904.03436"target="_blank" rel="external nofollow noopener noreferrer">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature&lt;/a> &lt;a href="https://www.semanticscholar.org/paper/Unsupervised-Embedding-Learning-via-Invariant-and-Ye-Zhang/e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Mang Ye, Xu Zhang, Pong C. Yuen, Shih-Fu Chang&lt;/p>
&lt;p>发表时间：(CVPR 2019)&lt;/p>
&lt;p>一个编码器的端到端对比学习&lt;/p>
&lt;/blockquote>
&lt;p>可以被理解成是 SimCLR 的一个前身，它没有使用额外的数据结构去存储大量的负样本，它的正负样本就是来自于同一个 mini bach，只用一个编码器进行端到端的学习。&lt;/p>
&lt;blockquote>
&lt;p>为什么它没有取得 SimCLR 那么好的结果呢？字典必须足够大，也就是说在做对比学习的时候，负样本最好是足够多，而本文的的 batch size 就是256，也就意味着它的负样本只有500多个，再加上它还缺少像 SimCLR 那样那么强大的数据增广以及最后提出的那个 mlp projector。&lt;/p>
&lt;/blockquote>
&lt;center>
 &lt;img src = "/images/Contrastive learning/InstDisc.assets/InvaSpread_1.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">InvaSpread 思想
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>同样的图片通过编码器以后，它的特征应该很类似，不同的图片，它的特征出来就应该不类似，这就是题目中说的invariant和 spreading&lt;/p>
&lt;p>对于相似的图片、相似的物体，特征应该保持不变性，但是对于不相似的物体或者完全不沾边的物体，特征应该尽可能的分散开&lt;/p>
&lt;/blockquote>
&lt;h2 id="method" class="heading-element">&lt;span>Method&lt;/span>
 &lt;a href="#method" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;center>
 &lt;img src = "/images/Contrastive learning/InstDisc.assets/InvaSpread_2.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">InvaSpread 网络
 &lt;/div>
&lt;/center>
&lt;p>&lt;strong>前向过程&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果 batch size 是256，一共有256个图片，经过数据增强，又得到了256张图片&lt;/p>
&lt;blockquote>
&lt;p>对于 $x_1 $这张图片来说， $\hat x_1$就是它的正样本，它的负样本是所有剩下的这些图片（包括原始的图片以及经过数据增强后的图片），&lt;/p>
&lt;p>正样本是256，负样本是$(256 - 1) \times 2$，就是除去样本本身之外 mini-batch 剩下的所有样本以及它经过数据增强后的样本。&lt;/p>
&lt;p>和 InstDisc 的区别：InstDisc中，正样本虽然是256，负样本却是从一个 memory bank 里抽出来的，用的负样本是4096甚至还可以更大&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>通过编码器以后，再过一层全连接层进行降维至128维；图中绿色的球在最后的特征空间上应该尽可能的接近，但是这个绿色的球跟别的颜色的特征应该尽可能的拉远&lt;/p>
&lt;/li>
&lt;li>
&lt;p>所用的目标函数也是 NCE loss 的一个变体&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="cmc" class="heading-element">&lt;span>CMC&lt;/span>
 &lt;a href="#cmc" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;blockquote>
&lt;p>文章标题：Contrastive Multiview Coding &lt;a href="https://www.semanticscholar.org/paper/Contrastive-Multiview-Coding-Tian-Krishnan/97f4d09175705be4677d675fa27e55defac44800"target="_blank" rel="external nofollow noopener noreferrer">&lt;img loading="lazy" src="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount" alt="citation" srcset="https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount&amp;amp;size=small, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount&amp;amp;size=medium 1.5x, https://img.shields.io/badge/dynamic/json?label=citation&amp;amp;query=citationCount&amp;amp;url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount&amp;amp;size=large 2x" data-title="citation" class="suffix-invalid suffix-invalid__small suffix-invalid__large" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&lt;/a>&lt;/p>
&lt;p>作者：Yonglong Tian, Dilip Krishnan, Phillip Isola&lt;/p>
&lt;p>发表时间：(2019)&lt;/p>
&lt;p>多视角下的对比学习&lt;/p>
&lt;/blockquote>
&lt;p>CMC正样本：一个物体的很多个视角&lt;/p>
&lt;p>工作目的就是去增大互信息（所有的视角之间的互信息）&lt;/p>
&lt;center>
 &lt;img src = "/images/Contrastive learning/InstDisc.assets/CMC.png">
 &lt;br>
 &lt;div style="color:black; border-bottrm: 1px solid #d9d9d9;
 display: inline-block;
 padding: 2px;">CMC 四个视角正样本和负样本
 &lt;/div>
&lt;/center>
&lt;blockquote>
&lt;p>选取的是 NYU RGBD 这个数据集（这个数据集有同时4个view，也就是有四个视角：原始的图像$V_1$、这个图像对应的深度信息$V_2$（每个物体离观察者到底有多远）、SwAV ace normal $V_3$、这个物体的分割图像$V_4$）&lt;/p>
&lt;/blockquote>
&lt;p>CMC是第一个或者说比较早的工作去做这种多视角的对比学习，它不仅证明了对比学习的灵活性，而且证明了这种多视角、多模态的这种可行性。&lt;/p>
&lt;p>open AI的clip模型：有一个图片，还有一个描述这个图片的文本，那这个图像和文本就可以当成是一个正样本对，就可以拿来做多模态的对比学习&lt;/p>
&lt;p>局限性：当处理不同的视角或者说不同的模态时候，可能需要不同的编码器，因为不同的输入可能长得很不一样，这就有可能会导致使用几个视角，有可能就得配几个编码器，在训练的时候这个计算代价就有点高&lt;/p>
&lt;blockquote>
&lt;p>Transformer有可能能同时处理不同模态的数据&lt;/p>
&lt;/blockquote></description></item></channel></rss>